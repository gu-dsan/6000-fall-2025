---
title: "Lecture 4"
subtitle: "MapReduce, Hadoop, and Hadoop Streaming"
author: "{{< var instructor.name >}}"
institute:
- "{{< var university.name >}}"
- "{{< var course.semester >}}"
format:
  revealjs:
    slide-number: true
    show-slide-number: print
    theme: custom.scss
    transition: fade
    background-transition: fade
    highlight-style: ayu-mirage
    code-copy: true
---


##

::: {.column width="47%"}

### Looking back

* Linux and version control

* Python and parallelization

* Intro to the Cloud

* Setup of AWS Services

:::
  
::: {.column width="47%"}
### Today

* Intro to Hadoop and MapReduce

* Hadoop Streaming

* Lab/Demo (depending on time): 
  * Start a cluster on AWS
  * Run a sample Hadoop job
  * Run a simluated mapreduce job using the command line
  * Run a Hadoop Streaming job

:::
  

## Yesterday's hardware for big data processing

::: {.column width="47%"}
### The 1990's solution
**One big box, all processors share memory**
  
This was:

* Very expensive
* Low volume

It was all _premium_ hardware.
**And yet is still was not big enough!**
  
:::

::: {.column width="10%"}
:::
  
::: {.column width="37%"}
<img src="img/yesterdays-hardware.jpg" width=350>
:::
  
  
  
## Enter commodity hardware!
  
::: {.column width="47%"}
### Consumer-grade hardware
Not expensive, premium nor fancy in any way

**Desktop-like servers are cheap, so buy a lot!**
  
* Easy to add capacity
* Cheaper per CPU/disk

### But

Need more complex software to be able to run on lots of smaller/cheaper machines.

:::
  
  
::: {.column width="47%"}
<img src="img/hardware-big-data.jpg" width=350>
:::
  
  
  
  
## Problems with cheap hardware
  
::: {.column width="47%"}
### Failures
* 1-5% hard drives/year
* 0.2% DIMMs/year

### Network speed vs. shared memory
* Much more latency
* Network slower than storage

### Uneven performance
:::
  
::: {.column width="47%"}
<img src="img/cheap-hw.jpg">
:::
  
# Big data processing systems build on average machines which fail pretty often! {.section}
  
## Hang on to this thought!
  
  
## Meet [Doug Cutting](https://en.wikipedia.org/wiki/Doug_Cutting)
  
::: {.column width="47%"}
<img src="img/doug-cutting.jpeg" width=350>
:::
  
::: {.column width="47%"}
* In 1997, Doug Cutting started writing the first version of [Lucene](https://lucene.apache.org/) (a full text search library). 
* In 2001, Lucene moves to the [Apache Software Foundation](https://apache.org/), and [Mike Cafarella](https://en.wikipedia.org/wiki/Mike_Cafarella) joins Doug and create a Lucene subproject called [Nutch](http://nutch.apache.org/), a web-crawler. Nutch uses Lucene to index the contents of a web page as it crawls it.
* Nutch and Lucene were deployed on a **single machine** (single core processor, 1GB RAM, 8 HDDs ~ 1TB), achieved decent performance, but they needed something that would be scalable enough to be able to index the web.
:::
  
  
  
## Doug and Mike set out to to improve Nutch
  
They needed a to build some kind of distributed storage layer to be the foundation of a scalable system. The came up with these requirements:
  
* Schemaless 
* Durable
* Capable of handling component failure
* Automatically rebalanced

::: {.fragment}  
**Does this sound familiar?**
:::

# In 2003 and 2004 Google publishes two seminal papers {.section}
  
## 
::: {.column width="47%"}
<img src="img/gfs-paper.png">
:::
  
::: {.column width="47%"}
### The Google File System (GFS) Paper

It describes how Google stored its information, at scale, using a reliable and high-available storage system can be built on commodity machines considering that failures are the norm rather than the exception.

GFS is:
* optimized for special application environment
* fault tolerance is built in
* centralized metadata management
* simplify the operation semantics
* decouple I/O and metadata operations
:::
  
  
  
## 
::: {.column width="47%"}
<img src="img/mr-paper.png">
:::
  
::: {.column width="47%"}

### The Google MapReduce Paper

Describes how Google processes data, at scale using MapReduce, a paradigm based on functional programming. MapReduce is an approach and infrastructure for doing things at scale. MapReduce is two things:
  
1. A data processing model named MapReduce 
1. A distributed, large scale data processing paradigm.

Which provides the following benefits:

* Moves the computation to the data
* Automatic parallelization and distribution
* Fault tolerance
* I/O scheduling
* Integrated status and monitoring

:::


## 

::: {.column width="47%"}
<img src="img/mr-paper.png">
:::
  
::: {.column width="47%"}

**The MapReduce model**

1. Map function takes an input pair and produces a set of intermediate key/value pairs. The MapReduce library groups together all intermediate values associated with the same intermediate key and passes them to the Reduce function.
2. Reduce function accepts an intermediate key and a set of values for that key. It merges together these values to form a possibly smaller set of values. Typically just zero or one output value is produced per Reduce invocation.


:::
  

## 
::: {.column width="47%"}
<img src="img/mr-paper.png">
:::
  
::: {.column width="47%"}


**The advantages**

1. The model is relatively easy to use, even for programmers without experience with parallel and distributed systems since it handles parallelization, fault-tolerance, locality optimization, and load balancing.
2. A large variety of problems are easily expressible as MapReduce computations.
3. This paper developed an implementation of MapReduce that scales to large clusters of machines comprising thousands of machines. 

:::

## The genesis of _Hadoop_: an implementation of Google's ideas {.smaller}
  
* Using the Google papers as a specification, they started implementing the ideas in Java in 2004 and created the _Nutch Distributed File System (NDFS)_.
* The main purpose of this file system was to abstract the cluster's storage so that it presents itself as a single, reliable, file system.
* Another first class featuer of the system was its ability to handle failures without operator interventions and it can run on inexpensive, commodity hardware components
* In 2006, Hadoop was created by moving NDFS (which becamie HDFS) and the MapReduce implementation out of Lucene. Hadoop 1.0 was released. 1.8TB of data sorts in 188 nodes in 48hours. Doug Cutting goes to work at Yahoo!
* In 2007, LinkedIn, Twitter and Facebook started adopting this new tool and contributing back to the project. Yahoo! is running their first production Hadoop cluster with 1,000 machines.
* In 2008, Yahoo! is running a 10,000 core cluster. World record created for fastest sorting of 1Tb in 209 seconds using a 910 node cluster. Cloudera is formed.
* In 2009, Yahoo!'s cluster is 24,000 cores and claims to sort 1TB in 62 seconds. Amazon introduces Elastic MapReduce. HDFS and MapReduce become their own separate sub-projects.
* In 2010, Yahoo!'s cluster is 4,000 and ~70PB, Facebook's is 2,300 nodes and ~40PB. 
* In 2011, Yahoo!'s cluster is 42,000 nodes 
* In 2017, Twitter Hadoop cluster has over 500 Petabytes of data. Biggest cluster over 10k nodes. [Read more here](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2017/the-infrastructure-behind-twitter-scale)



## Trivia question: why is it named Hadoop?

::: {.fragment}


::: {.column width="47%"}
<img src="img/nyt-hadoop2.png">
:::


::: {.column width="47%"}
<img src="img/apache-hadoop-logo.svg">
:::

:::

## Tangent: Why Open Source Matters!

The Google File System is not popular as a program among the business world... why???

Hadoop has become the standard for cluster storage because it is open source!!

Who has heard of Colossus? (Google's next version of GFS)

[Review of GFS vs. Hadoop](https://pierrezemb.fr/posts/colossus-google/)



## What is Hadoop 1.0: MapReduce Engine and HDFS

::: {.column width="47%"}
### MapReduce performs the computations and manages cluster
* The _Job Tracker_ is the master planner
* The _Task Tracker_ runs each task

### HDFS stores the data
:::
  
::: {.column width="47%"}
<img src="img/michael-noll-hadoop-1-0.png">
:::
  
  
  
##   
::: {.column width="47%"}
### Hadoop 1.0 Problems

* Monolithic
* MapReduce had too many responsibilites
* Assigning cluster resources
* Managing job execution
* Doing data processing
* Interfacing with client applications
* Only supported MapReduce
* Had a single _NameNode_ to manage the cluster (single point of failure)
* Batch oriented and extremely inefficient with iterative queries

<img src="img/arch1-0.png">
:::

  
::: {.column width="47%"}
::: {.fragment}
### YARN & Hadoop 2.0

* Cluster management capability gets pulled out of MapReduce and becomes YARN (Yet Anothe Resource Negotiator), decoupling cluster operations from data pipeline
* Allowed for other applications to run on a cluster
* Hadoop 2.0 was released in 2013

<img src="img/arch2-0.png">
:::
:::

##
::: {.column width="47%"}
### A MapReduce Job (1.0)
<img src="img/how-hadoop-runs-1-0.png">
:::
  
::: {.column width="47%"}
### A MapReduce Job (2.0+)
<img src="img/hddg_0701.png">
:::
  
  
  
  
## How YARN manages the Cluster
  
::: {.imgcenter}
![](img/hddg_0402.png){width=550}
:::
   
   
   
## The Hadoop Architecture is Scalable
   
::: {.column width="47%"}
 <img src="img/dawh_0202.png">
:::
   
::: {.column width="47%"}
 <img src="img/hadoop-doesnt-care-about-data-size.png">
:::
   
   
   
   
## Hadoop 3.0 (Released 2017)
   
  
* Supports multiple standby NameNodes.
* Supports multiple NameNodes for multiple namespaces.
* Storage overhead reduced from 200% to 50%
  * Supports GPUs
* Supports for Microsoft Azure Data Lake and Aliyun Object Storage System file-system connectors
* Rewrite of Hadoop Shell 
* MapReduce task Level Native Optimization.
* Introduces more powerful YARN

##
 
![](img/hadoop-ecosystem.png){.r-stretch}
    
The Current Hadoop Ecosystem: <https://hadoopecosystemtable.github.io/>
   
   
   
# Let's revisit Map and Reduce and learn more about MapReduce {.section}

## Map
   
::: {.column width="47%"}
 <img src="img/dawh_0203.png">
:::
   
::: {.column width="47%"}

### Map tasks are broken into the following phases:
* **record reader:** translates the _input split_ generated by the _input format_ into records or key/value pairs
* **map:** function that executes on every input record
* **combiner:** acts as a _localized_ reducer to pre-aggregate within the map phase
* **partitioner:** takes intermediate key/value pairs from the mapper (or combiner if used) and splits them up into shards, one shard per reducer
:::
   
  
   
## Reduce
   
::: {.column width="47%"}
 <img src="img/dawh_0204.png">
:::
   
   
::: {.column width="47%"}
### Reduce tasks are broken into the following phases:
* **shuffle and sort:** this phase takes the output files written by all of the partitioners and downloads the files to the local node where the _reducer_ code will run
* **reducer:** function that executes on every group of records for the same key
* **output format:** translates the final key/value pair and writes out to a file by a record writer
:::

## Combining Everything into MapReduce
   
::: {.center}
<img src="img/dawh_0205.png">
:::

# Some typical MapReduce patterns {.section}

## Map Only
       
::: {.column width="47%"}
 <img src="img/hddg_0205.png">
:::
 
::: {.column width="47%"}
### Examples of map-only jobs:
* Filtering
* Data reorganization 
* Running a task in an embarrassingly parallel way
:::
 
## Map and Reduce
 
::: {.column width="47%"}
### Single reducer 
<img src="img/hddg_0203.png">
:::
       
::: {.column width="47%"}
### Multiple reducers 
<img src="img/hddg_0204.png">
:::

## 
::: {.column width="47%"}
### Benefits of Hadoop/MapReduce

* Fault tolerance

* Map and Reduce functions are simple to understand and easy to program
:::
::: {.column width="47%"}
### Limitations of Hadoop/MapReduce
     
* Everything has to be expressed in a map or reduce, and sometimes you can't

* There is no control over the order in which map or reduce run

* Data is not indexed

* High overhead

* You cannot leverage parallelism if you have an extremely large number of very small files (smaller than a single block)

* Only suited for batch-processing, not real-time
:::


# Working with your data in a cluster {.section}



## Everything begins and ends in a distributed filesystem

::: {.center}
<img src="img/dawh_0208.png" width=850>
:::



## Distributed filesystems

::: {.center}
<img src="img/distributed-file-systems.png" width=850>
:::



## Cloud Object Storage vs HDFS

* Independent scalability

* Integration via simple REST API calls

* Lower cost

* No single point of failure




## Cloud based Hadoop Clusters

Amazon Elastic MapReduce (EMR) and Azure HDInsight

::: {.column width="47%"}
### On Premises
<img src="img/dawh_0202.png">
:::

::: {.column width="47%"}
### Cloud Based (note the storage layer)
<img src="img/cloud-hadoop-arch.png">
:::




## Accessing Data in Distributed Object/File System

### For files in the _Cluster HDFS_

* `directory/data/` which is a shortcut to
* `/user/hadoop/directory/data/`

### For files in _Amazon S3_

The service name is _S3_ and the data container is called a "bucket (`bucket_name`)"
* `s3://bucket_name/directory/data/`
 
### For Files in Azure Blob

The service name is _Azure Storage Account_ (`your_account`) and the data container is called a "container (`container`)". There can be many containers in a storage account.

- `wasb://container@your_account.blob.core.windows.net/directory/data/`



## HDFS essential commands

What commands have we discussed about the Linux file system that we want for HDFS?

```{{bash}}
#| code-line-numbers: false
# list files
hdfs dfs -ls <hdfs path>

# make directories
hdfs dfs -mkdir <folder location>

# touch an empty file
hdfs dfs -touchz <hdfs path>

# "put" file to hdfs
hdfs dfs -put <other file path> <dest hdfs path>

# "get" file from hdfs
hdfs dfs -get <other file path> <dest hdfs path>
```

## HDFS essential commands

```{{bash}}
# other flag commands, after command and subcommand
hdfs dfs -cat <hdfs path> #print contents of file
hdfs dfs -mv <old hdfs path> <new hdfs path> # move file/folder
hdfs dfs -rm <hdfs path> #only one file
hdfs dfs -rmr <hdfs path> # recursive flag!
hdfs dfs -chmod 777 <hdfs path> # set permissions
hdfs dfs -setfacl -m default:user:sqoop:rwx <hdfs path> # access control list
```

[Review common commands here](https://www.geeksforgeeks.org/hdfs-commands/)  
[More common commands](https://mindmajix.com/hadoop-hdfs-commands-with-examples)



## How to programmatically interact with HDFS from python?

* Use a python library (hdfs, snakebite, pydoop)

* What I do...

* subprocess to call the real functions!

```{{python}}

import subprocess

# local path of the file
local_path = '/home/hadoop/test_dir/myfile.txt'

# can be a new file name or a just folder destination
hdfs_path = '/user/hadoop/data_folder/'

# construct string command to execute in linux
linux_command = f'hdfs dfs -put {local_path} {hdfs_path}'

# run in linux, grab result code and message
result_code, result_message = subprocess.getstatusoutput(linux_command)

# raise an error if command did not exit properly
if result_code != 0:
  raise TypeError(f'exit code: {result_code}, {result_message}')

```


## Writing MapReduce applications

Hadoop is written in Java and provides a Java API that allows you to specify the following:

* The input and output data locations in your distributed object/file system

* The Map and Reduce functions which are providede in the form of Java classes

* Many kinds of job parameters and configurations

Oh no! What if I want to use other programming languages?


## {background-image="img/java-logo.png" background-size="contain"}

![](img/no-sign.png){style="width: 50%; margin: auto; display: block;"}



# [Introducing Hadoop Streaming]{style="color: #ffffff;"} { background-image="img/stream_1024.gif" background-size="cover"}


![](img/3x.png){style="width: 50%; margin: auto; display: block;"}


## The name _Hadoop Streaming_ comes from the _Unix Streams_


![](img/unix-streams){.imgcenter}



## Hadoop Streaming allows you to run _any executable script_ on the Hadoop Framework

It uses an existing Java program to run non-java programs!

![](img/dawh_0301.png){width=700 .imgcenter}

Keys and values can be single or compound

##
::: {.column width="47%"}

### Advantages of Hadoop Streaming

* Using existing code-base (non Java)

* Leveraging Hadoop framework to scale out
:::


::: {.column width="47%"}

### Limitations of Hadoop Streaming

* No combiner or partitioner, just map -> shuffle-sort -> reduce

* Everything is text. Your programs have to parse everything appropriately and convert strings to other data types as needed. You also have manually create the key/value pair for output.

* If using R or Python, you must limit your programs to the base libraries.
:::






## Basic Python Mapper

```{{python}}
#!/usr/bin/env python

import sys

if __name__ == "__main__":
 for line in sys.stdin:
     for word in line.split():
         sys.stdout.write("{}\t1\n".format(word))
```


* The first line makes this an executable Python script
* Read in line
* Split line by space into single words
* For every line: produce an output in the form of `word\t1` where `\t` is a tab
* Output is always a key and value pair!

## Basic Python Reducer

:::::: {.columns}
::: {.column width="47%"}
```{{python}}
#!/usr/bin/env python

import sys

def stdout(key, val):
  sys.stdout.write(f"{key}\t{val}\n")

if __name__ == '__main__':
 curkey = None
 total = 0
 for line in sys.stdin:
     key, val = line.split("\t")
     val = int(val)

     if key == curkey:
         total += val
     else:
         if curkey is not None:
              stdout(curkey, total)

         curkey = key
         total = val

 stdout(curkey, total)
```
:::

::: {.column width="10%"}
:::
::: {.column width="40%"}
* The first line makes this an executable Python script
* Make stdout function 
* Read in line and stores in memory
* Look at the key from the key/value
* Read next line and compare key
* Keep reading lines and storing lines in memory until the key is different
* When new key is received, take all the lines from the previous key and do something and write out result
* Repeat until all lines are processed
:::
::::::

## {.smaller}

```{{bash}}
hadoop jar /usr/lib/hadoop/hadoop-streaming.jar \
-files basic-mapper.py,basic-reducer.py \
-input [[input-location]] \
-output [[output-location]] \
-mapper basic-mapper.py \
-reducer basic-reducer.py
```


* `hadoop jar /usr/lib/hadoop/hadoop-streaming.jar` launches Hadoop with the `hadoop-streaming.jar` 

* `-files /home/hadoop/hadoop-streaming/basic-mapper.py,/home/hadoop/hadoop-streaming/basic-reducer.py` tells the job to "ship" the executable mapper and reducer scripts (the actual filename and absolute path) to every node on the cluster. **This line must always be second when you need to ship files with your job.**

* `-input [[input-location]]` tells the Hadoop the location of your source data in a distributed object/filesystem. If you specify a directory, all files in the directory will be used as inputs

* `-output [[output-location]]` tells the Hadoop the location of your output data in a distributed object/filesystem.  **This parameter is just a name of a location, and it must not exist before running the job otherwise the job will fail.** 

* `-mapper basic-mapper.py`  and `reducer basic-reducer.py` specify the commands to run as a mapper and reducer, respectively. **These must be shell scripts or native Linux commands.** If you are using a script, you must "ship" it to the nodes with the `-files` parameter above.



## Gotcha's

* The output location of the job (HFDF, S3, Blob) must *not exist* before running a job. You will get an error if you try to write out to an existing/non-empty location

* You have to _ship_ or _package_ your executables with the job.

* Hadoop/Java error messages can be cryptic and only give you errors regarding the framework. If you need to debug your script you will need to look at the stdout/stderr logs

* You need to specify the full path name of your mapper and reducer scripts




## Example code

```{{bash}}
hadoop jar /usr/lib/hadoop/hadoop-streaming.jar \
-files basic-mapper.py,basic-reducer.py \
-input /user/hadoop/in_data \
-output /user/hadoop/in_data \
-mapper basic-mapper.py \
-reducer basic-reducer.py
```



## Hadoop References


* Hadoop 3.3.1: <https://hadoop.apache.org/docs/r3.3.1/>
  
  * HDFS Filesystem Commands: <https://hadoop.apache.org/docs/r3.3.1/hadoop-project-dist/hadoop-common/FileSystemShell.html>

* Hadoop Streaming
<https://hadoop.apache.org/docs/r3.3.1/hadoop-streaming/HadoopStreaming.html>



# Cluster Demo (if there is time)
