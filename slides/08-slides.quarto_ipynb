{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Lecture 8\"\n",
        "subtitle: \"Spark Machine Learning\"\n",
        "author: \"{{< var instructor.name >}}\"\n",
        "institute:\n",
        "- \"{{< var university.name >}}\"\n",
        "- \"{{< var course.semester >}}\"\n",
        "format:\n",
        "  revealjs:\n",
        "    slide-number: true\n",
        "    show-slide-number: print\n",
        "    theme: custom.scss\n",
        "    transition: fade\n",
        "    background-transition: fade\n",
        "    highlight-style: ayu-mirage\n",
        "    code-copy: true\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "::: {.column width=\"47%\"}\n",
        "### Looking Back\n",
        "\n",
        "* Intro to Hadoop and MapReduce\n",
        "* Hadoop Streaming\n",
        "* Dask\n",
        "* Spark RDDs, DataFrames, SparkSQL\n",
        "\n",
        "### Future  \n",
        "\n",
        "* Spark NLP\n",
        "* DataBricks on Azure\n",
        "* Spark Streaming\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.column width=\"47%\"}\n",
        "### Today\n",
        "\n",
        "* Review of Distributed Hardware and Spark RDDs\n",
        "* Spark ML\n",
        "* Project Introduction\n",
        "\n",
        "* Lab: \n",
        "  * Practicing data cleaning with Spark DataFrames\n",
        "  * Using Spark ML\n",
        "\n",
        "* Quiz on Hadoop Streaming and HDFS (due Friday October 14)\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "# Essential Prior Topics {.section}\n",
        "\n",
        "## AWS Academy\n",
        "\n",
        "* Credit limit - $100\n",
        "* Course numbers:\n",
        "\n",
        "  * Course #1 - 24178\n",
        "  * Course #2 - 27354\n",
        "  * Course #3 - 22802\n",
        "  * Course #4 - 26418\n",
        "  \n",
        "STAY WITH COURSE 24178 UNLESS YOU HAVE RUN OUT OF CREDITS OR >$90 USED! \n",
        "\n",
        "Note that you will have to repeat several setup steps:\n",
        "\n",
        "- security group\n",
        "- EC2 keypair uploading (the AWS part only)\n",
        "- sagemaker setup\n",
        "- any S3 uploading or copying as well as bucket creation as necessary\n",
        "- EMR configuration\n",
        "\n",
        "## Grades\n",
        "\n",
        "- You are responsible for your grades\n",
        "\n",
        "- If there is a problem, you need to bring it up\n",
        "\n",
        "- All assignments are mandatory\n",
        "\n",
        "- State of assignments from syllabus\n",
        "\n",
        "- Group Project: 30%\n",
        "\n",
        "- HW Assignments: 45%\n",
        " \n",
        "- Lab Deliverables: 15% - Halfway through\n",
        " \n",
        "- Quizzes: 10% - One to two more\n",
        "\n",
        "\n",
        "## Review of File Systems\n",
        "\n",
        "What are the possible file system options for each item: **S3**, **HDFS**, **Local file system**\n",
        "\n",
        "::: {.fragment .highlight-red fragment-index=1}\n",
        "hadoop jar /usr/lib/hadoop/hadoop-streaming.jar #1 \\\n",
        ":::\n",
        "::: {.fragment .highlight-red fragment-index=2}\n",
        "-files basic-mapper.py,basic-reducer.py #2 \\\n",
        ":::\n",
        "::: {.fragment .highlight-red fragment-index=4}\n",
        "-input /user/hadoop/in_data #3 \\\n",
        "-output /user/hadoop/in_data #3 \\\n",
        ":::\n",
        "::: {.fragment .highlight-red fragment-index=6}\n",
        "-mapper basic-mapper.py #4 \\\n",
        "-reducer basic-reducer.py #4\n",
        ":::\n",
        "\n",
        "::: {.fragment .fade-up fragment-index=2}\n",
        "1. Local file system\n",
        ":::\n",
        "::: {.fragment .fade-up fragment-index=4}\n",
        "2. Local file system or S3\n",
        ":::\n",
        "::: {.fragment .fade-up fragment-index=6}\n",
        "3. HDFS or S3\n",
        ":::\n",
        "::: {.fragment .fade-up fragment-index=8}\n",
        "4. HDFS - why??\n",
        ":::\n",
        "\n",
        "# Spark: a Unified Engine {.section}\n",
        "\n",
        "## Connected and extensible\n",
        "\n",
        "::: {.imgcenter}\n",
        "<img src=\"img/spark-connectors.png\" width=600>\n",
        ":::\n",
        "\n",
        "## Caching and Persistence\n",
        "\n",
        "By default, RDDs are recomputed every time you run an action on them. This can be expensive (in time) if you need to use a dataset more than once.\n",
        "\n",
        "**Spark allows you to control what is cached in memory.**\n",
        "\n",
        "To tell spark to cache an object in memory, use `persist()` or `cache()`:\n",
        "\n",
        "* `cache():` is a shortcut for using default storage level, which is memory only\n",
        "* `persist():` can be customized to other ways to persist data (including both memory and/or disk)\n"
      ],
      "id": "521436b0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# caches error RDD in memory, but only after an action is run\n",
        "errors = logs.filter(lambda x: \"error\" in x and \"2019-12\" in x).cache()"
      ],
      "id": "b0ab910d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Review of PySparkSQL Cheatsheet\n",
        "\n",
        "[https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf)\n",
        "\n",
        "\n",
        "## `collect` **CAUTION**\n",
        "\n",
        "::: {.imgcenter}\n",
        "<img src=\"img/collect-warning.png\">\n",
        ":::\n",
        "\n",
        "# Spark Diagnostic UI {.section}\n",
        "\n",
        "## Understanding how the cluster is running your job\n",
        "\n",
        "Spark Application UI shows important facts about you Spark job:\n",
        "\n",
        "* Event timeline for each stage of your work\n",
        "* Directed acyclical graph (DAG) of your job\n",
        "* Spark job history\n",
        "* Status of Spark executors\n",
        "* Physical / logical plans for any SQL queries\n",
        "\n",
        "#### Tool to confirm you are getting the horizontal scaling that you need!\n",
        "\n",
        "Adapted from [AWS Glue Spark UI docs](https://docs.aws.amazon.com/glue/latest/dg/monitor-spark-ui.html)\n",
        "and [Spark UI docs](https://spark.apache.org/docs/latest/web-ui.html)\n",
        "\n",
        "\n",
        "\n",
        "## Spark UI - Event timeline\n",
        "\n",
        "::: {.imgcenter}\n",
        "<img src=\"img/spark-ui-timeline.png\" width=700>\n",
        ":::\n",
        "\n",
        "## Spark UI - DAG\n",
        "\n",
        "::: {.imgcenter}\n",
        "<img src=\"img/spark-ui-dag.png\" width=700>\n",
        ":::\n",
        "\n",
        "## Spark UI - Job History\n",
        "\n",
        "::: {.imgcenter}\n",
        "<img src=\"img/spark-ui-jobhistory.png\" width=700>\n",
        ":::\n",
        "\n",
        "## Spark UI - Executors\n",
        "\n",
        "::: {.imgcenter}\n",
        "<img src=\"img/spark-ui-executors.png\" width=700>\n",
        ":::\n",
        "\n",
        "## Spark UI - SQL\n",
        "\n",
        "::: {.imgcenter}\n",
        "<img src=\"img/spark-ui-sql.png\" width=700>\n",
        ":::\n",
        "\n",
        "# PySpark User Defined Functions\n",
        "\n",
        "## UDF Workflow\n",
        "\n",
        "::: {.imgcenter}\n",
        "<img src=\"img/spark-udf.png\" width=700>\n",
        ":::\n",
        "\n",
        "Adapted from [UDFs in Spark](https://blog.damavis.com/en/avoiding-udfs-in-apache-spark/)\n",
        "\n",
        "\n",
        "\n",
        "## UDF Example\n",
        "\n",
        "Problem: make a new column with ages for adults-only\n",
        "\n",
        "```\n",
        "+-------+--------------+\n",
        "|room_id|   guests_ages|\n",
        "+-------+--------------+\n",
        "|      1|  [18, 19, 17]|\n",
        "|      2|   [25, 27, 5]|\n",
        "|      3|[34, 38, 8, 7]|\n",
        "+-------+--------------+\n",
        "```\n",
        "\n",
        "## UDF Code Solution\n"
      ],
      "id": "5302b170"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import udf, col\n",
        "\n",
        "@udf(\"array<integer>\")\n",
        "   def filter_adults(elements):\n",
        "   return list(filter(lambda x: x >= 18, elements))\n",
        "\n",
        "# alternatively\n",
        "from pyspark.sql.types IntegerType, ArrayType\n",
        "@udf(returnType=ArrayType(IntegerType()))\n",
        "def filter_adults(elements):\n",
        "   return list(filter(lambda x: x >= 18, elements))"
      ],
      "id": "b1eb2670",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "+-------+----------------+------------+\n",
        "|room_id| guests_ages    | adults_ages|\n",
        "+-------+----------------+------------+\n",
        "| 1     | [18, 19, 17]   |    [18, 19]|\n",
        "| 2     | [25, 27, 5]    |    [25, 27]|\n",
        "| 3     | [34, 38, 8, 7] |    [34, 38]|\n",
        "| 4     |[56, 49, 18, 17]|[56, 49, 18]|\n",
        "+-------+----------------+------------+\n",
        "```\n",
        "\n",
        "\n",
        "## Alternative to Spark UDF\n"
      ],
      "id": "eb64a849"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Spark 3.1\n",
        "from pyspark.sql.functions import col, filter, lit\n",
        "\n",
        "df.withColumn('adults_ages',\n",
        "              filter(col('guests_ages'), lambda x: x >= lit(18))).show()"
      ],
      "id": "621e37c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## UDF Speed Comparison\n",
        "\n",
        "::: {.column width=\"57%\"}\n",
        "<img src=\"img/spark-udf-speed.png\" width=700>\n",
        ":::\n",
        "\n",
        "::: {.column width=\"41%\"}\n",
        "Costs:\n",
        "\n",
        "* Serialization/deserialization (think pickle files)\n",
        "* Data movement between JVM and Python\n",
        "* Less Spark optimization possible\n",
        "\n",
        "Other ways to make your Spark jobs faster [source](https://sparkbyexamples.com/spark/spark-performance-tuning/):\n",
        "\n",
        "* Cache/persist your data into memory\n",
        "* Using Spark DataFrames over Spark RDDs\n",
        "* Using Spark SQL functions before jumping into UDFs\n",
        "* Save to serialized data formats like Parquet\n",
        ":::\n",
        "\n",
        "## Pandas UDF\n",
        "\n",
        "From PySpark docs - Pandas UDFs are user defined functions that are executed by Spark using Arrow to transfer data and Pandas to work with the data, which allows vectorized operations. A Pandas UDF is defined using the pandas_udf as a decorator or to wrap the function, and no additional configuration is required. A Pandas UDF behaves as a regular PySpark function API in general.\n"
      ],
      "id": "2fbcd9c5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@pandas_udf(\"string\")\n",
        "def to_upper(s: pd.Series) -> pd.Series:\n",
        "    return s.str.upper()\n",
        "\n",
        "df = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\n",
        "df.select(to_upper(\"name\")).show()\n",
        "+--------------+\n",
        "|to_upper(name)|\n",
        "+--------------+\n",
        "|      JOHN DOE|\n",
        "+--------------+"
      ],
      "id": "64c614c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@pandas_udf(\"first string, last string\")\n",
        "def split_expand(s: pd.Series) -> pd.DataFrame:\n",
        "    return s.str.split(expand=True)\n",
        "\n",
        "\n",
        "df = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\n",
        "df.select(split_expand(\"name\")).show()\n",
        "+------------------+\n",
        "|split_expand(name)|\n",
        "+------------------+\n",
        "|       [John, Doe]|\n",
        "+------------------+"
      ],
      "id": "c89d9539",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.pandas_udf.html](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.pandas_udf.html)\n",
        "\n",
        "# Assumption: knowledge of ML techniques and process {.section}\n",
        "\n",
        "## Overview of ML\n",
        "\n",
        "::: {.imgcenter}\n",
        "<img src=\"img/ml_map.png\" width=800>\n",
        ":::\n",
        "\n",
        "\n",
        "## The advanced analytics/ML process\n",
        "\n",
        "::: {.column width=\"25%\"}\n",
        "1. Gather and collect data\n",
        "\n",
        "1. Clean and inspect data\n",
        "\n",
        "1. Perform feature engineering\n",
        "\n",
        "1. Split data into train/test\n",
        "\n",
        "1. Evaluate and compare models\n",
        ":::\n",
        "\n",
        "::: {.column width=\"5%\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"68%\"}\n",
        "<img src=\"img/spdg_2401.png\">\n",
        ":::\n",
        "\n",
        "\n",
        "# ML with Spark\n",
        "\n",
        "## What is `MLlib`\n",
        "\n",
        "::: {.column width=\"49%\"}\n",
        "#### Capabilities\n",
        "\n",
        "- Gather and clean data\n",
        "- Perform feature engineering\n",
        "- Perform feature selection\n",
        "- Train and tune models\n",
        "- Put models in production\n",
        "\n",
        "#### API is divided into two packages\n",
        "\n",
        "`org.apache.spark.ml` (High level API)\n",
        "- Provides higher level API built on top of `DataFrames`\n",
        "- Allows the construction of ML _pipelines_\n",
        "\n",
        "`org.apache.spark.mllib` (Predates `DataFrames`)\n",
        "- Original API built on top of RDDs\n",
        ":::\n",
        "\n",
        "::: {.column width=\"49%\"}\n",
        "#### Inspired by `scikit-learn`\n",
        "\n",
        "* `DataFrame`\n",
        "\n",
        "* `Transformer`\n",
        "\n",
        "* `Estimator`\n",
        "\n",
        "* `Pipeline`\n",
        "\n",
        "* `Parameter`\n",
        ":::\n",
        "\n",
        "\n",
        "# Transformers, esimators, and pipelines{.section}\n",
        "\n",
        "::: {.imgcenter}\n",
        "<img src=\"img/spdg_2402.png\" width=800>\n",
        ":::\n",
        "\n",
        "\n",
        "## Transformers\n",
        "\n",
        "::: {.column width=\"49%\"}\n",
        "`Transformers` take `DataFrames` as _input_, and return a **new DataFrame** as output. `Transformers` do not learn any parameters from the data, they simply apply rule-based transformations to either **prepare data for model training** or **generate predictions using a trained model.**  \n",
        "\n",
        "`Transformers` are run using the `.transform()` method\n",
        ":::\n",
        "\n",
        "::: {.column width=\"49%\"}\n",
        "<img src=\"img/spdg_2403.png\">\n",
        ":::\n",
        "\n",
        "## Some Examples of Transformers\n",
        "\n",
        "* Converting categorical variables to numeric (must do this)\n",
        "  * `StringIndexer`\n",
        "  * `OneHotEncoder` can act on multiple columns at a time \n",
        "\n",
        "* Data Normalization\n",
        "  * `Normalizer`\n",
        "  * `StandardScaler`\n",
        "\n",
        "* String Operations\n",
        "  * `Tokenizer`\n",
        "  * `StopWordsRemover`\n",
        "  * `PCA`\n",
        "  \n",
        "* Converting continuous to discrete\n",
        "  * `Bucketizer`\n",
        "  * `QuantileDiscretizer`\n",
        "  \n",
        "* Many more\n",
        "  * Spark 2.4.7: http://spark.apache.org/docs/2.4.7/ml-guide.html\n",
        "  * Spark 3.1.1: http://spark.apache.org/docs/3.1.1/ml-guide.html  \n",
        "\n",
        "\n",
        "\n",
        "## `MLlib` algorithms for machine learning models need **single, numeric features column** as input\n",
        "\n",
        "Each row in this column contains a vector of data points corresponding to the set of features used for prediction.\n",
        "\n",
        "* **Use the `VectorAssembler` transformer to create a single vector column from a list of columns.**\n",
        "\n",
        "* **All categorical data needs to be numeric for machine learning**\n"
      ],
      "id": "e8db1dfb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example from Spark docs\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "dataset = spark.createDataFrame(\n",
        "    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],\n",
        "    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"hour\", \"mobile\", \"userFeatures\"],\n",
        "    outputCol=\"features\")\n",
        "\n",
        "output = assembler.transform(dataset)\n",
        "print(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\n",
        "output.select(\"features\", \"clicked\").show(truncate=False)"
      ],
      "id": "250e7737",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimators\n",
        "\n",
        "::: {.column width=\"49%\"}\n",
        "`Estimators` learn (or \"fit\") parameters from your DataFrame via the `.fit()` method, and return a **model** which is a `Transformer`\n",
        ":::\n",
        "\n",
        "::: {.column width=\"49%\"}\n",
        "<img src=\"img/spdg_2502.png\">\n",
        ":::\n",
        "\n",
        "\n",
        "## Pipelines\n",
        "\n",
        "::: {.imgcenter}\n",
        "<img src=\"img/pipeline-oriented.png\" width=800>\n",
        ":::\n",
        "\n",
        "\n",
        "## Pipelines\n",
        "\n",
        "::: {.column width=\"49%\"}\n",
        "`Pipelines` combine multiple steps into a single workflow that can be easily run.\n",
        "* Data cleaning and feature processing via transformers, using `stages`\n",
        "* Model definition\n",
        "* Run the pipeline to do all transformations and fit the model\n",
        "\n",
        "The `Pipeline` constructor takes an _array_ of pipeline stages\n",
        ":::\n",
        "\n",
        "::: {.column width=\"49%\"}\n",
        "<img src=\"img/spdg_2404.png\">\n",
        ":::\n",
        "\n",
        "\n",
        "## Why Pipelines?\n",
        "\n",
        "1. **Cleaner Code:** Accounting for data at each step of preprocessing can get messy. With a Pipeline, you won’t need to manually keep track of your training and validation data at each step.\n",
        "\n",
        "1. **Fewer Bugs:** There are fewer opportunities to misapply a step or forget a preprocessing step.\n",
        "\n",
        "1. **Easier to Productionize:** It can be surprisingly hard to transition a model from a prototype to something deployable at scale, but Pipelines can help.\n",
        "\n",
        "1. **More Options for Model Validation:** We can easily apply cross-validation and other techniques to our Pipelines.\n",
        "\n",
        "\n",
        "## Example\n",
        "\n",
        "Using the [HMP Dataset](https://github.com/wchill/HMP_Dataset). The structure of the dataset looks like this:\n"
      ],
      "id": "86a34f01"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# read data from text file and split each line into words\n",
        "df.printSchema()\n",
        "\n",
        "root  \n",
        "|-- x: integer (nullable = true)  \n",
        "|-- y: integer (nullable = true)  \n",
        "|-- z: integer (nullable = true)  \n",
        "|-- class: string (nullable = false)  \n",
        "|-- source: string (nullable = false)"
      ],
      "id": "98d55270",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's transform strings to numbers\n",
        "\n",
        "* The `StringIndexer` is an _estimator_ having both `fit` and `transform` methods. \n",
        "* To create a StringIndexer object (indexer), pass the `class` column as inputCol, and `classIndex` as outputCol. \n",
        "* Then we fit the DataFrame to the indexer (to run the estimator), and transform the DataFrame. This creates a brand new DataFrame (indexed), which we can see below, containing the classIndex additional column.\n"
      ],
      "id": "fef395ad"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexer = StringIndexer(inputCol = 'class', outputCol = 'classIndex')\n",
        "indexed = indexer.fit(df).transform(df)  # This is a new data frame\n",
        "\n",
        "# Let's see it\n",
        "indexed.show(5)\n",
        "\n",
        "+---+---+---+-----------+--------------------+----------+\n",
        "|  x|  y|  z|      class|              source|classIndex|\n",
        "+---+---+---+-----------+--------------------+----------+\n",
        "| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|\n",
        "| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|\n",
        "| 28| 38| 52|Drink_glass|Accelerometer-201...|       2.0|\n",
        "| 29| 37| 51|Drink_glass|Accelerometer-201...|       2.0|\n",
        "| 30| 38| 52|Drink_glass|Accelerometer-201...|       2.0|\n",
        "+---+---+---+-----------+--------------------+----------+\n",
        "only showing top 5 rows"
      ],
      "id": "f517641b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## One-Hot Encode categorical variables\n",
        "\n",
        "* Unlike the `StringIndexer`, `OneHotEncoder` is a pure _transformer_, having only the `transform` method. It uses the same syntax of `inputCol` and `outputCol`.\n",
        "* `OneHotEncoder` creates a brand new DataFrame (encoded), with a `category_vec` column added to the previous DataFrame(indexed).\n",
        "* `OneHotEncoder` doesn't return several columns containing only zeros and ones; it returns a sparse-vector as seen in the categoryVec column. Thus, for the ‘Drink_glass’ class above, SparkML returns a sparse-vector that basically says there are 13 elements, and at position 2, the class value exists(1.0).\n"
      ],
      "id": "72193ed7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "\n",
        "# The OneHotEncoder is a pure transformer object. it does not use the fit()\n",
        "encoder = OneHotEncoder(inputCol = 'classIndex', outputCol = 'categoryVec')\n",
        "encoded = encoder.transform(indexed)  # This is a new data frame\n",
        "encoded.show(5, False)\n",
        "\n",
        "+---+---+---+-----------+----------------------------------------------------+----------+--------------+\n",
        "|x  |y  |z  |class      |source                                              |classIndex|categoryVec   |\n",
        "+---+---+---+-----------+----------------------------------------------------+----------+--------------+\n",
        "|29 |39 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n",
        "|29 |39 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n",
        "|28 |38 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n",
        "|29 |37 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n",
        "|30 |38 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n",
        "+---+---+---+-----------+----------------------------------------------------+----------+--------------+\n",
        "only showing top 5 rows"
      ],
      "id": "8f4c6616",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assembling the feature vector\n",
        "\n",
        "* The `VectorAssembler` object gets initialized with the same syntax as `StringIndexer` and `OneHotEncoder`. \n",
        "* The list `['x', 'y', 'z']` is paseed to `inputCols`, and we specify outputCol = ‘features’. \n",
        "* This is also a pure _transformer_.\n"
      ],
      "id": "4bf23eb0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "# VectorAssembler creates vectors from ordinary data types for us\n",
        "\n",
        "vectorAssembler = VectorAssembler(inputCols = ['x','y','z'], outputCol = 'features')\n",
        "# Now we use the vectorAssembler object to transform our last updated dataframe\n",
        "\n",
        "features_vectorized = vectorAssembler.transform(encoded)  # note this is a new df\n",
        "\n",
        "features_vectorized.show(5, False)\n",
        "\n",
        "+---+---+---+-----------+----------------------------------------------------+----------+--------------+----------------+\n",
        "|x  |y  |z  |class      |source                                              |classIndex|categoryVec   |features        |\n",
        "+---+---+---+-----------+----------------------------------------------------+----------+--------------+----------------+\n",
        "|29 |39 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[29.0,39.0,51.0]|\n",
        "|29 |39 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[29.0,39.0,51.0]|\n",
        "|28 |38 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[28.0,38.0,52.0]|\n",
        "|29 |37 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[29.0,37.0,51.0]|\n",
        "|30 |38 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[30.0,38.0,52.0]|\n",
        "+---+---+---+-----------+----------------------------------------------------+----------+--------------+----------------+\n",
        "only showing top 5 rows"
      ],
      "id": "b873c83a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalizing the dataset \n",
        "\n",
        "* `Normalizer` like all transformers have consistent syntax. Looking at the `Normalizer` object, it contains the parameter `p=1.0` (the default norm value for Pyspark Normalizer is `p=2.0`.)\n",
        "* `p=1.0` uses Manhattan Distance\n",
        "* `p=2.0` uses Euclidean Distance\n"
      ],
      "id": "31c46dcc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.ml.feature import Normalizer\n",
        "normalizer = Normalizer(inputCol = 'features', outputCol = 'features_norm', p=1.0)  # Manhattan Distance\n",
        "normalized_data = normalizer.transform(features_vectorized) # New data frame too.\n",
        "\n",
        "normalized_data.show(5)\n",
        ">>\n",
        "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n",
        "|  x|  y|  z|      class|              source|classIndex|   categoryVec|        features|       features_norm|\n",
        "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n",
        "| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,51.0]|[0.24369747899159...|\n",
        "| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,51.0]|[0.24369747899159...|\n",
        "| 28| 38| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[28.0,38.0,52.0]|[0.23728813559322...|\n",
        "| 29| 37| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,37.0,51.0]|[0.24786324786324...|\n",
        "| 30| 38| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[30.0,38.0,52.0]|[0.25,0.316666666...|\n",
        "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n",
        "only showing top 5 rows"
      ],
      "id": "579c194e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running the transformation `Pipeline`\n",
        "\n",
        "The `Pipeline` constructor below takes an array of Pipeline stages we pass to it. Here we pass the 4 stages defined earlier, in the right sequence, one after another.\n"
      ],
      "id": "e6dedb78"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline(stages = [indexer,encoder,vectorAssembler,normalizer])"
      ],
      "id": "6c5cecde",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the `Pipeline` object fit to our original `DataFrame df` \n"
      ],
      "id": "fba15407"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data_model = pipeline.fit(df)"
      ],
      "id": "44f2f946",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we transform our DataFrame `df` using the Pipeline Object.\n"
      ],
      "id": "625477b4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pipelined_data = data_model.transform(df)\n",
        "\n",
        "pipelined_data.show(5)\n",
        "\n",
        "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n",
        "|  x|  y|  z|      class|              source|classIndex|   categoryVec|        features|       features_norm|\n",
        "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n",
        "| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,51.0]|[0.24369747899159...|\n",
        "| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,51.0]|[0.24369747899159...|\n",
        "| 28| 38| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[28.0,38.0,52.0]|[0.23728813559322...|\n",
        "| 29| 37| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,37.0,51.0]|[0.24786324786324...|\n",
        "| 30| 38| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[30.0,38.0,52.0]|[0.25,0.316666666...|\n",
        "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n",
        "only showing top 5 rows"
      ],
      "id": "cfe6d9cd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## One last step before training\n"
      ],
      "id": "86a8ef6d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# first let's list out the columns we want to drop\n",
        "cols_to_drop = ['x','y','z','class','source','classIndex','features']\n",
        "\n",
        "# Next let's use a list comprehension with conditionals to select cols we need\n",
        "selected_cols = [col for col in pipelined_data.columns if col not in cols_to_drop]\n",
        "\n",
        "# Let's define a new train_df with only the categoryVec and features_norm cols\n",
        "df_train = pipelined_data.select(selected_cols)\n",
        "\n",
        "# Let's see our training dataframe.\n",
        "df_train.show(5)\n",
        "\n",
        "+--------------+--------------------+\n",
        "|   categoryVec|       features_norm|\n",
        "+--------------+--------------------+\n",
        "|(13,[2],[1.0])|[0.24369747899159...|\n",
        "|(13,[2],[1.0])|[0.24369747899159...|\n",
        "|(13,[2],[1.0])|[0.23728813559322...|\n",
        "|(13,[2],[1.0])|[0.24786324786324...|\n",
        "|(13,[2],[1.0])|[0.25,0.316666666...|\n",
        "only showing top 5 rows"
      ],
      "id": "1554c046",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### You now have a DataFrame optimized for SparkML!\n",
        "\n",
        "## Machine Learning Models in Spark\n",
        "\n",
        "There are many [Spark machine learning models](https://spark.apache.org/docs/3.1.1/ml-classification-regression.html)\n",
        "\n",
        "#### Regression\n",
        "- Linear regression - what is the optimization method?\n",
        "\n",
        "- Survival regression\n",
        "\n",
        "- Genearlized linear model (GLM)\n",
        "\n",
        "- Random forest regression\n",
        "\n",
        "\n",
        "#### Classification\n",
        "- Logistic regression\n",
        "- Gradient-boosted tree model (GBM)\n",
        "- Naive Bayes\n",
        "- Multilayer perception (neural network!)\n",
        "\n",
        "#### Other\n",
        "- Clustering (K-means, LDA, GMM)\n",
        "- Association rule mining\n",
        "\n",
        "\n",
        "## Building your Model\n",
        "\n",
        "Feature selection using easy R-like formulas\n"
      ],
      "id": "0715303c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example from Spark docs\n",
        "from pyspark.ml.feature import RFormula\n",
        "\n",
        "dataset = spark.createDataFrame(\n",
        "    [(7, \"US\", 18, 1.0),\n",
        "     (8, \"CA\", 12, 0.0),\n",
        "     (9, \"NZ\", 15, 0.0)],\n",
        "    [\"id\", \"country\", \"hour\", \"clicked\"])\n",
        "\n",
        "formula = RFormula(\n",
        "    formula=\"clicked ~ country + hour\",\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\")\n",
        "\n",
        "output = formula.fit(dataset).transform(dataset)\n",
        "output.select(\"features\", \"label\").show()"
      ],
      "id": "38b36ffe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What if you have too many columns for your model?\n",
        "\n",
        "Evaluating 1,000s or 10,000s of variables?\n",
        "\n",
        "## Chi-Squared Selector\n",
        "\n",
        "Pick categorical variables that are most dependent on the response variable\n",
        "\n",
        "Can even check the [p-values of specific variables!](https://stackoverflow.com/questions/50971964/pyspark-chisqselector-p-values-and-test-statistics)\n"
      ],
      "id": "e4249d83"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example from Spark docs\n",
        "from pyspark.ml.feature import ChiSqSelector\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (7, Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0,),\n",
        "    (8, Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0,),\n",
        "    (9, Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0,)], [\"id\", \"features\", \"clicked\"])\n",
        "\n",
        "selector = ChiSqSelector(numTopFeatures=1, featuresCol=\"features\",\n",
        "                         outputCol=\"selectedFeatures\", labelCol=\"clicked\")\n",
        "\n",
        "result = selector.fit(df).transform(df)\n",
        "\n",
        "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
        "result.show()"
      ],
      "id": "222ea198",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuning your model\n",
        "\n",
        "#### Part 1\n"
      ],
      "id": "c771a89f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example from Spark docs\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
        "\n",
        "# Prepare training and test data.\n",
        "data = spark.read.format(\"libsvm\")\\\n",
        "    .load(\"data/mllib/sample_linear_regression_data.txt\")\n",
        "train, test = data.randomSplit([0.9, 0.1], seed=12345)\n",
        "\n",
        "lr = LinearRegression(maxIter=10)\n",
        "\n",
        "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
        "# TrainValidationSplit will try all combinations of values and determine best model using\n",
        "# the evaluator.\n",
        "paramGrid = ParamGridBuilder()\\\n",
        "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
        "    .addGrid(lr.fitIntercept, [False, True])\\\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
        "    .build()"
      ],
      "id": "0c7a73ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuning your model\n",
        "\n",
        "#### Part 2\n"
      ],
      "id": "723e534c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# In this case the estimator is simply the linear regression.\n",
        "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
        "tvs = TrainValidationSplit(estimator=lr,\n",
        "                           estimatorParamMaps=paramGrid,\n",
        "                           evaluator=RegressionEvaluator(),\n",
        "                           # 80% of the data will be used for training, 20% for validation.\n",
        "                           trainRatio=0.8)\n",
        "\n",
        "# Run TrainValidationSplit, and choose the best set of parameters.\n",
        "model = tvs.fit(train)\n",
        "\n",
        "# Make predictions on test data. model is the model with combination of parameters\n",
        "# that performed best.\n",
        "model.transform(test)\\\n",
        "    .select(\"features\", \"label\", \"prediction\")\\\n",
        "    .show()"
      ],
      "id": "582577c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab {.section}\n"
      ],
      "id": "9b874ccb"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/ubuntu/repos/6000-fall-2025/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}