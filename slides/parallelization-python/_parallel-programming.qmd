# Parallel Programming {.section}

## Linear vs. Parallel

::: columns
::: {.column width="47%"}
### Linear/Sequential

1.  A program starts to run
2.  The program issues an instruction
3.  The instruction is executed
4.  Steps 2 and 3 are repeated
5.  The program finishes running
:::

::: {.column width="47%"}
### Parallel

1.  A program starts to run
2.  The program divides up the work into chunks of instructions and data
3.  Each chunk of work is executed independently
4.  The chunks of work are reassembled
5.  The program finishes running
:::
:::

## Linear vs. Parallel

::: columns
::: {.column width="47%"}
<img src="img/linear.png" width="400"/>
:::

::: {.column width="47%"}
<img src="img/parallel.png" width="400"/>
:::
:::

## Linear vs. Parallel

From a data science perspective

::: columns
::: {.column width="47%"}
### Linear

-   The data remains monolithic
-   Procedures act on the data sequentially
    -   Each procedure has to complete before the next procedure can start
-   You can think of this as a single pipeline
:::

::: {.column width="47%"}
### Parallel

-   The data can be split up into chunks
-   The same procedures can be run on each chunk at the same time
-   Or, independent procedures can run on different chunks at the same time
-   Need to bring things back together at the end
:::
:::

::: fragment
### What are some examples of linear and parallel data science workflows?
:::

## Embarrasingly Parallel

It's **easy** to speed things up when:

-   You need to calculate the same thing many times
-   Calculations are **independent** of each other
-   Each calculation takes a decent amount of time

Just run **multiple calculations at the same time**

## Embarrasingly Parallel

The concept is based on the old middle/high school math problem:

> If 5 people can shovel a parking lot in 6 hours, how long will it take 100 people to shovel the same parking lot?

Basic idea is that many hands (cores/instances) make lighter (faster/more efficient) work of the same problem, as long as the effort can be split up appropriately into nearly equal parcels

::: {.aside}
The classical answer to the problem is 18 minutes
:::

## Embarassingly parallel 

::: {.incremental}

- If you can truly split up your problem into multiple **independent** parts, then you can
often get linear speedups with the number of parallel components (to a limit)
  - The more cores you use and the more you parallelize, the more you incur communication overhead and decrease available RAM, so the speedup is almost certainly sub-linear, i.e. for a 4-core machine you'll probably get a 3-4x speedup, but rarely a full 4x speedup^[Gorelick & Ozsvald, 2020. *High Performance Python*, O'Reilly]
- The question often is, which part of your problem is embarassingly parallel?
- Amdahl's law (which we'll see in a few slides) shows how parallelization can benefit overall if a large proportion of the problem is parallelizable
- It's not all milk and honey. Setting up, programming, evaluating, debugging parallel computations requires better infrastructure and more expertise.

:::

# When might parallel programming not be more efficient? {.section}

## Some limitations

#### You can get speedups by parallelizing computations, but

- Having to transport data between parallel processes (memory bottlenecks)  and communication between processes (I/O bottlenecks) can make things more expensive and can exceed the benefits of parallelization
- If you're moving a lot of data but not doing a lot of parallel computing, it's often not worth the effort

::: {.fragment}

#### Setting up and debugging parallel programs can be difficult

But this has become easier with better software, like the `multiprocessing` module in Python

:::

::: {.fragment}
#### Making sure that we can get back all the pieces needs monitoring

- Failure tolerance and protections (Hadoop, e.g.)
- Proper collection and aggregation of the processed data

:::

## Amdahl's Law

::: {.column width="50%"}
::: {.imgcenter }
![](img/AmdahlsLaw.svg)
:::
:::

::: {.column width="45%"}
$$
\lim_{s\rightarrow\infty} S_{latency} = \frac{1}{1-p}
$$

where $s$ is the speedup of that part of the task (which is $p$ proportion of the overall task) benefitting from improved resources.

:::

If 50% of the task is embarassingly parallel, you can get a maximum speedup of 2-fold, while if 90% is embarassingly parallel, you can get a maximum speedup of $1/(1-0.9) = 10$ fold. 

::: {.aside}
By <a href="https://en.wikipedia.org/wiki/User:Daniels220" class="extiw" title="wikipedia:User:Daniels220">Daniels220</a> at <a href="https://en.wikipedia.org/wiki/" class="extiw" title="wikipedia:">English Wikipedia</a>, <a href="https://creativecommons.org/licenses/by-sa/3.0" title="Creative Commons Attribution-Share Alike 3.0">CC BY-SA 3.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=6678551">Link</a>
:::

## Pros and cons of parallelization

::: {.column width="47%"}
### Yes

-   Group by analysis
-   Simulations
-   Resampling / Bootstrapping
-   Optimization
-   Cross-validation
-   Training bagged models (like Random Forests)
-   Multiple chains in a Bayesian MCMC
-   Scoring (predicting) using trained models
:::

::: {.column width="47%"}
::: fragment
### No

-   SQL Operations
-   Inverting a matrix
-   Training linear regression
-   Training logistic regression
-   Training trees
-   Training neural nets
-   Training boosted models (like gradient boosted trees)
-   Each chain in a Bayesian MCMC
-   Most things time series
:::
:::

::: {.fragment}
::: {.callout-note appearance="simple"}
For processes in the "No" column, each step depends on a previous step, and so they cannot be parallelized. However, there are approximate numerical methods applicable to big data which are parallelizable and get you to the right answer, based on parallely taking random subsets of the data. We'll see some of these when we look at Spark ML
:::
:::

## Pros and cons of parallelization

::: columns
::: {.column width="47%"}
### Pros

-   Higher efficiency

-   Using modern infrastructure

-   Scalable to larger data, more complex procedures

    -   *proviso* procedures are embarassingly parallel
:::

::: {.column .right width="47%"}
::: fragment
### Cons

-   Higher programming complexity
-   Need proper software infrastructure (MPI, Hadoop, etc)
-   Need to ensure right packages/modules are distributed across processors
-   Need to account for a proportion of jobs failing, and recovering from them
-   Hence, Hadoop/Spark and other technologies
-   Higher setup cost in terms of time/expertise/money
:::
:::
:::

::: fragment
There are good solutions today for most of the cons, so the pros have it and so this paradigm is widely accepted and implemented
:::
