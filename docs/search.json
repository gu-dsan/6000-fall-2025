[
  {
    "objectID": "tech-ref/terminal-and-ssh.html",
    "href": "tech-ref/terminal-and-ssh.html",
    "title": "Terminal and SSH keys",
    "section": "",
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet."
  },
  {
    "objectID": "tech-ref/terminal-and-ssh.html#set-up-your-terminal",
    "href": "tech-ref/terminal-and-ssh.html#set-up-your-terminal",
    "title": "Terminal and SSH keys",
    "section": "Set up your terminal",
    "text": "Set up your terminal\nWe will be using the Terminal in many tasks in this course. By “Terminal”, we mean a command-line terminal, where you will use it to type instructions and connect to your remote resources.\nWhen we say “terminal” in this course, it means that you need to open up the terminal application for your respective operating system. The terminal is a “local” application, meaning it is running on your “local” machine (your laptop.)\n\nWindows Users\nWindows users will be using Windows Powershell.\n\n\n\n\n\n\nImportant\n\n\n\nMake sure you have the latest version of Windows 10 or Windows 11\n\n\nWindows Powershell is most likely installed if you have Windows 10. If you don’t have Powershell, take a look at this article that explains how to install it.\nYou can find Powershell by typing “Powershell” into the search bar:\n\nOnce Powershell is running, this is your terminal:\n\n\nAdditional Powershell Configuration\n\n\n\n\n\n\nImportant\n\n\n\nWindows users must do this step, and you only need to do this once. You need to perform this to be able to use agent forwarding which is explained further.\n\n\n\nExit Powershell if running\nStart a new Powershell session using run as Administrator\nEnter the following command (you can cut/paste from here):\nGet-Service -Name ssh-agent | Set-Service -StartupType Manual\nExit Powershell. You should not need to run as administrator going forward.\n\n\n\n\nMac and Linux Users\nFor Mac and Linux users, you will open up the Terminal.\n\nMacs and Linux machines have a built in Terminal.\nOr, you can use iTerm app"
  },
  {
    "objectID": "tech-ref/terminal-and-ssh.html#create-your-ssh-keypair-on-your-laptop",
    "href": "tech-ref/terminal-and-ssh.html#create-your-ssh-keypair-on-your-laptop",
    "title": "Terminal and SSH keys",
    "section": "Create your ssh keypair on your laptop",
    "text": "Create your ssh keypair on your laptop\nWhen you want to connect to a remote machine, the method is called “Secure Shell”. This creates a connection between the local machine (where your terminal window lives) and the “remote” machine (where the commands you will send actually execute). In order for the local and remote machines to authenticate (trust) each other, we have to create a special password-like files called a keypair. It is called a keypair because there is a public version and a private version. Read more about SSH Keys.\n\n\n\n\n\n\nNote\n\n\n\nNOTE: You only need to create your ssh public/private keypair one time only. If you already have a public/private keypair on your laptop let us know.\n\n\n\nOpen a terminal (on your laptop) if not already open. By default, every time you open a terminal it will open in your home directory.\nAt the command prompt run the following command: ssh-keygen -t rsa -b 4096 and press enter\nYou will see this prompt, just press enter\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/User/.ssh/id_rsa):\nYou will see this prompt, just press enter\nCreated directory '/home/User/.ssh'.\nEnter passphrase (empty for no passphrase):\nYou will see this prompt, just press enter\nEnter same passphrase again:\nYou will see these messages (your randomart will look different) and your keypair has been created.\nYour identification has been saved in /home/User_name/.ssh/id_rsa.\nYour public key has been saved in /home/User_name/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:xPJtMLmJSO73x/NQo3qMpqF6r6St4ONmshS8QZqfmHA User_name@WinDev1802Eval\n\nThe key's randomart image is:\n+---[RSA 2048]----+\n|                 |\n|       . .       |\n| .  . . *        |\n|+. o . = *       |\n|++E o . S o o    |\n|.=+o     . o .   |\n|+oo o o  +o      |\n|+= +.o oo.*.     |\n|*+=++ooooo o.    |\n+----[SHA256]-----+\n\n\nSee your key files\n\nOpen a terminal if not already open\nChange to your .ssh directory\n\nThis is a hidden directory so if you list your files using ls you won’t see it. For seeing all files, use ls -la.\nTo change into the .ssh directory type cd .ssh\n\nType pwd to print your current working directory.\n\nWindows users in Powershell will see:\nPS C:\\\\Users\\\\your_name\\.ssh&gt; pwd\n\nPath\n----\nC:\\\\Users\\\\your_name\\.ssh\n\n\nPS C:\\\\Users\\\\your_name\\.ssh&gt;\nMac users will see:\npwd\n/Users/myusername/.ssh\nLinux users will see:\n$ pwd\n\n/home/myusername/.ssh\n\n\nNext, we need to open the new key file we just made.\n\nType ls to list the files in the directory.\n\nWhat is displayed may look different. You will not have a config file unless you have already created one.\n\nType ls -la to list all the files in the directory, even the hidden ones.\n\nWhat is displayed may look different. You will not have a config file unless you have already created one.\n\n\n\n\nGet your public key info\n\nThe file id_rsa is your private key and this file will not leave your computer.\nThe file id_rsa.pub is the public key, whose contents we will upload to cloud services so you authenticate.\nThe known_hosts is a file that gets generated as you connect to different remote systems.\n\nThis is useful so you know you are connecting to the same server as previous times.\n\n\\$ ls -la\n\ntotal 32\n\ndrwxr-xr-x  6  your_name staff   192 May 29 20:39 .\ndrwxr-xr-x+ 75 your_name staff  2400 May 30 13:35 ..\n-rw-r--r--  1  your_name staff   181 May 29 15:50 config\n-r--------  1  your_name staff  3243 May 29 15:50 id_rsa\n-rw-r--r--  1  your_name staff   742 May 29 15:50 id_rsa.pub\n-rw-r--r--  1  your_name staff   363 May 29 20:42 known_hosts\nView the contents of your public_key file by running the command cat id_rsa.pub\n\nWhat is shown is a sample public key, yours will be different\n\n\\$ cat id_rsa.pub\n\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCnKuIRXwZu0JZH0/Q2XNrYYTaJT7bMtXGhGQaSSOZs6MhQ4SkSbHiygO7RauQf741buLnASzY27GKMMMml6InwfxJWrF60KhNK0r869POQkuZa9v9/cmYcEIzmAJe1xRPABEZ2yfbTG9Wq4sg9cU0mwt1Bx7wiN4QNf0Bak62EC8JWTbcKLduuzO1zabIb5xW9gfR9b4K3HwmqRLl18S8bNsfYQZfvtlwd0mCWQUeuEGbDOgqh//nLIj6DeXdyxbD5xrz79iOAuAK2nXAjNCEtKpxNGQr2Py7aWQjlH+U5laDEHVg4hzmBY7yoZ5eC3Ye45yPqpQA1y8JrbXVhPJRP User\\@WinDev1802Eval\n\n\n\nExtract your public key\n\nOpen a text editor (Notepad on Windows or Textpad on Mac, NOT MICROSOFT WORD) and select the output of your terminal with all the text from the ssh-rsa beginning all the way to the end, and paste it in your text editor as-is. We will use this in the next step.\n\nYou can also just copy/paste from your terminal screen.\nOn a Mac, you can also copy the contents of the id_rsa.pub file using\n\npbcopy &lt; id_rsa.pub"
  },
  {
    "objectID": "tech-ref/terminal-and-ssh.html#add-your-public-ssh-key-to-github",
    "href": "tech-ref/terminal-and-ssh.html#add-your-public-ssh-key-to-github",
    "title": "Terminal and SSH keys",
    "section": "Add your public SSH Key to GitHub",
    "text": "Add your public SSH Key to GitHub\n\nCreate a GitHub Account if you do not already have one\nGo to www.github.com to create a GitHub account if you do not already have one. Your username has to be globally unique, and the email address you use to register GitHub can be any email address you own.\nUpload your Public key to GitHub\n\nLog into to your GitHub account if you are not already logged in\nClick on your profile icon on the top-right of the screen and select Settings from the dropdown\nClick on SSH and GPG keys from the left hand menu\nClick on the New SSH key button on the top-right\nGive your key a name. This is just a name and is meaningful to you.\nPaste the contents of the public key in the Key box. Leave the “Key Type” dropdown as “Authentication Key”.\nClick the Add SSH Key button\n\nTest that your ssh key works with GitHub\n\nOpen a terminal if not already open on your laptop\nAt the command prompt, type ssh -T git@github.com and press enter to test. If it works, you will see something like this, with your GitHub username:\nThe authenticity of host 'github.com (192.30.253.112)' can't be established.\nRSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8.\nAre you sure you want to continue connecting (yes/no)? yes\nWarning: Permanently added 'github.com,192.30.253.112' (RSA) to the list of known hosts.\nHi wahalulu! You've successfully authenticated, but GitHub does not provide shell access.\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou are now ready to use ssh authentication with GitHub.\n\n\n\nCreate a Personal Access Token on GitHub\n\nLog into to your GitHub account if you are not already logged in\nClick on your profile icon on the top-right of the screen and select Settings from the dropdown\nClick Developer settings\nClick the Personal access tokens tab\nClick the Generate new token button\nEnter a token description (you can call it big-data-class)\nSelect the repo permission, and then click the Generate token button\n\nCopy the token and save it in a text file. You will need this token later on in the semester and if you lose it you will need to re-generate a token"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Amit Arora, Jeff Jacobs\n   Online\n   &lt;a href=“mailto:aa1603@georgetown.edu”&gt;aa1603@georgetown.edu\n   \n   Schedule an appointment\n\n\n\n\n\n   Wednesdays (Prof Amit), Tuesdays (Prof Jeff)\n   August 27 – December 20, 2025\n   9:30 AM–12:00 PM\n   Reiss 262 (W, Amit), TBD (Tu, Jeff)\n   Slack (Join)\nThis syllabus was last updated on Saturday, August 9, 2025 ‘amt’ 12:00 AM"
  },
  {
    "objectID": "syllabus.html#books-software-and-cloud-resources",
    "href": "syllabus.html#books-software-and-cloud-resources",
    "title": "Syllabus",
    "section": "Books, Software and Cloud Resources",
    "text": "Books, Software and Cloud Resources\n\nReadings (for assigned readings)\nThere is no required textbook for the course. We have selected specific chapters from several sources as well as several seminal papers in the big data space, and these will be provided to you in PDF format. We may also provide supplemental materials (articles, links, videos, etc.) to complement the readings. You must read assigned readings prior to the lectures.\n\n\nCloud Resources\nYou will use cloud resources on Amazon Web Services. We will discuss how to setup your accounts and environments in class and lab within the first couple of weeks. You will get credits on both platforms that will be enough to support your coursework throughout the semester.\n\n\nModules\n\n\n\n\n\n\n\n\nWeek\nModule\nDetails\n\n\n\n\n1\nCourse overview. Introduction to big data concepts.\nOverview of course objectives and an introduction to big data and Linux shell basics.\n\n\n2\nIntroduction to the cloud.\nIntroduction to cloud computing concepts and platforms like AWS and Azure.\n\n\n3\nParallelization (multiprocessing, asyncio)\nExploring parallel computing with Python using multiprocessing and asyncio for concurrency.\n\n\n4\nDuckDB, Polars, and file formats\nWorking with DuckDB, Polars, and the Parquet file format for efficient data processing.\n\n\n5\nData Warehouse\nUnderstanding data warehousing with Presto, Snowflake, and Athena; hands-on lab with Presto/Athena.\n\n\n6\nIntroduction to Spark, RDDs, and Dataframes\nIntroduction to Apache Spark, focusing on RDDs and DataFrames for distributed data processing.\n\n\n7\nSpark DataFrames and Spark SQL\nAdvanced use of Spark DataFrames and Spark SQL for structured data analysis.\n\n\n8\nSpark ML\nUtilizing Spark’s MLlib for scalable machine learning tasks.\n\n\n9\nSpark NLP\nApplying Spark for Natural Language Processing (NLP) tasks on large datasets.\n\n\n10\nSpark Streaming\nReal-time data processing with Spark Streaming for handling streaming data.\n\n\n11\nAccelerate Python workloads with Ray, RAPIDS\nLeveraging Ray and RAPIDS to accelerate Python-based data processing with distributed computing and GPUs.\n\n\n12\nVector databases\nIntroduction to vector databases and their use in managing large-scale AI-driven applications.\n\n\n13\nData engineering with serverless (Lambda)\nBuilding and deploying data engineering solutions using AWS Lambda and other serverless technologies.\n\n\n\nThanksgiving break\nNo class due to Thanksgiving break.\n\n\n14\nProject, open-discussion\nFinal project presentations and open discussion on course topics.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIT IS YOUR RESPONSIBILITY TO MANAGE THE CREDITS AND RESOURCES PROVIDED TO YOU. YOU MUST SHUT DOWN YOUR CLOUD RESOURCES WHEN NOT IN USE."
  },
  {
    "objectID": "syllabus.html#learning-activities-communication-and-evaluation",
    "href": "syllabus.html#learning-activities-communication-and-evaluation",
    "title": "Syllabus",
    "section": "Learning Activities, Communication and Evaluation",
    "text": "Learning Activities, Communication and Evaluation\nThis is a hands-on, practical, workshop style course that provides opportunities to use the tools and techniques discussed in class. Although this is not a programming course per se, there is programming involved.\n\nLectures and Labs\nThis course is split into a lecture/lab format, where every class session will have a lecture portion, and most sessions will have an in-class lab portion:\n\nDuring the lecture, we will discuss the concepts and techniques as well as the history and development of these big data tools and cloud platforms.\nDuring the lab sessions, you will be completing exercises and following examples which are designed to show you how to implement the ideas and concepts with various tools. We will start the labs in class but we will not finish. It is your responsibility to complete the labs (which is part of the grade) and will enable your learning.\n\nLectures may not cover all the material and some topics will be introduced in the lab or through readings/assignments.\n\n\nOffice Hours\nInstructors and TAs will hold recurring office hours to answer questions, review material, and support your learning. The times, dates, and location of office hours will be announced via Canvas and Slack in advance.\n\n\nReadings\nOn certain weeks, readings will be assigned to prepare you for the lecture material being presented. These readings should take an hour or less per week.\n\n\nOnline Quizzes\nQuizzes will be given a few times during the semester during lab or lecture at random intervals and times. Quizzes ensure you are keeping up with the material presented in the class. Quizzes are meant to be brief and low-stress with a time limit of 5-10 minutes. The material will be drawn from lectures, labs, and readings.\n\n\nLab Deliverables\nEach lab will have a deliverable. It is essential that you learn the skills presented in the labs so that you can effectively complete the assignments and the big data project. The lab deliverables can sometimes be completed during lab, however, it is your responsibility to complete the deliverable as part of your work outside of lecture/lab time.\n\n\nHomework Assignments\nYou will be several homework assignments for roughly half of the semester. The goal of these problem sets is to hone your big data skills by answering some questions about large datasets. The problem sets will build on the labs and will be much more in-depth. Deliverables from the assignment will usually include code written for your programs and the output produced.\n\n\n\n\n\n\nWarning\n\n\n\nPlease start assignments as soon as they are posted. These assignments can take several hours to complete depending on your familiarity with the material. You will not complete the assignments on time if you start the day they are due.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe reuse problem set questions, we expect students not to copy, refer to, or look at the solutions in preparing their answers. Since this is a graduate-level class, we expect students to want to learn and not search online for answers. See Academic Integrity section.\n\n\n\n\nBig Data Analytics Project\nStudents will assemble into groups of no more than 4 students. You will perform and write up an analysis of a big dataset using the tools learned in class. Big is defined as “a dataset that is so large that you cannot work with it on a laptop.”\nYou will each conduct big data project using Reddit textual data. This project will encompass all of the skills you will learn throughout the semester including data ingestion, transformation, analysis, natural language processing, and machine learning. Similar to previous core courses, the big data project will be a product you can share with the world to demonstrate your data science expertise. Intermediate project assignments will help you incrementally build towards your final portfolio. There will be assignments on exploratory data analysis, machine learning, and natural language processing. Each successive assignment and the final submissions will incorporate feedback from your peers, TAs, and professors. The final timelines and deliverables for the project will be announced in class."
  },
  {
    "objectID": "syllabus.html#grading-and-evaluation",
    "href": "syllabus.html#grading-and-evaluation",
    "title": "Syllabus",
    "section": "Grading and Evaluation",
    "text": "Grading and Evaluation\n\nGroup project : 40%\nAssignments : 30%\nLab completions : 20%\nQuizzes : 10%\n\nTotal is 100%. We have no plans to curve the final grade, and the final letter grade will be:\n\nA: &gt;= 92.5\nA-: 89.5 - 92.49\nB+: 87.99 - 89.49\nB: 81.5 - 87.98\nB-: 79.5 - 81.49"
  },
  {
    "objectID": "syllabus.html#submitting-your-work",
    "href": "syllabus.html#submitting-your-work",
    "title": "Syllabus",
    "section": "Submitting your work",
    "text": "Submitting your work\n\nGitHub classroom\nWe use Github Classroom for all class deliverables: assignments, labs, and the final project. Submitting your work is the process of committing your files and results to your local repository and then pushing it to GitHub.\n\n\n\n\n\n\nImportant\n\n\n\nYou must submit everything through GitHub!\n\n\n\nUse the final-submission commit message\nWhen you are ready for your work to be evaluated, you MUST use the commit message final-submission. If you do not use the commit message final-submission we will assume that you are still working in the repository and we will only grade what is present. By submitting that commit message, you are stating that you are finished with the assignment and are ready for feedback.\n\n\n\n\n\n\nImportant\n\n\n\nMake sure you understand the difference between a git commit and a push, and that you push your repository successfully to GitHub.\n\n\nIn case you need to make a correction after your final-submission and the submission deadline has not yet passed, then you can amend your previous commit. See amending a commit for instructions. Do not change the commit message, it should continue say “final-submission” after the amend.\n\n\n\n\n\n\nWarning\n\n\n\nNo further edits to your GitHub repository are allowed after using the final-submission commit message.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWe will use commit datetime and commit message to assess lateness.\n\n\n\n\n\nLate policy\nIn lieu of extensions, there is a tiered deduction scale if a deliverable is late. Late penalties only apply to labs and assignments.\nWe will assess exceptional circumstances on a case-by-case basis, and only if we are made aware before a deliverable’s deadline, not after.\n\nA late penalty of 10% per day, up to 4 days, will be assessed for assignments and labs that are submitted with a final-submission commit message after the deadline. You may still submit a missed lab or assignment up until the last day of class (Dec 20th, 2025) with a maximum possible grade of 60%.\nMissed in-class quizzes cannot be made up and will receive a grade of zero.\nProject deadlines are fixed and have no extensions or late penalty. A missed project deliverable will receive a grade of zero."
  },
  {
    "objectID": "syllabus.html#other-course-policies",
    "href": "syllabus.html#other-course-policies",
    "title": "Syllabus",
    "section": "Other course policies",
    "text": "Other course policies\n\nAttendance and punctuality\nAttendance is mandatory and will be taken. Given the technical nature of this course, and the breadth of topics discussed, you are expected to attend each class, to complete all readings, and to participate actively in lectures, discussions and exercises. We understand there may be times you may need to miss class, please inform us in advance if you are not able to attend class for any reason. However, it is up to you to keep up.\n\n\nParticipation\nWe love participation. Read. Raise your hand. Ask questions. Make comments. Challenge us. Acknowledge us. If we speak for three hours to a silent classroom, it is a lot more boring and tiring for everyone.\n\n\nLaptop and phone use\nYou must bring your laptop to class to work on labs. No phone use is allowed during lecture. You may use your laptop during lecture to take notes, but please refrain from other activities. We reserve the right to ask you to put your phones and laptops away. You may not use your computer or phone while your peers or guest speakers are presenting.\n\n\nCommunication and Slack Rules\n\nAll announcements will be posted on Canvas and Slack\nUse Slack for any question you may have about the course, about assignments or any technical issue. This way everyone can learn from each others questions. We will be monitoring and providing answers on a regular basis. Make sure you understand what is allowed in Slack.\nIndividual emails containing any course question that is not personal will not be answered\nSlack DMs are not to be used unless we DM you first and you can respond to our message. Students may not initiate DMs.\nKeep an eye on the questions posted in Slack. Use the search function. It’s very possible that we have already answered a question, and we reserve the right to point you to the syllabus, previous Slack messages, or other document containing the information requested\nAssignment, lab and project questions will only be answered on Slack up to 12 hours before something is due\n\n\n\nOpen Door Policy\nPlease approach or get in touch with us if something is not working for you regarding the class, methods, etc. Our pledge to you is to provide the best learning experience possible. If you have any issue please do not wait until the last minute to speak with us. You will find that we are fair, reasonable, and flexible and we care deeply about your learning and success."
  },
  {
    "objectID": "syllabus.html#academic-integrity",
    "href": "syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nAs a Jesuit, Catholic university, committed to the education of the whole person, Georgetown expects all members of the academic community, students and faculty, to strive for excellence in scholarship and in character.The University spells out the specific minimum standards for academic integrity in its Honor Code, as well as the procedures to be followed if academic dishonesty is suspected.\nOver and above the honor code, in this course we will seek to create an engaged and passionate learning environment, characterized by respect and courtesy in both our discourse and our ways of paying attention to one another.\nThe code of academic integrity applies to all courses at Georgetown University. Please become familiar with the code. All students are expected to maintain the highest level of academic integrity throughout the course of the semester.Please note that acts of academic dishonesty during the course will be prosecuted and harsh penalties may be sought for such acts. Students are responsible for knowing what acts constitute academic dishonesty. The code may be found at https://bulletin.georgetown.edu/regulations/honor/.\n\n\n\n\n\n\nCaution\n\n\n\nWe have a ZERO TOLERANCE POLICY and students found to be in violation will be reported and penalized. The consequences of any violation may include: additional points penalty, getting a grade of zero, automatically failing the course, and suspension or expulsion from the program.\n\n\n\nDefinition of collaboration\nIn the spirit of fostering a collective and inclusive learning environment, we acknowledge that you will work and study with your peers. We also acknowledge that you use web resources (code examples specifically), and that in writing a program many of you will most likely use the same libraries, functions and other similar instructions in your scripts. However:\n\nYou must write your own code. This will be verified for every assignment against every submission, and any similarity greater than 60% between students on a given assignment will be considered to be unauthorized collaboration.\nYou must do your individual work in your own cloud resources. This will be verified for every assignment. We know the fingerprint of your cloud account and subscriptions and we can tell.\n\n\n\nWhat is allowed\n\nCollaborating with other students during in-class labs to facilitate collective learning\nUsing Slack for helping one-another as long as:\n\nYou do not provide answers directly but only discuss potential approaches\nYou only share up to a few lines of code for everyone’s benefit for the resolution of a specific question or issue\n\nUsing anything (code, resources, tips, approaches, etc.) provided by the instructional team\n\n\n\nWhat is forbidden\nThe following actions are not permitted in any way and are considered a violation of academic integrity:\n\nCopying and sharing code between students in individual assignments or across goups in the group project\nSharing anything on any individual assignment\nUsing code snippets found online (stack overflow, etc.) and not commenting the source\nPlagiarism of any kind\nUsing any Generative Artificial Intelligence tool without acknowledging it\nUsing someone else’s cloud resources\nMaking your private GitHub repos public\nSharing or posting any course materials anywhere\nFaking or tampering with git commit dates or messages"
  },
  {
    "objectID": "syllabus.html#use-of-generative-ai-tools",
    "href": "syllabus.html#use-of-generative-ai-tools",
    "title": "Syllabus",
    "section": "Use of Generative AI tools",
    "text": "Use of Generative AI tools\nWe recognize the recent availability of very powerful generative AI tools like Chat-GPT, GitHub Copilot, and others. These tools can help us be more effective and we embrace their use.\n\n\n\n\n\n\nImportant\n\n\n\nYou are allowed to use GAI tools in a non substantial way.\nWhat does non substantial mean?\nIt means that whatever is generated by GAI must not make up the majority of the work you do.\nAny use of these tools must abide to the following rules:\n\nYou must acknowledge the use of GAI tools\nYou must comment which code blocks were generated by GAI\nYou must note which written sections were generated by GAI\nIf you used a prompt to ask the GAI tool to do something, you must include it\n\nFor this course, valid uses of gen-ai can be:\n\nGenerating a code snippet or single function to perform a task. It’s likely you’ll need to modify it anyway\nCommenting code\nUsing it as a writing aid (spelling, grammar, word choice, limited phrase translation) on content created by you, not the actual writing. Note: non-native English speakers cannot use gen-ai to fully translate content written in another language.\nGenerating visualization starter code (you can accelerate the generation of the starting point, but you still need to customize the viz with all the best practices learned in this course)\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAny deviation from these rules is considered a violation of academic integrity and will be acted on.\nYou typically KNOW when you are crossing the line into un-ethical territory. As a general rule, If you feel like you might be crossing a line, then you probably are!\n\n\nIn addition to what we are stating here, please take a look at the Data Science and Analytics Program’s ChatGPT usage guidelines."
  },
  {
    "objectID": "syllabus.html#resources-and-policies",
    "href": "syllabus.html#resources-and-policies",
    "title": "Syllabus",
    "section": "Georgetown University resources and policies",
    "text": "Georgetown University resources and policies\n\nGeorgetown University’s Plagiarism Policy\nPlagiarism or academic dishonesty in any form will not be tolerated and may result in a failing grade. All Honor Code violations will be submitted to the Honor Council.\nAcademic integrity is central to the learning and teaching process. Students are expected to conduct themselves in a manner that will contribute to the maintenance of academic integrity by making all reasonable efforts to prevent the occurrence of academic dishonesty. Academic dishonesty includes (but is not limited to) obtaining or giving aid on an examination, having unauthorized prior knowledge of an examination, doing work for another student, and plagiarism of all types, including copying code.\nPlagiarism is the intentional or unintentional presentation of another person’s idea or product as one’s own. Plagiarism includes, but is not limited to the following: copying verbatim all or part of another’s written work; using phrases, charts, figures, illustrations, code, or mathematical/scientific solutions without citing the source; paraphrasing ideas, conclusions, or research without citing the source; and using all or part of a literary plot, poem, film, musical score, or other artistic product without attributing the work to its creator. Students can avoid unintentional plagiarism by following carefully accepted scholarly practices. Notes taken for papers and research projects should accurately record sources cited, quoted, paraphrased, or summarized sources and articles should be acknowledged in footnotes.\n\n\nHonor System\nAll students are expected to maintain the highest standards of academic and personal integrity in pursuit of their education at Georgetown. Academic dishonesty, including plagiarism, in any form, is a serious offense, and students found in violation are subject to academic penalties that include, but are not limited to, failure of the course, termination from the program, and revocation of degrees already conferred. All students are held to the Georgetown University Honor Code. For more information about the Honor Code http://gervaseprograms.georgetown.edu/honor/\n\n\nAcademic Integrity and Courtesy\nAs a Jesuit, Catholic university committed to the education of the whole person, Georgetown expects all members of the academic community, students and faculty, to strive for excellence in scholarship and in character. The University spells out the specific minimum standards for academic integrity in its Honor Code and the procedures to be followed if academic dishonesty is suspected. Over and above the honor code, in this course, we will seek to create an engaged and passionate learning environment characterized by respect and courtesy in both our discourse and our ways of paying attention to one another.\n\n\nAcademic Resource Center\nThe Academic Resource Center (ARC) is the campus office responsible for reviewing medical documentation and determining reasonable accommodations for students with disabilities. You can reach the ARC via email at arc@georgetown.edu.\n\n\nCounseling and Psychiatric Services (CAPS)\nAs Georgetown faculty, you are among the most important individuals in some of the students’ lives. They may turn to you when they are struggling and in times of need, or you may be one of the first to notice when they are distressed.\nThe CAPS website has tips for faculty on how to deal with struggling or distressed students. 202.687.6985 or after hours, call (833) 960-3006 to reach Fonemed, a telehealth service; individuals may ask for the on-call CAPS clinician.\n\n\nEmergency Preparedness and HOYAlert\nWe encourage all faculty to become familiar with Georgetown’s Office of Emergency Management and sign up for HOYAlert to receive important safety and University operating status updates. Faculty teaching at the Georgetown Downtown campus might also want to sign up for AlertDC to obtain safety and traffic updates.\n\n\nOffice of Institutional Compliance and Ethics\nThe Office of Institutional Compliance and Ethics supports and coordinates many compliance-related activities the University undertakes. With the endorsement and assistance of the University’s senior leadership, this Office is responsible for leading the development, implementation, and operation of the Georgetown Institutional Compliance and Ethics Program.\n\n\nOffice of Institutional Diversity, Equity and Affirmative Action (IDEAA)\nThe mission of IDEAA is to promote a deep understanding and appreciation among the diverse members of the University community to result in justice and equality in educational, employment, and contracting opportunities, as well as to lead efforts to create an inclusive academic and work environment.\n\n\nTitle IX/Sexual Misconduct\nGeorgetown University and its faculty are committed to supporting survivors and those impacted by sexual misconduct, which includes sexual assault, sexual harassment, relationship violence, and stalking. Georgetown requires faculty members unless otherwise designated as confidential, to report all disclosures of sexual misconduct to the University Title IX Coordinator or a Deputy Title IX Coordinator. Suppose you disclose an incident of sexual misconduct to a professor in or outside of the classroom (except disclosures in papers). In that case, that faculty member must report the incident to the Title IX Coordinator or Deputy Title IX Coordinator. The coordinator will, in turn, reach out to the student to provide support, resources, and the option to meet—[Please note that the student is not required to meet with the Title IX coordinator.]. More information about reporting options and resources can be found on the Sexual Misconduct Website.\nIf you would prefer to speak to someone confidentially, Georgetown has a number of fully confidential professional resources that can provide support and assistance. These resources include:\n\nHealth Education Services for Sexual Assault Response and Prevention: confidential email sarp@georgetown.edu\nCounseling and Psychiatric Services (CAPS): 202.687.6985 or after hours, call (833) 960-3006 to reach Fonemed, a telehealth service; individuals may ask for the on-call CAPS clinician\n\nTitle IX Sexual Misconduct Statement Please know that as faculty members, we are committed to supporting survivors of sexual misconduct, including relationship violence and sexual assault. However, university policy also requires us to report any disclosures about sexual misconduct to the Title IX Coordinator, whose role is to coordinate the University’s response to sexual misconduct.\nGeorgetown has a number of fully confidential professional resources who can provide support and assistance to survivors of sexual assault and other forms of sexual misconduct. These resources include:\n\nGetting Help\nJen Schweer, MA, LPCAssociate Director of Health Education Services for Sexual Assault Response and Prevention (202) 687-032jls242@georgetown.edu\nErica Shirley, Trauma SpecialistCounseling and Psychiatric Services (CAPS)(202) 687-6985els54@georgetown.edu\n\n\n\nThreat Assessment\nGeorgetown University established its Threat Assessment program as part of an extensive emergency planning initiative. The program at Georgetown has been developed and implemented to meet current best practices and national standards for hazard planning in higher education institutions and workplace violence prevention.\n\n\nSpecial Accommodations\nIf you believe that you have a disability that will affect your performance in this class, don’t hesitate to get in touch with the Academic Resource Center for further information. The center is located in the Leavey Center, Suite 335. The Academic Resource Center is the campus office responsible for reviewing documentation provided by students with disabilities and determining reasonable accommodations according to the Americans with Disabilities Act (ADA) and University policies."
  },
  {
    "objectID": "slides/14-slides.html#data-preparation-accounts-for-about-80-of-the-work-of-data-scientists",
    "href": "slides/14-slides.html#data-preparation-accounts-for-about-80-of-the-work-of-data-scientists",
    "title": "Lecture 14",
    "section": "Data preparation accounts for about 80% of the work of data scientists",
    "text": "Data preparation accounts for about 80% of the work of data scientists\n &gt; Source: Forbes article"
  },
  {
    "objectID": "slides/14-slides.html#why-does-this-happen",
    "href": "slides/14-slides.html#why-does-this-happen",
    "title": "Lecture 14",
    "section": "Why does this happen?",
    "text": "Why does this happen?\n\n\nSame set of data sources…\nMultiple different feature pipelines..\nMultiple ML models..\nBut, an overlapping set of ML features..\nMore problems…\n\nFeature duplication\nSlow time to market\nInaccurate predictions"
  },
  {
    "objectID": "slides/14-slides.html#solution",
    "href": "slides/14-slides.html#solution",
    "title": "Lecture 14",
    "section": "Solution…",
    "text": "Solution…\nMachine Learning Feature Store\n\n\nFor a moment, think of the feature store as a database but for ML features.\nIn a Feature Store\n\nFeatures are now easy to find (GUI, SDK)\nFeature transformations are reproducible (feature engineering pipelines can now refer to a consistent set of data)\nML training pipeline has a reliable, curated, maintained data source to get training datasets rather than having to look at the data lake directly\nLow latency lookup for realtime inference\nConsistent features for training and inference"
  },
  {
    "objectID": "slides/14-slides.html#feature-stores-you-can-use",
    "href": "slides/14-slides.html#feature-stores-you-can-use",
    "title": "Lecture 14",
    "section": "Feature Stores you can use",
    "text": "Feature Stores you can use\n\nFeast: Open Source for Production ML\nfeathr: A scalable, unified data and AI engineering platform for enterprise\nTecton: Feature Platform for Real-Time Machine Learning\nAmazon Sagemaker Feature Store"
  },
  {
    "objectID": "slides/14-slides.html#why-do-we-need-vector-databases",
    "href": "slides/14-slides.html#why-do-we-need-vector-databases",
    "title": "Lecture 14",
    "section": "Why do we need vector databases?",
    "text": "Why do we need vector databases?\n\nCentral to Generative AI apps.\n\nExtend the capabilities of LLMs.\nAvoid hallucinations.\n\nUse cases include\n\nProduct search\nTroubleshooting and incident response\nOf course, RAG!"
  },
  {
    "objectID": "slides/14-slides.html#algorithms-for-similarity-searh",
    "href": "slides/14-slides.html#algorithms-for-similarity-searh",
    "title": "Lecture 14",
    "section": "Algorithms for similarity searh",
    "text": "Algorithms for similarity searh\n\nk-Nearest Neighbor (k-NN)\nApproximate Nearest Neighbor\n\nHierarchical Navigable Small Worlds algorithm (HNSW)\nFaiss"
  },
  {
    "objectID": "slides/14-slides.html#langchain-and-vector-dbs",
    "href": "slides/14-slides.html#langchain-and-vector-dbs",
    "title": "Lecture 14",
    "section": "Langchain and Vector DBs",
    "text": "Langchain and Vector DBs"
  },
  {
    "objectID": "slides/14-slides.html#large-scale-data-ingestion",
    "href": "slides/14-slides.html#large-scale-data-ingestion",
    "title": "Lecture 14",
    "section": "Large scale data ingestion",
    "text": "Large scale data ingestion"
  },
  {
    "objectID": "slides/12-slides.html#aws-academy",
    "href": "slides/12-slides.html#aws-academy",
    "title": "Lecture 12",
    "section": "AWS Academy",
    "text": "AWS Academy\n\nCredit limit - $100\nCourse numbers:\n\nCourse #1 - 24178\nCourse #2 - 27354\nCourse #3 - 22802\nCourse #4 - 26418\n\n\nSTAY WITH COURSE 24178 UNLESS YOU HAVE RUN OUT OF CREDITS OR &gt;$90 USED!\nNote that you will have to repeat several setup steps:\n\nsecurity group\nEC2 keypair uploading (the AWS part only)\nany S3 uploading or copying as well as bucket creation as necessary\nEMR configuration"
  },
  {
    "objectID": "slides/12-slides.html#lambda",
    "href": "slides/12-slides.html#lambda",
    "title": "Lecture 12",
    "section": "Lambda",
    "text": "Lambda\n\nLambda is a powerful tool to build serverless microfunctions in a variety of programming languages that can be triggered in a variety of ways.\nThe execution of a Lambda can scale to meet the burst needs of users going to a website, or the number of rows of incoming data.\nThere can be thousands of executions happening simultaneously.\nLambda is billed by the millisecond (ms)."
  },
  {
    "objectID": "slides/12-slides.html#docker",
    "href": "slides/12-slides.html#docker",
    "title": "Lecture 12",
    "section": "Docker",
    "text": "Docker\nhttps://www.docker.com/resources/what-container/\n\n\nA container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another.\nA Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings.\nContainer images become containers at runtime and in the case of Docker containers – images become containers when they run on Docker Engine."
  },
  {
    "objectID": "slides/12-slides.html#dockerfile",
    "href": "slides/12-slides.html#dockerfile",
    "title": "Lecture 12",
    "section": "Dockerfile",
    "text": "Dockerfile\n```{bash}\n# syntax=docker/dockerfile:1\n\n# adapted from https://www.philschmid.de/aws-lambda-with-custom-docker-image\n# https://docs.aws.amazon.com/lambda/latest/dg/python-image.html\nFROM public.ecr.aws/lambda/python:3.9\n\n# copy requirements file and install necessary packages\nADD requirements.txt ${LAMBDA_TASK_ROOT}\nRUN pip3 install -r ${LAMBDA_TASK_ROOT}/requirements.txt --target \"${LAMBDA_TASK_ROOT}\"\n\n# Copy function code to docker container\nCOPY app.py ${LAMBDA_TASK_ROOT}\n\n# app (name of py file)\n# handler (name of function to execute for lambda job)\nCMD [ \"app.lambda_handler\" ]\n```"
  },
  {
    "objectID": "slides/12-slides.html#what-is-data-engineering",
    "href": "slides/12-slides.html#what-is-data-engineering",
    "title": "Lecture 12",
    "section": "What is Data Engineering?",
    "text": "What is Data Engineering?\nAs the scale of the data grew, the existing ETL processes alone were not sufficient, a separate discipline was needed for:\n\nCollecting data\nManaging storage\nCataloging\nMaking it available for applications such as analytics & machine learning)\nSecurity\nLifecycle management\nAnd more…\n\n\nFrom Wikipedia: Data engineering refers to the building of systems to enable the collection and usage of data. This data is usually used to enable subsequent analysis and data science; which often involves machine learning. Making the data usable usually involves substantial compute and storage, as well as data processing and cleaning."
  },
  {
    "objectID": "slides/12-slides.html#what-do-data-engineers-do",
    "href": "slides/12-slides.html#what-do-data-engineers-do",
    "title": "Lecture 12",
    "section": "What do Data Engineers do?",
    "text": "What do Data Engineers do?\nData engineers build systems that collect data from different sources and make this data available for analytics and ML application. This usually involves the following:\n\nAcquisition: Finding all the different datasets around the business.\n\nThese could be availble in databases, shared drives, ingested directly from IoT devices, external datasets, and more.\n\nCleansing: The raw data usually cannot be used as is, it needs to be cleaned.\nConversion: Since the data is coming from different sources, it would probably in different formats (database tables, CSV, JSON, custom). Needs to be converted into a common format (such as parquet) before it becomes usable.\n\nMultiple datasets need to be joined together to answer a business question."
  },
  {
    "objectID": "slides/12-slides.html#what-do-data-engineers-do-contd.",
    "href": "slides/12-slides.html#what-do-data-engineers-do-contd.",
    "title": "Lecture 12",
    "section": "What do Data Engineers do (contd.)?",
    "text": "What do Data Engineers do (contd.)?\n\nDisambiguation: How to interpret what the data means? Use a data catalog and then with the help of subject matter experts (often called Data Stewards) add meaningful description to the datasets.\nDeduplication: Having a single source of truth!\nData Governance: for how long to store the data, how to enforce access controls (Principle of least privilege) etc.\n\nOnce this is done, data may be stored in a central repository such as a data lake or data lakehouse. Data engineers may also copy and move subsets of data into a data warehouse."
  },
  {
    "objectID": "slides/12-slides.html#data-engineering-tools-technologies",
    "href": "slides/12-slides.html#data-engineering-tools-technologies",
    "title": "Lecture 12",
    "section": "Data engineering tools & technologies",
    "text": "Data engineering tools & technologies\nData engineers work with a variety of tools and technologies, including:\n\nETL Tools: ETL (extract, transform, load) tools move data between systems. They access data, then apply rules to “transform” the data through steps that make it more suitable for analysis.\nSQL: Structured Query Language (SQL) is the standard language for querying relational databases.\nPython: Python is a general programming language. Data engineers may choose to use Python for ETL tasks. Spark (pyspark) for Big Data, Apache Flink for streaming data.\nCloud Data Storage: Including Amazon S3, Azure Data Lake Storage (ADLS), Google Cloud Storage, etc.\nCloud Data Warehouses: Data ready for use by data scientists and analysts is stored in data warehouses, such as Amazon Redshift, Google BigQuery, Azure Data Warehouse, Snowflake etc."
  },
  {
    "objectID": "slides/12-slides.html#popular-data-engineering-tools",
    "href": "slides/12-slides.html#popular-data-engineering-tools",
    "title": "Lecture 12",
    "section": "Popular Data engineering tools",
    "text": "Popular Data engineering tools\n\nSource: https://www.secoda.co/blog/the-top-20-most-commonly-used-data-engineering-tools"
  },
  {
    "objectID": "slides/12-slides.html#data-lakes",
    "href": "slides/12-slides.html#data-lakes",
    "title": "Lecture 12",
    "section": "Data Lakes",
    "text": "Data Lakes\n\n\nFrom Wikipedia: A data lake is a system or repository of data stored in its natural/raw format, usually object blobs or files. A data lake is usually a single store of data including raw copies of source system data, sensor data, social data etc., and transformed data used for tasks such as reporting, visualization, advanced analytics and machine learning. A data lake can include structured data from relational databases (rows and columns), semi-structured data (CSV, logs, XML, JSON), unstructured data (emails, documents, PDFs) and binary data (images, audio, video)."
  },
  {
    "objectID": "slides/12-slides.html#data-lake-cloud-provider-definitions",
    "href": "slides/12-slides.html#data-lake-cloud-provider-definitions",
    "title": "Lecture 12",
    "section": "Data Lake (Cloud Provider Definitions)",
    "text": "Data Lake (Cloud Provider Definitions)\nDefinitions in the wild (emphasis mine).\n\n\nAWS\nA data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. You can store your data as-is, without having to first structure the data, and run different types of analytics—from dashboards and visualizations to big data processing, real-time analytics, and machine learning to guide better decisions.\n\nGCP\nA data lake provides a scalable and secure platform that allows enterprises to: ingest any data from any system at any speed—even if the data comes from on-premises, cloud, or edge-computing systems; store any type or volume of data in full fidelity; process data in real time or batch mode; and analyze data using SQL, Python, R, or any other language, third-party data, or analytics application.\n\nAzure\nAzure Data Lake includes all the capabilities required to make it easy for developers, data scientists, and analysts to store data of any size, shape, and speed, and do all types of processing and analytics across platforms and languages.\n\nDataBricks\nA data lake is a central location that holds a large amount of data in its native, raw format. Compared to a hierarchical data warehouse, which stores data in files or folders, a data lake uses a flat architecture and object storage to store the data.‍"
  },
  {
    "objectID": "slides/12-slides.html#working-with-a-cloud-data-lake",
    "href": "slides/12-slides.html#working-with-a-cloud-data-lake",
    "title": "Lecture 12",
    "section": "Working with a Cloud Data Lake",
    "text": "Working with a Cloud Data Lake\nA cloud data lake is setup using the cloud provider’s object store (S3, GCS, Azure Blob Storage).\n\nThe object stores are extremely scalable, for context, the maximum size of an object in S3 is 5 TB and there is no limit to number of objects in an S3 bucket.\nThey are extremely duarable, 99.999999999%. Provide strong consistency (read-after-write, listing buckets and objects, granting permissions etc.).\nCost effective, with multiple storage classes.\nIntegration with data processing tools such as Spark, machine learning tools such as SageMaker, data warehouses such as RedShift and data cataloging tools.\nThey provide Fine Grained Access Control to the data."
  },
  {
    "objectID": "slides/12-slides.html#data-warehouses",
    "href": "slides/12-slides.html#data-warehouses",
    "title": "Lecture 12",
    "section": "Data Warehouses",
    "text": "Data Warehouses\n\n\nFrom Wikipedia: In computing, a data warehouse (DW or DWH), also known as an enterprise data warehouse (EDW), is a system used for reporting and data analysis and is considered a core component of business intelligence. DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data in one single place that are used for creating analytical reports for workers throughout the enterprise."
  },
  {
    "objectID": "slides/12-slides.html#data-warehouse-cloud-provider-definitions",
    "href": "slides/12-slides.html#data-warehouse-cloud-provider-definitions",
    "title": "Lecture 12",
    "section": "Data Warehouse (Cloud Provider Definitions)",
    "text": "Data Warehouse (Cloud Provider Definitions)\nDefinitions in the wild (emphasis mine).\n\n\nAWS\nA data warehouse is a central repository of information that can be analyzed to make more informed decisions. Data flows into a data warehouse from transactional systems, relational databases, and other sources, typically on a regular cadence. Business analysts, data engineers, data scientists, and decision makers access the data through business intelligence (BI) tools, SQL clients, and other analytics applications.\nAzure\nA data warehouse is a centralized repository of integrated data from one or more disparate sources. Data warehouses store current and historical data and are used for reporting and analysis of the data.\n\nGCP\nA data warehouse is an enterprise system used for the analysis and reporting of structured and semi-structured data from multiple sources, such as point-of-sale transactions, marketing automation, customer relationship management, and more.\nSnowflake\nA data warehouse (DW) is a relational database that is designed for analytical rather than transactional work. It collects and aggregates data from one or many sources so it can be analyzed to produce business insights."
  },
  {
    "objectID": "slides/12-slides.html#working-with-a-cloud-data-warehouse",
    "href": "slides/12-slides.html#working-with-a-cloud-data-warehouse",
    "title": "Lecture 12",
    "section": "Working with a Cloud Data Warehouse",
    "text": "Working with a Cloud Data Warehouse\nAll cloud providers provide a data warehouse solution that works in conjunction with their data lake solution.\n\nAWS has Redshift, Azure has Synapse Analytics. GCP has BigQuery and then there is Snowflake.\nIn a data warehouse, Online analytical processing (OLAP) allows for fast querying and analysis of data from different perspectives. It also helps in pre-aggregating and pre-calculating the information available in the archive.\nData warehouses are Peta Byte scale (Amazon RedShift, Google BigQuery, Azure Synapse Analytics).\nData warehouses can have dedicated compute provisioned or be serverless (BigQuery is serverless, Redshift allows both options now).\nData warehouses now offer integrated ML capabilities, you can build models with SQL and use them in queries (Amazon RedShift ML, Google BigQuery ML, Azure Synapse Analytics ML).\nIntegration with reporting and dashboarding tools such as Tableau, Grafana, Looker etc. and data analytics tools such as Spark and data cataloging tools.\nThey provide Fine Grained Access Control to the data."
  },
  {
    "objectID": "slides/12-slides.html#combining-data-lakes-and-data-warehouses",
    "href": "slides/12-slides.html#combining-data-lakes-and-data-warehouses",
    "title": "Lecture 12",
    "section": "Combining Data Lakes and Data Warehouses",
    "text": "Combining Data Lakes and Data Warehouses\nCombine the flexibility, cost-efficiency, and scale of data lakes with the data management and ACID transactions of data warehouses to provide a single architecture that can enable business intelligence and machine learning on all data.\n\nBuild a lakehouse architecture on AWS\nThe Databricks lakehouse platform\nOpen data lakehouse on Google Cloud\nThe data lakehouse, the data warehouse and a modern data platform on Azure"
  },
  {
    "objectID": "slides/12-slides.html#nosql-databases",
    "href": "slides/12-slides.html#nosql-databases",
    "title": "Lecture 12",
    "section": "NoSQL Databases",
    "text": "NoSQL Databases\nAt some point we needed to think beyond relation databases, because: - Data became more and more complex (not all data is tabular, thinkin JSON data emitted by an IoT device). - Cost of storage decreased (everything did not need to be stored in the 3rd normal form) - More data on the cloud meant data needed to be placed across different servers (scale-out) - Data needed to be placed intelligently in geo locations of interest - And more…\n\nFrom Wikipedia: A NoSQL (originally referring to “non-SQL” or “non-relational”) database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases. Such databases have existed since the late 1960s, but the name “NoSQL” was only coined in the early 21st century, triggered by the needs of Web 2.0 companies. NoSQL databases are increasingly used in big data and real-time web applications. NoSQL systems are also sometimes called Not only SQL to emphasize that they may support SQL-like query languages or sit alongside SQL databases in polyglot-persistent architectures."
  },
  {
    "objectID": "slides/12-slides.html#types-of-nosql-databases",
    "href": "slides/12-slides.html#types-of-nosql-databases",
    "title": "Lecture 12",
    "section": "Types of NoSQL Databases",
    "text": "Types of NoSQL Databases\nOver time, four major types of NoSQL databases emerged: document databases, key-value databases, wide-column stores, and graph databases.\n\nDocument databases store data in documents similar to JSON (JavaScript Object Notation) objects. Each document contains pairs of fields and values. The values can typically be a variety of types including things like strings, numbers, booleans, arrays, or objects.\nKey-value databases are a simpler type of database where each item contains keys and values.\nWide-column stores store data in tables, rows, and dynamic columns.\nGraph databases store data in nodes and edges. Nodes typically store information about people, places, and things, while edges store information about the relationships between the nodes."
  },
  {
    "objectID": "slides/12-slides.html#examples-of-nosql-databases",
    "href": "slides/12-slides.html#examples-of-nosql-databases",
    "title": "Lecture 12",
    "section": "Examples of NoSQL Databases",
    "text": "Examples of NoSQL Databases\n\n\n\n\n\n\n\nNOSQL Database Type\nExamples\n\n\n\n\nDocument Database\nAmazon DocumentDB, MongoDB, Cosmos DB, ArangoDB, Couchbase Server, CouchDB\n\n\nKey-value Database\nAmazon DynamoDB, Couchbase, Memcached, Redis\n\n\nWide-column datastores\nAmazon DynamoDB, Apache Cassandra, Google Bigtable, Azure Tables\n\n\nGraph databases\nAmazon Neptune, Neo4j\n\n\n\nAs a data scientist you would work with a NoSQL database through an SDK/API. Several programming languages are supported including Python, Java, Go, C++ etc."
  },
  {
    "objectID": "slides/12-slides.html#example-of-documents-in-a-document-database",
    "href": "slides/12-slides.html#example-of-documents-in-a-document-database",
    "title": "Lecture 12",
    "section": "Example of documents in a document database",
    "text": "Example of documents in a document database\nHere is an example of a document inserted in a key-value/document database such as MongoDB.\n```{bash}\n{\n   \"name\" : \"IS 350\",\n   \"year\" : \"2015\",\n   \"make\" : \"Lexus\",\n   \"colors\" : [\"black\",\"white\",\"grey\"],\n   \"spec\" : {\n      \"engine\" : \"V6\",\n      \"wheelbase\" : \"110.2 in\",\n      \"length\" : \"183.7 in\"\n   }\n}\n```\nThe same example can be inserted in an Amazon DynamoDB table called (say) Cars.\n```{python}\ndatabase = boto3.resource('dynamodb')\ntable = database.Table('cars')\nitem = {\n   \"name\" : \"IS 350\",\n   \"year\" : \"2015\",\n   \"make\" : \"Lexus\",\n   \"colors\" : [\"black\",\"white\",\"grey\"],\n   \"spec\" : {\n      \"engine\" : \"V6\",\n      \"wheelbase\" : \"110.2 in\",\n      \"length\" : \"183.7 in\"\n   }\n}\ntable.put_item(Item = item)\n```"
  },
  {
    "objectID": "slides/12-slides.html#other-data-stores-to-know-about",
    "href": "slides/12-slides.html#other-data-stores-to-know-about",
    "title": "Lecture 12",
    "section": "Other Data stores to know about",
    "text": "Other Data stores to know about\nBesides the general concepts about data lkakes, warehouses, different types of databases, there are some purpose built databases that are good to know about.\n\nSplunk\nElasticsearch\nDuckDB\nMany many more…"
  },
  {
    "objectID": "slides/12-slides.html#splunk",
    "href": "slides/12-slides.html#splunk",
    "title": "Lecture 12",
    "section": "Splunk",
    "text": "Splunk\nSplunk is a software platform to search, analyze and visualize the machine-generated data gathered from the websites, applications, sensors, devices etc. which make up your IT infrastructure and business. See https://www.splunk.com/.\n\nThe logfile we analyzed in assignment 4 s3://bigdatateaching/forensicswiki/2012_logs.txt, is the typical kind of data that gets ingested into Splunk.\n\n\n\n\nImage courtsey: https://docs.splunk.com/Documentation/Splunk/9.0.2/SearchTutorial/Aboutthesearchapp"
  },
  {
    "objectID": "slides/12-slides.html#elasticsearch",
    "href": "slides/12-slides.html#elasticsearch",
    "title": "Lecture 12",
    "section": "Elasticsearch",
    "text": "Elasticsearch\nFrom https://www.elastic.co/what-is/elasticsearch, emphasis mine\nElasticsearch is a distributed, free and open search and analytics engine for all types of data, including textual, numerical, geospatial, structured, and unstructured. Elasticsearch is built on Apache Lucene and was first released in 2010 by Elasticsearch N.V. (now known as Elastic).\nCommonly referred to as the ELK Stack (after Elasticsearch, Logstash, and Kibana), the Elastic Stack now includes a rich collection of lightweight shipping agents known as Beats for sending data to Elasticsearch.\nData is inserted in Elasticsearch indexes as JSON documents using a REST API/SDK.\n\n\n\nImage courtsey: https://www.elastic.co/kibana"
  },
  {
    "objectID": "slides/12-slides.html#duckdb",
    "href": "slides/12-slides.html#duckdb",
    "title": "Lecture 12",
    "section": "DuckDB",
    "text": "DuckDB\nDuckDB is an in-process SQL OLAP database management system.\n\nIt is like sqllite, but for analytics. What does this mean? It means that your database runs inside your process, there are no servers to manage, no remote system to connect to. Very snappy, easy to experiment with SQL like syntax.\n\nDuckDB does vectorized processing i.e. loads chunks of data into memory (tries to keep everything in the CPU’s L1 and L2 cache) and is thus able to handle datasets bigger than the amount of RAM available.\nDuckDB supports Python, R and a host of other languages.\nWhen to use DuckDB: when you cannot use Pandas for rapid experimentation with Big Data, especially when combining DuckDB with Arrow.\nWhen not to use DuckDB: DuckDB has a valuable but very niche use-case.\n\n\nPaper on DuckDB by Hannes Mühleisen & Mark Raasveldt\nDuckDB: an Embeddable Analytical Database https://hannes.muehleisen.org/publications/SIGMOD2019-demo-duckdb.pdf"
  },
  {
    "objectID": "slides/12-slides.html#further-reading",
    "href": "slides/12-slides.html#further-reading",
    "title": "Lecture 12",
    "section": "Further Reading",
    "text": "Further Reading\nPlease lookup these topics on Google for further reading. Not providing specific links here because they all point to vendor specific products.\n\nData Catalog\nData Governance\nData Mesh\nData Fabric"
  },
  {
    "objectID": "slides/12-slides.html#lab-analyzing-nyc-taxi-dataset-with-duckdb",
    "href": "slides/12-slides.html#lab-analyzing-nyc-taxi-dataset-with-duckdb",
    "title": "Lecture 12",
    "section": "Lab: Analyzing NYC-Taxi dataset with DuckDB",
    "text": "Lab: Analyzing NYC-Taxi dataset with DuckDB"
  },
  {
    "objectID": "slides/12-slides.html#a-simple-example-of-using-duckdb-and-apache-arrow-using-nyc-taxi-dataset",
    "href": "slides/12-slides.html#a-simple-example-of-using-duckdb-and-apache-arrow-using-nyc-taxi-dataset",
    "title": "Lecture 12",
    "section": "A simple example of using DuckDB and Apache Arrow using NYC Taxi dataset",
    "text": "A simple example of using DuckDB and Apache Arrow using NYC Taxi dataset\nThis notebook reads the NYC taxi dataset files for the year 2021 (about ~29 million rows) and runs some analytics operation on this dataset. This dataset is too big to fit into memory.\n\nWe read the data from S3 using apache Arrow (pyarrow).\nThe zero-copy integration between DuckDB and Apache Arrow allows for rapid analysis of larger than memory datasets in Python and R using either SQL or relational APIs.\nWe create a DuckDB instance in memory and using the connection to this in-memory database We run some simple analytics operations using SQL syntax.\n\nAlso see https://duckdb.org/2021/12/03/duck-arrow.html"
  },
  {
    "objectID": "slides/10-slides.html#aws-academy",
    "href": "slides/10-slides.html#aws-academy",
    "title": "Lecture 10",
    "section": "AWS Academy",
    "text": "AWS Academy\n\nCredit limit - $100\nCourse numbers:\n\nCourse #1 - 24178\nCourse #2 - 27354\nCourse #3 - 22802\nCourse #4 - 26418\n\n\nSTAY WITH COURSE 24178 UNLESS YOU HAVE RUN OUT OF CREDITS OR &gt;$90 USED!\nNote that you will have to repeat several setup steps:\n\nsecurity group\nEC2 keypair uploading (the AWS part only)\nsagemaker setup\nany S3 uploading or copying as well as bucket creation as necessary\nEMR configuration"
  },
  {
    "objectID": "slides/10-slides.html#connected-and-extensible",
    "href": "slides/10-slides.html#connected-and-extensible",
    "title": "Lecture 10",
    "section": "Connected and extensible",
    "text": "Connected and extensible"
  },
  {
    "objectID": "slides/10-slides.html#caching-and-persistence",
    "href": "slides/10-slides.html#caching-and-persistence",
    "title": "Lecture 10",
    "section": "Caching and Persistence",
    "text": "Caching and Persistence\nBy default, RDDs are recomputed every time you run an action on them. This can be expensive (in time) if you need to use a dataset more than once.\nSpark allows you to control what is cached in memory.\nTo tell spark to cache an object in memory, use persist() or cache():\n\ncache(): is a shortcut for using default storage level, which is memory only\npersist(): can be customized to other ways to persist data (including both memory and/or disk)\n\n# caches error RDD in memory, but only after an action is run\nerrors = logs.filter(lambda x: \"error\" in x and \"2019-12\" in x).cache()"
  },
  {
    "objectID": "slides/10-slides.html#collect-caution",
    "href": "slides/10-slides.html#collect-caution",
    "title": "Lecture 10",
    "section": "collect CAUTION",
    "text": "collect CAUTION"
  },
  {
    "objectID": "slides/10-slides.html#spark-ui---executors",
    "href": "slides/10-slides.html#spark-ui---executors",
    "title": "Lecture 10",
    "section": "Spark UI - Executors",
    "text": "Spark UI - Executors"
  },
  {
    "objectID": "slides/10-slides.html#udf-speed-comparison",
    "href": "slides/10-slides.html#udf-speed-comparison",
    "title": "Lecture 10",
    "section": "UDF Speed Comparison",
    "text": "UDF Speed Comparison\n\n\n\nCosts:\n\nSerialization/deserialization (think pickle files)\nData movement between JVM and Python\nLess Spark optimization possible\n\nOther ways to make your Spark jobs faster source:\n\nCache/persist your data into memory\nUsing Spark DataFrames over Spark RDDs\nUsing Spark SQL functions before jumping into UDFs\nSave to serialized data formats like Parquet"
  },
  {
    "objectID": "slides/10-slides.html#pipelines",
    "href": "slides/10-slides.html#pipelines",
    "title": "Lecture 10",
    "section": "Pipelines",
    "text": "Pipelines"
  },
  {
    "objectID": "slides/10-slides.html#most-popular-aiml-packages",
    "href": "slides/10-slides.html#most-popular-aiml-packages",
    "title": "Lecture 10",
    "section": "Most Popular AI/ML Packages",
    "text": "Most Popular AI/ML Packages"
  },
  {
    "objectID": "slides/10-slides.html#spark-nlp-terminology",
    "href": "slides/10-slides.html#spark-nlp-terminology",
    "title": "Lecture 10",
    "section": "Spark NLP Terminology",
    "text": "Spark NLP Terminology\nAnnotators\n\nLike the ML tools we used in Spark\nAlways need input and output columns\nTwo flavors:\n\nApproach - like ML estimators that need a fit() method to make an Annotator Model or Transformer\nModel - like ML transformers and uses transform() method only\n\n\nAnnotator Models\n\nPretrained public versions of models available through .pretained() method"
  },
  {
    "objectID": "slides/10-slides.html#reddit-data",
    "href": "slides/10-slides.html#reddit-data",
    "title": "Lecture 10",
    "section": "Reddit Data!",
    "text": "Reddit Data!"
  },
  {
    "objectID": "slides/10-slides.html#assignment-details",
    "href": "slides/10-slides.html#assignment-details",
    "title": "Lecture 10",
    "section": "Assignment Details",
    "text": "Assignment Details\nReviewing Project Milestone 1: EDA"
  },
  {
    "objectID": "slides/10-slides.html#open-qa",
    "href": "slides/10-slides.html#open-qa",
    "title": "Lecture 10",
    "section": "Open Q&A",
    "text": "Open Q&A\nQuestions about the project requirements?\nQuestions about using Databricks?"
  },
  {
    "objectID": "slides/10-slides.html#up-to-now-weve-worked-with-batch-data",
    "href": "slides/10-slides.html#up-to-now-weve-worked-with-batch-data",
    "title": "Lecture 10",
    "section": "Up to now, we’ve worked with batch data",
    "text": "Up to now, we’ve worked with batch data\nProcessing large, already collected, batches of data."
  },
  {
    "objectID": "slides/10-slides.html#batch-examples",
    "href": "slides/10-slides.html#batch-examples",
    "title": "Lecture 10",
    "section": "Batch examples",
    "text": "Batch examples\nExamples of batch data analysis?\n\n\nAnalysis of terabytes of logs collected over a long period of time\nAnalysis of code bases on GitHub or other large repositories of textual information such as Wikipedia\nNightly analysis on large data sets collected over a 24 hour period"
  },
  {
    "objectID": "slides/10-slides.html#streaming-examples",
    "href": "slides/10-slides.html#streaming-examples",
    "title": "Lecture 10",
    "section": "Streaming Examples",
    "text": "Streaming Examples\nExamples of streaming data analysis?\n\n\nCredit card fraud detection\nSensor data processing\nOnline advertising based on user actions\nSocial media notifications\n\n\n\nInternational Data Coporation (IDC) forecasts that by 2025 IoT devices will generate 79.4 zettabytes of data."
  },
  {
    "objectID": "slides/10-slides.html#how-do-we-work-with-streams",
    "href": "slides/10-slides.html#how-do-we-work-with-streams",
    "title": "Lecture 10",
    "section": "How do we work with streams?",
    "text": "How do we work with streams?\nProcessing every value coming from a stream of data. That is, data values that are constantly arriving"
  },
  {
    "objectID": "slides/10-slides.html#spark-solved-this-problem-by-creating-dstreams-using-microbatching",
    "href": "slides/10-slides.html#spark-solved-this-problem-by-creating-dstreams-using-microbatching",
    "title": "Lecture 10",
    "section": "Spark solved this problem by creating DStreams using microbatching",
    "text": "Spark solved this problem by creating DStreams using microbatching\nDStreams are represented as a sequence of RDDs.\n\n\n\nA StreamingContext object can be created from an existing SparkContext object.\nsc = ...\nssc = StreamingContext(sc, 1)"
  },
  {
    "objectID": "slides/10-slides.html#important-points-about-streamingcontext",
    "href": "slides/10-slides.html#important-points-about-streamingcontext",
    "title": "Lecture 10",
    "section": "Important points about StreamingContext",
    "text": "Important points about StreamingContext\n\nOnce the context has been started, no new streaming computations can be setup or added\nOnce a context has been stopped, it cannot be restarted\nOnly one StreamingContext can active with a Spark session at the same time\nstop() on the StreamingContext also stops the the SparkContext\nMultiple StreamingContext can be created as long as the previous one is stopped"
  },
  {
    "objectID": "slides/10-slides.html#dstreams-had-some-issues",
    "href": "slides/10-slides.html#dstreams-had-some-issues",
    "title": "Lecture 10",
    "section": "DStreams had some issues",
    "text": "DStreams had some issues\n\nLack of a single API for batch and stream processing: Even though DStreams and RDDs have consistent APIs (i.e., same operations and same semantics), developers still had to explicitly rewrite their code to use different classes when converting their batch jobs to streaming jobs.\nLack of separation between logical and physical plans: Spark Streaming executes the DStream operations in the same sequence in which they were specified by the developer. Since developers effectively specify the exact physical plan, there is no scope for automatic optimizations, and developers have to hand-optimize their code to get the best performance.\nLack of native support for event-time windows: DStreams define window operations based only on the time when each record is received by Spark Streaming (known as processing time). However, many use cases need to calculate windowed aggregates based on the time when the records were generated (known as event time) instead of when they were received or processed. The lack of native support of event-time windows made it hard for developers to build such pipelines with Spark Streaming."
  },
  {
    "objectID": "slides/10-slides.html#what-is-structured-streaming",
    "href": "slides/10-slides.html#what-is-structured-streaming",
    "title": "Lecture 10",
    "section": "What is Structured Streaming?",
    "text": "What is Structured Streaming?\nA single, unified programming model and interface for batch and stream processing\nThis unified model offers a simple API interface for both batch and streaming workloads. You can use familiar SQL or batch-like DataFrame queries on your stream as you would on a batch, leaving dealing with the underlying complexities of fault tolerance, optimizations, and tardy data to the engine.\nA broader definition of stream processing\nBig data processing applications have grown complex enough that the line between real-time processing and batch processing has blurred significantly. The aim with Structured Streaming was to broaden its applicability from traditional stream processing to a larger class of applications; any application that periodically (e.g., every few hours) to continuously (like traditional streaming applications) processes data should be expressible using Structured Streaming."
  },
  {
    "objectID": "slides/10-slides.html#the-programming-model-of-structured-streaming",
    "href": "slides/10-slides.html#the-programming-model-of-structured-streaming",
    "title": "Lecture 10",
    "section": "The Programming Model of Structured Streaming",
    "text": "The Programming Model of Structured Streaming"
  },
  {
    "objectID": "slides/10-slides.html#the-programming-model-of-structured-streaming-1",
    "href": "slides/10-slides.html#the-programming-model-of-structured-streaming-1",
    "title": "Lecture 10",
    "section": "The Programming Model of Structured Streaming",
    "text": "The Programming Model of Structured Streaming\n\n\nEvery new record received in the data stream is like a new row being appended to the unbounded input table.\nStructured Streaming will automatically convert this batch-like query to a streaming execution plan. This is called incrementalization\nStructured Streaming figures out what state needs to be maintained to update the result each time a record arrive\nFinally, developers specify triggering policies to control when to update the results. Each time a trigger fires, Structured Streaming checks for new data (i.e., a new row in the input table) and incrementally updates the result."
  },
  {
    "objectID": "slides/10-slides.html#specifying-output-mode",
    "href": "slides/10-slides.html#specifying-output-mode",
    "title": "Lecture 10",
    "section": "Specifying output mode",
    "text": "Specifying output mode\nAppend mode\nOnly the new rows appended to the result table since the last trigger will be written to the external storage. This is applicable only in queries where existing rows in the result table cannot change (e.g., a map on an input stream).\nUpdate mode\nOnly the rows that were updated in the result table since the last trigger will be changed in the external storage. This mode works for output sinks that can be updated in place, such as a MySQL table.\nComplete mode\nThe entire updated result table will be written to external storage."
  },
  {
    "objectID": "slides/10-slides.html#the-5-fundamentals-steps-of-a-structured-streaming-query",
    "href": "slides/10-slides.html#the-5-fundamentals-steps-of-a-structured-streaming-query",
    "title": "Lecture 10",
    "section": "The 5 Fundamentals steps of a Structured Streaming Query",
    "text": "The 5 Fundamentals steps of a Structured Streaming Query\n\nDefine input sources\nTransform data\nDefine output sink and output mode\nSpecify processing details\nStart the query"
  },
  {
    "objectID": "slides/10-slides.html#define-input-sources",
    "href": "slides/10-slides.html#define-input-sources",
    "title": "Lecture 10",
    "section": "1. Define input sources",
    "text": "1. Define input sources\n\n spark = SparkSession...\n    lines = (spark\n      .readStream.format(\"socket\")\n      .option(\"host\", \"localhost\")\n      .option(\"port\", 9999)\n      .load())\n\n\nThis code generates the lines DataFrame as an unbounded table of newline-separated text data read from localhost:9999. Note that, similar to batch sources with spark.read, this does not immediately start reading the streaming data; it only sets up the configurations necessary for reading the data once the streaming query is explicitly started.\nBesides sockets, Apache Spark natively supports reading data streams from Apache Kafka and various file-based formats (Parquet, ORC, JSON, etc.). A streaming query can define multiple input sources, both streaming and batch, which can be combined using DataFrame operations like unions and joins."
  },
  {
    "objectID": "slides/10-slides.html#transform-data",
    "href": "slides/10-slides.html#transform-data",
    "title": "Lecture 10",
    "section": "2. Transform data",
    "text": "2. Transform data\nfrom pyspark.sql.functions import *\nwords = lines.select(split(col(\"value\"), \"\\\\s\").alias(\"word\")) \ncounts = words.groupBy(\"word\").count()\n\nNow we can apply the usual DataFrame operations\nNote that these operations to transform the lines streaming DataFrame would work in the exact same way if lines were a batch DataFrame."
  },
  {
    "objectID": "slides/10-slides.html#define-output-sink-and-output-mode",
    "href": "slides/10-slides.html#define-output-sink-and-output-mode",
    "title": "Lecture 10",
    "section": "3. Define output sink and output mode",
    "text": "3. Define output sink and output mode\nwriter = counts.writeStream.format(\"console\").outputMode(\"complete\")"
  },
  {
    "objectID": "slides/10-slides.html#specify-processing-details",
    "href": "slides/10-slides.html#specify-processing-details",
    "title": "Lecture 10",
    "section": "4. Specify Processing details",
    "text": "4. Specify Processing details\n\ncheckpointDir = \"...\"\n    writer2 = (writer\n      .trigger(processingTime=\"1 second\")\n      .option(\"checkpointLocation\", checkpointDir))\n\nTriggering details\n\nDefault: When the trigger is not explicitly specified, then by default, the streaming query executes data in micro-batches where the next micro-batch is triggered as soon as the previous micro-batch has completed.\nProcessing time with trigger interval: You can explicitly specify the Processing Time trigger with an interval, and the query will trigger micro-batches at that fixed interval.\nOnce: In this mode, the streaming query will execute exactly one micro-batch. It processes new data available in a single batch and then stops itself. This is useful when you want to control the triggering and processing from an external scheduler that will start the query on a schedule (e.g., to control cost by only executing a query once per day).\nContinuous:  experimental as of Spark 3.0"
  },
  {
    "objectID": "slides/10-slides.html#start-the-query",
    "href": "slides/10-slides.html#start-the-query",
    "title": "Lecture 10",
    "section": "5. Start the query",
    "text": "5. Start the query\nstreamingQuery = writer2.start()"
  },
  {
    "objectID": "slides/10-slides.html#spark-streaming-under-the-hood",
    "href": "slides/10-slides.html#spark-streaming-under-the-hood",
    "title": "Lecture 10",
    "section": "Spark Streaming under the hood",
    "text": "Spark Streaming under the hood\n\n\nSpark SQL analyzes and optimizes this logical plan to ensure that it can be executed incrementally and efficiently on streaming data.\nSpark SQL starts a background thread that continuously executes a loop\nThis loop continues until the query is terminated"
  },
  {
    "objectID": "slides/10-slides.html#spark-streaming-under-the-hood-1",
    "href": "slides/10-slides.html#spark-streaming-under-the-hood-1",
    "title": "Lecture 10",
    "section": "Spark Streaming under the hood",
    "text": "Spark Streaming under the hood\n\nThe loop\n\nBased on the configured trigger interval, the thread checks the streaming sources for the availability of new data.\nIf available, the new data is executed by running a micro-batch. From the optimized logical plan, an optimized Spark execution plan is generated that reads the new data from the source, incrementally computes the updated result, and writes the output to the sink according to the configured output mode.\nFor every micro-batch, the exact range of data processed (e.g., the set of files or the range of Apache Kafka offsets) and any associated state are saved in the configured checkpoint location so that the query can deterministically reproc‐ ess the exact range if needed."
  },
  {
    "objectID": "slides/10-slides.html#spark-streaming-under-the-hood-2",
    "href": "slides/10-slides.html#spark-streaming-under-the-hood-2",
    "title": "Lecture 10",
    "section": "Spark Streaming under the hood",
    "text": "Spark Streaming under the hood\n\nThe loop continues until the query is terminated which can be for one of the following reasons:\n\nA failure has occurred in the query (either a processing error or a failure in the cluster).\nThe query is explicitly stopped using streamingQuery.stop().\nIf the trigger is set to Once, then the query will stop on its own after executing a single micro-batch containing all the available data."
  },
  {
    "objectID": "slides/10-slides.html#data-transformations",
    "href": "slides/10-slides.html#data-transformations",
    "title": "Lecture 10",
    "section": "Data Transformations",
    "text": "Data Transformations\nEach execution is considered as a micro-batch. DataFrame operations can be broadly classified into stateless and stateful operations based on whether executing the operation incrementally requires maintaining a state."
  },
  {
    "objectID": "slides/10-slides.html#stateless-transformations",
    "href": "slides/10-slides.html#stateless-transformations",
    "title": "Lecture 10",
    "section": "Stateless Transformations",
    "text": "Stateless Transformations\nAll projection operations (e.g., select(), explode(), map(), flatMap()) and selection operations (e.g., filter(), where()) process each input record individually without needing any information from previous rows. This lack of dependence on prior input data makes them stateless operations.\nA streaming query having only stateless operations supports the append and update output modes, but not complete mode. This makes sense: since any processed output row of such a query cannot be modified by any future data, it can be written out to all streaming sinks in append mode (including append-only ones, like files of any format). On the other hand, such queries naturally do not combine information across input records, and therefore may not reduce the volume of the data in the result. Complete mode is not supported because storing the ever-growing result data is usually costly. This is in sharp contrast with stateful transformations, as we will discuss next."
  },
  {
    "objectID": "slides/10-slides.html#stateful-transformations",
    "href": "slides/10-slides.html#stateful-transformations",
    "title": "Lecture 10",
    "section": "Stateful Transformations",
    "text": "Stateful Transformations\nThe simplest example of a stateful transformation is DataFrame.groupBy().count(), which generates a running count of the number of records received since the beginning of the query. In every micro-batch, the incremental plan adds the count of new records to the previous count generated by the previous micro-batch. This partial count communicated between plans is the state. This state is maintained in the memory of the Spark executors and is checkpointed to the configured location in order to tolerate failures. While Spark SQL automatically manages the life cycle of this state to ensure correct results, you typically have to tweak a few knobs to control the resource usage for maintaining state. In this section, we are going to explore how different stateful operators manage their state under the hood."
  },
  {
    "objectID": "slides/10-slides.html#stateful-streaming-aggregations",
    "href": "slides/10-slides.html#stateful-streaming-aggregations",
    "title": "Lecture 10",
    "section": "Stateful Streaming Aggregations",
    "text": "Stateful Streaming Aggregations\nStructured Streaming can incrementally execute most DataFrame aggregation operations. You can aggregate data by keys (e.g., streaming word count) and/or by time (e.g., count records received every hour).\n\nAggregations Not Based on Time\nAggregations with Event-Time Windows"
  },
  {
    "objectID": "slides/10-slides.html#mapping-of-event-time-to-tumbling-windows",
    "href": "slides/10-slides.html#mapping-of-event-time-to-tumbling-windows",
    "title": "Lecture 10",
    "section": "Mapping of event time to tumbling windows",
    "text": "Mapping of event time to tumbling windows"
  },
  {
    "objectID": "slides/10-slides.html#mapping-of-event-time-to-multiple-overlapping-windows",
    "href": "slides/10-slides.html#mapping-of-event-time-to-multiple-overlapping-windows",
    "title": "Lecture 10",
    "section": "Mapping of event time to multiple overlapping windows",
    "text": "Mapping of event time to multiple overlapping windows"
  },
  {
    "objectID": "slides/10-slides.html#updated-counts-in-the-result-table-after-each-five-minute-trigger",
    "href": "slides/10-slides.html#updated-counts-in-the-result-table-after-each-five-minute-trigger",
    "title": "Lecture 10",
    "section": "Updated counts in the result table after each five-minute trigger",
    "text": "Updated counts in the result table after each five-minute trigger"
  },
  {
    "objectID": "slides/10-slides.html#using-watermarks",
    "href": "slides/10-slides.html#using-watermarks",
    "title": "Lecture 10",
    "section": "Using watermarks",
    "text": "Using watermarks\nA watermark is defined as a moving threshold in event time that trails behind the maximum event time seen by the query in the processed data. The trailing gap, known as the watermark delay, defines how long the engine will wait for late data to arrive."
  },
  {
    "objectID": "slides/10-slides.html#aws-kinesis",
    "href": "slides/10-slides.html#aws-kinesis",
    "title": "Lecture 10",
    "section": "AWS Kinesis",
    "text": "AWS Kinesis\n\n\n\n\nSimilar to Apache Kafka\nAbstracts away much of the configuration details\nCosts based on usage\n\nRead more comparisons here."
  },
  {
    "objectID": "slides/10-slides.html#spark-sql-connector-for-aws-kinesis",
    "href": "slides/10-slides.html#spark-sql-connector-for-aws-kinesis",
    "title": "Lecture 10",
    "section": "Spark SQL Connector for AWS Kinesis",
    "text": "Spark SQL Connector for AWS Kinesis\nConnector to make Kinesis work with Spark Structured Streaming\nhttps://github.com/qubole/kinesis-sql"
  },
  {
    "objectID": "slides/08-slides.html#aws-academy",
    "href": "slides/08-slides.html#aws-academy",
    "title": "Lecture 8",
    "section": "AWS Academy",
    "text": "AWS Academy\n\nCredit limit - $100\nCourse numbers:\n\nCourse #1 - 24178\nCourse #2 - 27354\nCourse #3 - 22802\nCourse #4 - 26418\n\n\nSTAY WITH COURSE 24178 UNLESS YOU HAVE RUN OUT OF CREDITS OR &gt;$90 USED!\nNote that you will have to repeat several setup steps:\n\nsecurity group\nEC2 keypair uploading (the AWS part only)\nsagemaker setup\nany S3 uploading or copying as well as bucket creation as necessary\nEMR configuration"
  },
  {
    "objectID": "slides/08-slides.html#grades",
    "href": "slides/08-slides.html#grades",
    "title": "Lecture 8",
    "section": "Grades",
    "text": "Grades\n\nYou are responsible for your grades\nIf there is a problem, you need to bring it up\nAll assignments are mandatory\nState of assignments from syllabus\nGroup Project: 30%\nHW Assignments: 45%\nLab Deliverables: 15% - Halfway through\nQuizzes: 10% - One to two more"
  },
  {
    "objectID": "slides/08-slides.html#review-of-file-systems",
    "href": "slides/08-slides.html#review-of-file-systems",
    "title": "Lecture 8",
    "section": "Review of File Systems",
    "text": "Review of File Systems\nWhat are the possible file system options for each item: S3, HDFS, Local file system\n\nhadoop jar /usr/lib/hadoop/hadoop-streaming.jar #1\n\n\n\n-files basic-mapper.py,basic-reducer.py #2\n\n\n\n-input /user/hadoop/in_data #3\n-output /user/hadoop/in_data #3\n\n\n\n-mapper basic-mapper.py #4\n-reducer basic-reducer.py #4\n\n\n\nLocal file system\n\n\n\n\nLocal file system or S3\n\n\n\n\nHDFS or S3\n\n\n\n\nHDFS - why??"
  },
  {
    "objectID": "slides/08-slides.html#connected-and-extensible",
    "href": "slides/08-slides.html#connected-and-extensible",
    "title": "Lecture 8",
    "section": "Connected and extensible",
    "text": "Connected and extensible"
  },
  {
    "objectID": "slides/08-slides.html#caching-and-persistence",
    "href": "slides/08-slides.html#caching-and-persistence",
    "title": "Lecture 8",
    "section": "Caching and Persistence",
    "text": "Caching and Persistence\nBy default, RDDs are recomputed every time you run an action on them. This can be expensive (in time) if you need to use a dataset more than once.\nSpark allows you to control what is cached in memory.\nTo tell spark to cache an object in memory, use persist() or cache():\n\ncache(): is a shortcut for using default storage level, which is memory only\npersist(): can be customized to other ways to persist data (including both memory and/or disk)\n\n# caches error RDD in memory, but only after an action is run\nerrors = logs.filter(lambda x: \"error\" in x and \"2019-12\" in x).cache()"
  },
  {
    "objectID": "slides/08-slides.html#review-of-pysparksql-cheatsheet",
    "href": "slides/08-slides.html#review-of-pysparksql-cheatsheet",
    "title": "Lecture 8",
    "section": "Review of PySparkSQL Cheatsheet",
    "text": "Review of PySparkSQL Cheatsheet\nhttps://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf"
  },
  {
    "objectID": "slides/08-slides.html#collect-caution",
    "href": "slides/08-slides.html#collect-caution",
    "title": "Lecture 8",
    "section": "collect CAUTION",
    "text": "collect CAUTION"
  },
  {
    "objectID": "slides/08-slides.html#understanding-how-the-cluster-is-running-your-job",
    "href": "slides/08-slides.html#understanding-how-the-cluster-is-running-your-job",
    "title": "Lecture 8",
    "section": "Understanding how the cluster is running your job",
    "text": "Understanding how the cluster is running your job\nSpark Application UI shows important facts about you Spark job:\n\nEvent timeline for each stage of your work\nDirected acyclical graph (DAG) of your job\nSpark job history\nStatus of Spark executors\nPhysical / logical plans for any SQL queries\n\nTool to confirm you are getting the horizontal scaling that you need!\nAdapted from AWS Glue Spark UI docs and Spark UI docs"
  },
  {
    "objectID": "slides/08-slides.html#spark-ui---event-timeline",
    "href": "slides/08-slides.html#spark-ui---event-timeline",
    "title": "Lecture 8",
    "section": "Spark UI - Event timeline",
    "text": "Spark UI - Event timeline"
  },
  {
    "objectID": "slides/08-slides.html#spark-ui---dag",
    "href": "slides/08-slides.html#spark-ui---dag",
    "title": "Lecture 8",
    "section": "Spark UI - DAG",
    "text": "Spark UI - DAG"
  },
  {
    "objectID": "slides/08-slides.html#spark-ui---job-history",
    "href": "slides/08-slides.html#spark-ui---job-history",
    "title": "Lecture 8",
    "section": "Spark UI - Job History",
    "text": "Spark UI - Job History"
  },
  {
    "objectID": "slides/08-slides.html#spark-ui---executors",
    "href": "slides/08-slides.html#spark-ui---executors",
    "title": "Lecture 8",
    "section": "Spark UI - Executors",
    "text": "Spark UI - Executors"
  },
  {
    "objectID": "slides/08-slides.html#spark-ui---sql",
    "href": "slides/08-slides.html#spark-ui---sql",
    "title": "Lecture 8",
    "section": "Spark UI - SQL",
    "text": "Spark UI - SQL"
  },
  {
    "objectID": "slides/08-slides.html#udf-workflow",
    "href": "slides/08-slides.html#udf-workflow",
    "title": "Lecture 8",
    "section": "UDF Workflow",
    "text": "UDF Workflow\n\n\n\nAdapted from UDFs in Spark"
  },
  {
    "objectID": "slides/08-slides.html#udf-example",
    "href": "slides/08-slides.html#udf-example",
    "title": "Lecture 8",
    "section": "UDF Example",
    "text": "UDF Example\nProblem: make a new column with ages for adults-only\n+-------+--------------+\n|room_id|   guests_ages|\n+-------+--------------+\n|      1|  [18, 19, 17]|\n|      2|   [25, 27, 5]|\n|      3|[34, 38, 8, 7]|\n+-------+--------------+"
  },
  {
    "objectID": "slides/08-slides.html#udf-code-solution",
    "href": "slides/08-slides.html#udf-code-solution",
    "title": "Lecture 8",
    "section": "UDF Code Solution",
    "text": "UDF Code Solution\nfrom pyspark.sql.functions import udf, col\n\n@udf(\"array&lt;integer&gt;\")\n   def filter_adults(elements):\n   return list(filter(lambda x: x &gt;= 18, elements))\n\n# alternatively\nfrom pyspark.sql.types IntegerType, ArrayType\n@udf(returnType=ArrayType(IntegerType()))\ndef filter_adults(elements):\n   return list(filter(lambda x: x &gt;= 18, elements))\n+-------+----------------+------------+\n|room_id| guests_ages    | adults_ages|\n+-------+----------------+------------+\n| 1     | [18, 19, 17]   |    [18, 19]|\n| 2     | [25, 27, 5]    |    [25, 27]|\n| 3     | [34, 38, 8, 7] |    [34, 38]|\n| 4     |[56, 49, 18, 17]|[56, 49, 18]|\n+-------+----------------+------------+"
  },
  {
    "objectID": "slides/08-slides.html#alternative-to-spark-udf",
    "href": "slides/08-slides.html#alternative-to-spark-udf",
    "title": "Lecture 8",
    "section": "Alternative to Spark UDF",
    "text": "Alternative to Spark UDF\n# Spark 3.1\nfrom pyspark.sql.functions import col, filter, lit\n\ndf.withColumn('adults_ages',\n              filter(col('guests_ages'), lambda x: x &gt;= lit(18))).show()"
  },
  {
    "objectID": "slides/08-slides.html#udf-speed-comparison",
    "href": "slides/08-slides.html#udf-speed-comparison",
    "title": "Lecture 8",
    "section": "UDF Speed Comparison",
    "text": "UDF Speed Comparison\n\n\n\nCosts:\n\nSerialization/deserialization (think pickle files)\nData movement between JVM and Python\nLess Spark optimization possible\n\nOther ways to make your Spark jobs faster source:\n\nCache/persist your data into memory\nUsing Spark DataFrames over Spark RDDs\nUsing Spark SQL functions before jumping into UDFs\nSave to serialized data formats like Parquet"
  },
  {
    "objectID": "slides/08-slides.html#pandas-udf",
    "href": "slides/08-slides.html#pandas-udf",
    "title": "Lecture 8",
    "section": "Pandas UDF",
    "text": "Pandas UDF\nFrom PySpark docs - Pandas UDFs are user defined functions that are executed by Spark using Arrow to transfer data and Pandas to work with the data, which allows vectorized operations. A Pandas UDF is defined using the pandas_udf as a decorator or to wrap the function, and no additional configuration is required. A Pandas UDF behaves as a regular PySpark function API in general.\n@pandas_udf(\"string\")\ndef to_upper(s: pd.Series) -&gt; pd.Series:\n    return s.str.upper()\n\ndf = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\ndf.select(to_upper(\"name\")).show()\n+--------------+\n|to_upper(name)|\n+--------------+\n|      JOHN DOE|\n+--------------+\n@pandas_udf(\"first string, last string\")\ndef split_expand(s: pd.Series) -&gt; pd.DataFrame:\n    return s.str.split(expand=True)\n\n\ndf = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\ndf.select(split_expand(\"name\")).show()\n+------------------+\n|split_expand(name)|\n+------------------+\n|       [John, Doe]|\n+------------------+\nhttps://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.pandas_udf.html"
  },
  {
    "objectID": "slides/08-slides.html#overview-of-ml",
    "href": "slides/08-slides.html#overview-of-ml",
    "title": "Lecture 8",
    "section": "Overview of ML",
    "text": "Overview of ML"
  },
  {
    "objectID": "slides/08-slides.html#the-advanced-analyticsml-process",
    "href": "slides/08-slides.html#the-advanced-analyticsml-process",
    "title": "Lecture 8",
    "section": "The advanced analytics/ML process",
    "text": "The advanced analytics/ML process\n\n\nGather and collect data\nClean and inspect data\nPerform feature engineering\nSplit data into train/test\nEvaluate and compare models"
  },
  {
    "objectID": "slides/08-slides.html#what-is-mllib",
    "href": "slides/08-slides.html#what-is-mllib",
    "title": "Lecture 8",
    "section": "What is MLlib",
    "text": "What is MLlib\n\nCapabilities\n\nGather and clean data\nPerform feature engineering\nPerform feature selection\nTrain and tune models\nPut models in production\n\nAPI is divided into two packages\norg.apache.spark.ml (High level API) - Provides higher level API built on top of DataFrames - Allows the construction of ML pipelines\norg.apache.spark.mllib (Predates DataFrames) - Original API built on top of RDDs\n\nInspired by scikit-learn\n\nDataFrame\nTransformer\nEstimator\nPipeline\nParameter"
  },
  {
    "objectID": "slides/08-slides.html#transformers",
    "href": "slides/08-slides.html#transformers",
    "title": "Lecture 8",
    "section": "Transformers",
    "text": "Transformers\n\nTransformers take DataFrames as input, and return a new DataFrame as output. Transformers do not learn any parameters from the data, they simply apply rule-based transformations to either prepare data for model training or generate predictions using a trained model.\nTransformers are run using the .transform() method"
  },
  {
    "objectID": "slides/08-slides.html#some-examples-of-transformers",
    "href": "slides/08-slides.html#some-examples-of-transformers",
    "title": "Lecture 8",
    "section": "Some Examples of Transformers",
    "text": "Some Examples of Transformers\n\nConverting categorical variables to numeric (must do this)\n\nStringIndexer\nOneHotEncoder can act on multiple columns at a time\n\nData Normalization\n\nNormalizer\nStandardScaler\n\nString Operations\n\nTokenizer\nStopWordsRemover\nPCA\n\nConverting continuous to discrete\n\nBucketizer\nQuantileDiscretizer\n\nMany more\n\nSpark 2.4.7: http://spark.apache.org/docs/2.4.7/ml-guide.html\nSpark 3.1.1: http://spark.apache.org/docs/3.1.1/ml-guide.html"
  },
  {
    "objectID": "slides/08-slides.html#mllib-algorithms-for-machine-learning-models-need-single-numeric-features-column-as-input",
    "href": "slides/08-slides.html#mllib-algorithms-for-machine-learning-models-need-single-numeric-features-column-as-input",
    "title": "Lecture 8",
    "section": "MLlib algorithms for machine learning models need single, numeric features column as input",
    "text": "MLlib algorithms for machine learning models need single, numeric features column as input\nEach row in this column contains a vector of data points corresponding to the set of features used for prediction.\n\nUse the VectorAssembler transformer to create a single vector column from a list of columns.\nAll categorical data needs to be numeric for machine learning\n\n# Example from Spark docs\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\ndataset = spark.createDataFrame(\n    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],\n    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\n\nassembler = VectorAssembler(\n    inputCols=[\"hour\", \"mobile\", \"userFeatures\"],\n    outputCol=\"features\")\n\noutput = assembler.transform(dataset)\nprint(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\noutput.select(\"features\", \"clicked\").show(truncate=False)"
  },
  {
    "objectID": "slides/08-slides.html#estimators",
    "href": "slides/08-slides.html#estimators",
    "title": "Lecture 8",
    "section": "Estimators",
    "text": "Estimators\n\nEstimators learn (or “fit”) parameters from your DataFrame via the .fit() method, and return a model which is a Transformer"
  },
  {
    "objectID": "slides/08-slides.html#pipelines",
    "href": "slides/08-slides.html#pipelines",
    "title": "Lecture 8",
    "section": "Pipelines",
    "text": "Pipelines"
  },
  {
    "objectID": "slides/08-slides.html#pipelines-1",
    "href": "slides/08-slides.html#pipelines-1",
    "title": "Lecture 8",
    "section": "Pipelines",
    "text": "Pipelines\n\nPipelines combine multiple steps into a single workflow that can be easily run. * Data cleaning and feature processing via transformers, using stages * Model definition * Run the pipeline to do all transformations and fit the model\nThe Pipeline constructor takes an array of pipeline stages"
  },
  {
    "objectID": "slides/08-slides.html#why-pipelines",
    "href": "slides/08-slides.html#why-pipelines",
    "title": "Lecture 8",
    "section": "Why Pipelines?",
    "text": "Why Pipelines?\n\nCleaner Code: Accounting for data at each step of preprocessing can get messy. With a Pipeline, you won’t need to manually keep track of your training and validation data at each step.\nFewer Bugs: There are fewer opportunities to misapply a step or forget a preprocessing step.\nEasier to Productionize: It can be surprisingly hard to transition a model from a prototype to something deployable at scale, but Pipelines can help.\nMore Options for Model Validation: We can easily apply cross-validation and other techniques to our Pipelines."
  },
  {
    "objectID": "slides/08-slides.html#example",
    "href": "slides/08-slides.html#example",
    "title": "Lecture 8",
    "section": "Example",
    "text": "Example\nUsing the HMP Dataset. The structure of the dataset looks like this:\n# read data from text file and split each line into words\ndf.printSchema()\n\nroot  \n|-- x: integer (nullable = true)  \n|-- y: integer (nullable = true)  \n|-- z: integer (nullable = true)  \n|-- class: string (nullable = false)  \n|-- source: string (nullable = false)"
  },
  {
    "objectID": "slides/08-slides.html#lets-transform-strings-to-numbers",
    "href": "slides/08-slides.html#lets-transform-strings-to-numbers",
    "title": "Lecture 8",
    "section": "Let’s transform strings to numbers",
    "text": "Let’s transform strings to numbers\n\nThe StringIndexer is an estimator having both fit and transform methods.\nTo create a StringIndexer object (indexer), pass the class column as inputCol, and classIndex as outputCol.\nThen we fit the DataFrame to the indexer (to run the estimator), and transform the DataFrame. This creates a brand new DataFrame (indexed), which we can see below, containing the classIndex additional column.\n\nfrom pyspark.ml.feature import StringIndexer\n\nindexer = StringIndexer(inputCol = 'class', outputCol = 'classIndex')\nindexed = indexer.fit(df).transform(df)  # This is a new data frame\n\n# Let's see it\nindexed.show(5)\n\n+---+---+---+-----------+--------------------+----------+\n|  x|  y|  z|      class|              source|classIndex|\n+---+---+---+-----------+--------------------+----------+\n| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|\n| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|\n| 28| 38| 52|Drink_glass|Accelerometer-201...|       2.0|\n| 29| 37| 51|Drink_glass|Accelerometer-201...|       2.0|\n| 30| 38| 52|Drink_glass|Accelerometer-201...|       2.0|\n+---+---+---+-----------+--------------------+----------+\nonly showing top 5 rows"
  },
  {
    "objectID": "slides/08-slides.html#one-hot-encode-categorical-variables",
    "href": "slides/08-slides.html#one-hot-encode-categorical-variables",
    "title": "Lecture 8",
    "section": "One-Hot Encode categorical variables",
    "text": "One-Hot Encode categorical variables\n\nUnlike the StringIndexer, OneHotEncoder is a pure transformer, having only the transform method. It uses the same syntax of inputCol and outputCol.\nOneHotEncoder creates a brand new DataFrame (encoded), with a category_vec column added to the previous DataFrame(indexed).\nOneHotEncoder doesn’t return several columns containing only zeros and ones; it returns a sparse-vector as seen in the categoryVec column. Thus, for the ‘Drink_glass’ class above, SparkML returns a sparse-vector that basically says there are 13 elements, and at position 2, the class value exists(1.0).\n\nfrom pyspark.ml.feature import OneHotEncoder\n\n# The OneHotEncoder is a pure transformer object. it does not use the fit()\nencoder = OneHotEncoder(inputCol = 'classIndex', outputCol = 'categoryVec')\nencoded = encoder.transform(indexed)  # This is a new data frame\nencoded.show(5, False)\n\n+---+---+---+-----------+----------------------------------------------------+----------+--------------+\n|x  |y  |z  |class      |source                                              |classIndex|categoryVec   |\n+---+---+---+-----------+----------------------------------------------------+----------+--------------+\n|29 |39 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n|29 |39 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n|28 |38 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n|29 |37 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n|30 |38 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n+---+---+---+-----------+----------------------------------------------------+----------+--------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "slides/08-slides.html#assembling-the-feature-vector",
    "href": "slides/08-slides.html#assembling-the-feature-vector",
    "title": "Lecture 8",
    "section": "Assembling the feature vector",
    "text": "Assembling the feature vector\n\nThe VectorAssembler object gets initialized with the same syntax as StringIndexer and OneHotEncoder.\nThe list ['x', 'y', 'z'] is paseed to inputCols, and we specify outputCol = ‘features’.\nThis is also a pure transformer.\n\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n# VectorAssembler creates vectors from ordinary data types for us\n\nvectorAssembler = VectorAssembler(inputCols = ['x','y','z'], outputCol = 'features')\n# Now we use the vectorAssembler object to transform our last updated dataframe\n\nfeatures_vectorized = vectorAssembler.transform(encoded)  # note this is a new df\n\nfeatures_vectorized.show(5, False)\n\n+---+---+---+-----------+----------------------------------------------------+----------+--------------+----------------+\n|x  |y  |z  |class      |source                                              |classIndex|categoryVec   |features        |\n+---+---+---+-----------+----------------------------------------------------+----------+--------------+----------------+\n|29 |39 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[29.0,39.0,51.0]|\n|29 |39 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[29.0,39.0,51.0]|\n|28 |38 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[28.0,38.0,52.0]|\n|29 |37 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[29.0,37.0,51.0]|\n|30 |38 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[30.0,38.0,52.0]|\n+---+---+---+-----------+----------------------------------------------------+----------+--------------+----------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "slides/08-slides.html#normalizing-the-dataset",
    "href": "slides/08-slides.html#normalizing-the-dataset",
    "title": "Lecture 8",
    "section": "Normalizing the dataset",
    "text": "Normalizing the dataset\n\nNormalizer like all transformers have consistent syntax. Looking at the Normalizer object, it contains the parameter p=1.0 (the default norm value for Pyspark Normalizer is p=2.0.)\np=1.0 uses Manhattan Distance\np=2.0 uses Euclidean Distance\n\nfrom pyspark.ml.feature import Normalizer\nnormalizer = Normalizer(inputCol = 'features', outputCol = 'features_norm', p=1.0)  # Manhattan Distance\nnormalized_data = normalizer.transform(features_vectorized) # New data frame too.\n\nnormalized_data.show(5)\n&gt;&gt;\n+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n|  x|  y|  z|      class|              source|classIndex|   categoryVec|        features|       features_norm|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,51.0]|[0.24369747899159...|\n| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,51.0]|[0.24369747899159...|\n| 28| 38| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[28.0,38.0,52.0]|[0.23728813559322...|\n| 29| 37| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,37.0,51.0]|[0.24786324786324...|\n| 30| 38| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[30.0,38.0,52.0]|[0.25,0.316666666...|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "slides/08-slides.html#running-the-transformation-pipeline",
    "href": "slides/08-slides.html#running-the-transformation-pipeline",
    "title": "Lecture 8",
    "section": "Running the transformation Pipeline",
    "text": "Running the transformation Pipeline\nThe Pipeline constructor below takes an array of Pipeline stages we pass to it. Here we pass the 4 stages defined earlier, in the right sequence, one after another.\nfrom pyspark.ml import Pipeline\npipeline = Pipeline(stages = [indexer,encoder,vectorAssembler,normalizer])\nNow the Pipeline object fit to our original DataFrame df\ndata_model = pipeline.fit(df)\nFinally, we transform our DataFrame df using the Pipeline Object.\npipelined_data = data_model.transform(df)\n\npipelined_data.show(5)\n\n+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n|  x|  y|  z|      class|              source|classIndex|   categoryVec|        features|       features_norm|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,51.0]|[0.24369747899159...|\n| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,51.0]|[0.24369747899159...|\n| 28| 38| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[28.0,38.0,52.0]|[0.23728813559322...|\n| 29| 37| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,37.0,51.0]|[0.24786324786324...|\n| 30| 38| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[30.0,38.0,52.0]|[0.25,0.316666666...|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "slides/08-slides.html#one-last-step-before-training",
    "href": "slides/08-slides.html#one-last-step-before-training",
    "title": "Lecture 8",
    "section": "One last step before training",
    "text": "One last step before training\n# first let's list out the columns we want to drop\ncols_to_drop = ['x','y','z','class','source','classIndex','features']\n\n# Next let's use a list comprehension with conditionals to select cols we need\nselected_cols = [col for col in pipelined_data.columns if col not in cols_to_drop]\n\n# Let's define a new train_df with only the categoryVec and features_norm cols\ndf_train = pipelined_data.select(selected_cols)\n\n# Let's see our training dataframe.\ndf_train.show(5)\n\n+--------------+--------------------+\n|   categoryVec|       features_norm|\n+--------------+--------------------+\n|(13,[2],[1.0])|[0.24369747899159...|\n|(13,[2],[1.0])|[0.24369747899159...|\n|(13,[2],[1.0])|[0.23728813559322...|\n|(13,[2],[1.0])|[0.24786324786324...|\n|(13,[2],[1.0])|[0.25,0.316666666...|\nonly showing top 5 rows\nYou now have a DataFrame optimized for SparkML!"
  },
  {
    "objectID": "slides/08-slides.html#machine-learning-models-in-spark",
    "href": "slides/08-slides.html#machine-learning-models-in-spark",
    "title": "Lecture 8",
    "section": "Machine Learning Models in Spark",
    "text": "Machine Learning Models in Spark\nThere are many Spark machine learning models\nRegression\n\nLinear regression - what is the optimization method?\nSurvival regression\nGenearlized linear model (GLM)\nRandom forest regression\n\nClassification\n\nLogistic regression\nGradient-boosted tree model (GBM)\nNaive Bayes\nMultilayer perception (neural network!)\n\nOther\n\nClustering (K-means, LDA, GMM)\nAssociation rule mining"
  },
  {
    "objectID": "slides/08-slides.html#building-your-model",
    "href": "slides/08-slides.html#building-your-model",
    "title": "Lecture 8",
    "section": "Building your Model",
    "text": "Building your Model\nFeature selection using easy R-like formulas\n# Example from Spark docs\nfrom pyspark.ml.feature import RFormula\n\ndataset = spark.createDataFrame(\n    [(7, \"US\", 18, 1.0),\n     (8, \"CA\", 12, 0.0),\n     (9, \"NZ\", 15, 0.0)],\n    [\"id\", \"country\", \"hour\", \"clicked\"])\n\nformula = RFormula(\n    formula=\"clicked ~ country + hour\",\n    featuresCol=\"features\",\n    labelCol=\"label\")\n\noutput = formula.fit(dataset).transform(dataset)\noutput.select(\"features\", \"label\").show()"
  },
  {
    "objectID": "slides/08-slides.html#what-if-you-have-too-many-columns-for-your-model",
    "href": "slides/08-slides.html#what-if-you-have-too-many-columns-for-your-model",
    "title": "Lecture 8",
    "section": "What if you have too many columns for your model?",
    "text": "What if you have too many columns for your model?\nEvaluating 1,000s or 10,000s of variables?"
  },
  {
    "objectID": "slides/08-slides.html#chi-squared-selector",
    "href": "slides/08-slides.html#chi-squared-selector",
    "title": "Lecture 8",
    "section": "Chi-Squared Selector",
    "text": "Chi-Squared Selector\nPick categorical variables that are most dependent on the response variable\nCan even check the p-values of specific variables!\n# Example from Spark docs\nfrom pyspark.ml.feature import ChiSqSelector\nfrom pyspark.ml.linalg import Vectors\n\ndf = spark.createDataFrame([\n    (7, Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0,),\n    (8, Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0,),\n    (9, Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0,)], [\"id\", \"features\", \"clicked\"])\n\nselector = ChiSqSelector(numTopFeatures=1, featuresCol=\"features\",\n                         outputCol=\"selectedFeatures\", labelCol=\"clicked\")\n\nresult = selector.fit(df).transform(df)\n\nprint(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\nresult.show()"
  },
  {
    "objectID": "slides/08-slides.html#tuning-your-model",
    "href": "slides/08-slides.html#tuning-your-model",
    "title": "Lecture 8",
    "section": "Tuning your model",
    "text": "Tuning your model\nPart 1\n# Example from Spark docs\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n\n# Prepare training and test data.\ndata = spark.read.format(\"libsvm\")\\\n    .load(\"data/mllib/sample_linear_regression_data.txt\")\ntrain, test = data.randomSplit([0.9, 0.1], seed=12345)\n\nlr = LinearRegression(maxIter=10)\n\n# We use a ParamGridBuilder to construct a grid of parameters to search over.\n# TrainValidationSplit will try all combinations of values and determine best model using\n# the evaluator.\nparamGrid = ParamGridBuilder()\\\n    .addGrid(lr.regParam, [0.1, 0.01]) \\\n    .addGrid(lr.fitIntercept, [False, True])\\\n    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n    .build()"
  },
  {
    "objectID": "slides/08-slides.html#tuning-your-model-1",
    "href": "slides/08-slides.html#tuning-your-model-1",
    "title": "Lecture 8",
    "section": "Tuning your model",
    "text": "Tuning your model\nPart 2\n# In this case the estimator is simply the linear regression.\n# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\ntvs = TrainValidationSplit(estimator=lr,\n                           estimatorParamMaps=paramGrid,\n                           evaluator=RegressionEvaluator(),\n                           # 80% of the data will be used for training, 20% for validation.\n                           trainRatio=0.8)\n\n# Run TrainValidationSplit, and choose the best set of parameters.\nmodel = tvs.fit(train)\n\n# Make predictions on test data. model is the model with combination of parameters\n# that performed best.\nmodel.transform(test)\\\n    .select(\"features\", \"label\", \"prediction\")\\\n    .show()"
  },
  {
    "objectID": "slides/06-slides.html#starting-with-our-big-dataset",
    "href": "slides/06-slides.html#starting-with-our-big-dataset",
    "title": "Lecture 6",
    "section": "Starting with our BIG dataset",
    "text": "Starting with our BIG dataset"
  },
  {
    "objectID": "slides/06-slides.html#the-data-is-split",
    "href": "slides/06-slides.html#the-data-is-split",
    "title": "Lecture 6",
    "section": "The data is split",
    "text": "The data is split"
  },
  {
    "objectID": "slides/06-slides.html#the-data-is-distributed-across-a-cluster-of-machines",
    "href": "slides/06-slides.html#the-data-is-distributed-across-a-cluster-of-machines",
    "title": "Lecture 6",
    "section": "The data is distributed across a cluster of machines",
    "text": "The data is distributed across a cluster of machines\n\n\n\n\nYou can think of your split/distributed data as a single collection]"
  },
  {
    "objectID": "slides/06-slides.html#important-latency-numbers",
    "href": "slides/06-slides.html#important-latency-numbers",
    "title": "Lecture 6",
    "section": "Important Latency Numbers",
    "text": "Important Latency Numbers"
  },
  {
    "objectID": "slides/06-slides.html#memory-vs.-disk",
    "href": "slides/06-slides.html#memory-vs.-disk",
    "title": "Lecture 6",
    "section": "Memory vs. Disk",
    "text": "Memory vs. Disk"
  },
  {
    "objectID": "slides/06-slides.html#memory-vs.-network",
    "href": "slides/06-slides.html#memory-vs.-network",
    "title": "Lecture 6",
    "section": "Memory vs. Network",
    "text": "Memory vs. Network"
  },
  {
    "objectID": "slides/06-slides.html#memory-disk-and-network",
    "href": "slides/06-slides.html#memory-disk-and-network",
    "title": "Lecture 6",
    "section": "Memory, Disk and Network",
    "text": "Memory, Disk and Network"
  },
  {
    "objectID": "slides/06-slides.html#mapreducehadoop-was-groundbreaking",
    "href": "slides/06-slides.html#mapreducehadoop-was-groundbreaking",
    "title": "Lecture 6",
    "section": "MapReduce/Hadoop was groundbreaking",
    "text": "MapReduce/Hadoop was groundbreaking\n\nIt provided a simple API (map and reduce steps)\nIt provided fault tolerance, which made it possible to scale to 100s/1000s of nodes of commodity machines where the likelihood of a node failing midway through a job was very high\n\nComputations on very large datasets failed and recovered and jobs completed"
  },
  {
    "objectID": "slides/06-slides.html#fault-tolerance-came-at-a-cost",
    "href": "slides/06-slides.html#fault-tolerance-came-at-a-cost",
    "title": "Lecture 6",
    "section": "Fault tolerance came at a cost!",
    "text": "Fault tolerance came at a cost!\n\nBetween each map and reduce step, MapReduce shuffles its data and writes intermediate data to disk\n\nReading/writing to disk is 100x slower than in-memory\nNetwork communication is 1,000,000x slower than in-memory"
  },
  {
    "objectID": "slides/06-slides.html#what-is-spark",
    "href": "slides/06-slides.html#what-is-spark",
    "title": "Lecture 6",
    "section": "What is Spark",
    "text": "What is Spark\n\n\n\n\nA simple programming model that can capture streaming, batch, and interactive workloads\nRetains fault-tolerance\nUses a different strategy for handling latency: it keeps all data immutable and in memory\nProvides speed and flexibility"
  },
  {
    "objectID": "slides/06-slides.html#spark-stack",
    "href": "slides/06-slides.html#spark-stack",
    "title": "Lecture 6",
    "section": "Spark Stack",
    "text": "Spark Stack"
  },
  {
    "objectID": "slides/06-slides.html#connected-and-extensible",
    "href": "slides/06-slides.html#connected-and-extensible",
    "title": "Lecture 6",
    "section": "Connected and extensible",
    "text": "Connected and extensible"
  },
  {
    "objectID": "slides/06-slides.html#three-data-structure-apis",
    "href": "slides/06-slides.html#three-data-structure-apis",
    "title": "Lecture 6",
    "section": "Three data structure APIs",
    "text": "Three data structure APIs\n\nRDDs (Resilient Distributed Datasets)\nDataFrames SQL-like structured datasets with query operations\nDatasets A mixture of RDDs and DataFrames\n\nWe’ll only use RDDs and DataFrames in this course."
  },
  {
    "objectID": "slides/06-slides.html#spark-vs.-hadoop",
    "href": "slides/06-slides.html#spark-vs.-hadoop",
    "title": "Lecture 6",
    "section": "Spark vs. Hadoop",
    "text": "Spark vs. Hadoop\n\n\n\n\n\n\n\nHadoop Limitation\nSpark Approach\n\n\n\n\nFor iterative processes and interactive use, Hadoop and MapReduce’s mandatory dumping of output to disk proved to be a huge bottleneck. In ML, for example, users rely on iterative processes to train-test-retrain.\nSpark uses an in-memory processing paradigm, which lowers the disk IO substantially. Spark uses DAGs to store details of each transformation done on a parallelized dataset and does not process them to get results until required (lazy).\n\n\nTraditional Hadoop applications needed the data first to be copied to HDFS (or other distributed filesystem) and then did the processing.\nSpark works equally well with HDFS or any POSIX style filesystem. However, parallel Spark needs the data to be distributed.\n\n\nMappers needed a data-localization phase in which the data was written to the local filesystem to bring resilience.\nResilience in Spark is brough about by the DAGs, in which a missing RDD is re-calculated by following the path from which the RDD was created.\n\n\nHadoop is built on Java and you must use Java to take advantage of all of it’s capabilities. Although you can run non-Java scripts with Hadoop Streaming, it is still running a Java Framework.\nSpark is developed in Scala, and it has a unified API with so you can use Spark with Scala, Java, R and Python."
  },
  {
    "objectID": "slides/06-slides.html#example-word-count-yes-again",
    "href": "slides/06-slides.html#example-word-count-yes-again",
    "title": "Lecture 6",
    "section": "Example: word count (yes, again!)",
    "text": "Example: word count (yes, again!)\nThe “Hello, World!” of programming with large scale data.\n# read data from text file and split each line into words\nrdd = sc.textFile(\"...\") \n\ncount = rdd.flatMap(lambda line: line.split(\" \")) \\\n                     .map(lambda word: (word, 1)) \\\n                     .reduceByKey(lambda a,b:a +b)  # sum all the 1's for each key\nThat’s it!"
  },
  {
    "objectID": "slides/06-slides.html#transformations-and-actions-key-spark-concept",
    "href": "slides/06-slides.html#transformations-and-actions-key-spark-concept",
    "title": "Lecture 6",
    "section": "Transformations and Actions (key Spark concept)",
    "text": "Transformations and Actions (key Spark concept)"
  },
  {
    "objectID": "slides/06-slides.html#how-to-create-and-rdd",
    "href": "slides/06-slides.html#how-to-create-and-rdd",
    "title": "Lecture 6",
    "section": "How to create and RDD?",
    "text": "How to create and RDD?\nRDDs can be created in two ways:\n\n\nTransforming an existing RDD: just like a call to map on a list returns a new list, many higher order functions defined on RDDs return a new RDD\nFrom a SparkContext or SparkSession object: the SparkContext object (renamed SparkSession) can be though of as your handle to the Spark cluster. It represents a connection between the Spark Cluster and your application/client. It defines a handful of methods which can be used to create and populate a new RDD:\n\nparallelize: converts a local object into an RDD\ntextFile: reads a text file from your filesystem and returns an RDD of strings"
  },
  {
    "objectID": "slides/06-slides.html#transformations-and-actions",
    "href": "slides/06-slides.html#transformations-and-actions",
    "title": "Lecture 6",
    "section": "Transformations and Actions",
    "text": "Transformations and Actions\nSpark defines transformations and actions on RDDs:\nTransformations return new RDDs as results.    Actions compute a result based on an RDD which is either returned or saved to an external filesystem."
  },
  {
    "objectID": "slides/06-slides.html#transformations-and-actions-1",
    "href": "slides/06-slides.html#transformations-and-actions-1",
    "title": "Lecture 6",
    "section": "Transformations and Actions",
    "text": "Transformations and Actions\nSpark defines transformations and actions on RDDs:\nTransformations return new RDDs as results.  Transfomations are lazy, their result RDD is not immediately computed.   Actions compute a result based on an RDD which is either returned or saved to an external filesystem.  Actions are eager, their result is immediately computed."
  },
  {
    "objectID": "slides/06-slides.html#common-rdd-transformations",
    "href": "slides/06-slides.html#common-rdd-transformations",
    "title": "Lecture 6",
    "section": "Common RDD Transformations",
    "text": "Common RDD Transformations\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nmap\nExpresses a one-to-one transformation and transforms each element of a collection into one element of the resulting collection\n\n\nflatMap\nExpresses a one-to-many transformation and transforms each element to 0 or more elements\n\n\nfilter\nApplies filter function that returns a boolean and returs an RDD of elements that have passed the filter condition\n\n\ndistinct\nReturns RDD with duplicates removed"
  },
  {
    "objectID": "slides/06-slides.html#common-rdd-actions",
    "href": "slides/06-slides.html#common-rdd-actions",
    "title": "Lecture 6",
    "section": "Common RDD Actions",
    "text": "Common RDD Actions\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\ncollect\nReturns all distributed elements of the RDD to the driver\n\n\ncount\nReturns the number of elements in an RDD\n\n\ntake\nReturns the first n elements of the RDD\n\n\nreduce\nCombines elements of the RDD together using some function and returns result"
  },
  {
    "objectID": "slides/06-slides.html#collect-caution",
    "href": "slides/06-slides.html#collect-caution",
    "title": "Lecture 6",
    "section": "collect CAUTION",
    "text": "collect CAUTION"
  },
  {
    "objectID": "slides/06-slides.html#another-example",
    "href": "slides/06-slides.html#another-example",
    "title": "Lecture 6",
    "section": "Another example",
    "text": "Another example\nLet’s assume that we have an RDD of strings which contains gigabytes of logs collected over the previous year. Each element of the RDD represents one line of logs.\nAssuming the dates come in the form YYYY-MM-DD:HH:MM:SS and errors are logged with a prefix that includes the word “error”…\nHow would you determine the number of errors that were logged in December 2019?\n\n# read data from text file and split each line into words\nlogs = sc.textFile(\"...\") \n\n# this is a transformation\nerrors = logs.filter(lambda x: \"error\" in x and \"2019-12\" in x)\n\n# this is an action\nerrors.count()\nSpark computes RDDs the first time they are used in an action!"
  },
  {
    "objectID": "slides/06-slides.html#caching-and-persistence",
    "href": "slides/06-slides.html#caching-and-persistence",
    "title": "Lecture 6",
    "section": "Caching and Persistence",
    "text": "Caching and Persistence\nBy default, RDDs are recomputed every time you run an action on them. This can be expensive (in time) if you need to use a dataset more than once.\nSpark allows you to control what is cached in memory.\nTo tell spark to cache an object in memory, use persist() or cache():\n\ncache(): is a shortcut for using default storage level, which is memory only\npersist(): can be customized to other ways to persist data (including both memory and/or disk)\n\n# caches error RDD in memory, but only after an action is run\nerrors = logs.filter(lambda x: \"error\" in x and \"2019-12\" in x).cache()"
  },
  {
    "objectID": "slides/06-slides.html#using-memory-is-great-for-iterative-workloads",
    "href": "slides/06-slides.html#using-memory-is-great-for-iterative-workloads",
    "title": "Lecture 6",
    "section": "Using memory is great for iterative workloads",
    "text": "Using memory is great for iterative workloads"
  },
  {
    "objectID": "slides/06-slides.html#dataframes-in-a-nutshell",
    "href": "slides/06-slides.html#dataframes-in-a-nutshell",
    "title": "Lecture 6",
    "section": "DataFrames in a nutshell",
    "text": "DataFrames in a nutshell\nDataFrames are…\nDatasets organized into named columns\nConceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood.\nA relational API over Spark’s RDDs\nBecause sometimes it’s more convenient to use declarative relational APIs than functional APIs for analysis jobs:\n\n\nselect\nwhere\nlimit\n\n\n\norderBy\ngroupBy\njoin\n\nAble to be automatically aggresively optimized\nSparkSQL applies years of research on relational optimizations in the database community to Spark"
  },
  {
    "objectID": "slides/06-slides.html#dataframe-data-types",
    "href": "slides/06-slides.html#dataframe-data-types",
    "title": "Lecture 6",
    "section": "DataFrame Data Types",
    "text": "DataFrame Data Types\nSparkSQL’s DataFrames operate on a restricted (yet broad) set of data types. These are the most common:\n\nInteger types (at different lengths): ByteType, ShortType, IntegerType, LongType\nDecimal types: Float, Double\nBooleanType\nStringType\nDate/Time: TimestampType, DateType"
  },
  {
    "objectID": "slides/06-slides.html#a-dataframe",
    "href": "slides/06-slides.html#a-dataframe",
    "title": "Lecture 6",
    "section": "A DataFrame",
    "text": "A DataFrame"
  },
  {
    "objectID": "slides/06-slides.html#getting-a-look-at-your-data",
    "href": "slides/06-slides.html#getting-a-look-at-your-data",
    "title": "Lecture 6",
    "section": "Getting a look at your data",
    "text": "Getting a look at your data\nThere are a few ways you can have a look at your data in DataFrames:\n\nshow() pretty-prints DataFrame in tabular form. Shows first 20 elements\nprintSchema() prints the schema of your DataFrame in a tree format."
  },
  {
    "objectID": "slides/06-slides.html#common-dataframe-transformations",
    "href": "slides/06-slides.html#common-dataframe-transformations",
    "title": "Lecture 6",
    "section": "Common DataFrame Transformations",
    "text": "Common DataFrame Transformations\nLike on RDD’s, transformations on DataFrames are:\n\nOperations that return another DataFrame as a results\nAre lazily evaluated"
  },
  {
    "objectID": "slides/06-slides.html#some-common-transformations-include",
    "href": "slides/06-slides.html#some-common-transformations-include",
    "title": "Lecture 6",
    "section": "Some common transformations include:",
    "text": "Some common transformations include:\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nselect\nSelects a set of named columns and returns a new DataFrame with those columns as a result\n\n\nagg\nPerforms aggregations on a series of columns and returns a new DataFrame with the calculated output\n\n\ngroupBy\nGroups the DataFrame using the specified columns, usually used before some kind of aggregation\n\n\njoin\nInner join with another DataFrame\n\n\n\nOther transformations include: filter, limit, orderBy, where."
  },
  {
    "objectID": "slides/06-slides.html#specifying-columns",
    "href": "slides/06-slides.html#specifying-columns",
    "title": "Lecture 6",
    "section": "Specifying columns",
    "text": "Specifying columns\nMost methods take a parameter of type Column or String, always referring to some attribute/column in the the DataFrame.\nYou can select and work with columns in ways using the DataFrame API:\n\nUsing $ notation: df.filter($\"age\" &gt; 18)\nReferring to the DataFrame: df.filter(df(\"age\") &gt; 18)\nUsing SQL query string: df.filter(\"age &gt; 18\")"
  },
  {
    "objectID": "slides/06-slides.html#filtering-in-sparksql",
    "href": "slides/06-slides.html#filtering-in-sparksql",
    "title": "Lecture 6",
    "section": "Filtering in SparkSQL",
    "text": "Filtering in SparkSQL\nThe DataFrame API makes two methods available for filtering: filter and where. They are equivalent!\nemployee_df.filter(\"age &gt; 30\").show()\nis equivalent to\nemployee_df.where(\"age &gt; 30\").show()"
  },
  {
    "objectID": "slides/06-slides.html#use-either-dataframe-api-and-sparksql",
    "href": "slides/06-slides.html#use-either-dataframe-api-and-sparksql",
    "title": "Lecture 6",
    "section": "Use either DataFrame API and SparkSQL",
    "text": "Use either DataFrame API and SparkSQL\nThe DataFrame API and SparkSQL syntax can be used interchangeably!\nExample: return the firstname and lastname of all the employees over the age over 25 that reside in Washington D.C."
  },
  {
    "objectID": "slides/06-slides.html#dataframe-api",
    "href": "slides/06-slides.html#dataframe-api",
    "title": "Lecture 6",
    "section": "DataFrame API",
    "text": "DataFrame API\nresults = df.select(\"firstname\", \"lastname\") \\\n            .where(\"city == 'Washington D.C.' && age &gt;= 25\")"
  },
  {
    "objectID": "slides/06-slides.html#sparksql",
    "href": "slides/06-slides.html#sparksql",
    "title": "Lecture 6",
    "section": "SparkSQL",
    "text": "SparkSQL\nspark.sql(\"select firstname, lastname from df_view where city == 'Washington D.C.' and age &gt;= 25\")\n          \n# * Note: you have to register `df` using `df.createOrReplaceTempView(\"df_view\")`"
  },
  {
    "objectID": "slides/06-slides.html#grouping-and-aggregating-on-dataframes",
    "href": "slides/06-slides.html#grouping-and-aggregating-on-dataframes",
    "title": "Lecture 6",
    "section": "Grouping and aggregating on DataFrames",
    "text": "Grouping and aggregating on DataFrames\nSome of the most common tasks on structured data tables include:\n\nGrouping by a certain attributed\nDoing some kind of aggregation on the grouping, like a count\n\nFor grouping and aggregating, SparkSQL provides a groupBy function which returns a RelationalGroupedDataset which has several standard aggregation functions like count, sum, max, min, and avg."
  },
  {
    "objectID": "slides/06-slides.html#how-to-group",
    "href": "slides/06-slides.html#how-to-group",
    "title": "Lecture 6",
    "section": "How to group",
    "text": "How to group\n\nCall a groupBy on a specific attribute/column of a DataFrame\nfollowed by a call to agg\n\nresults = df.groupBy(\"state\") \\\n            .agg(sum(\"sales\"))"
  },
  {
    "objectID": "slides/06-slides.html#actions-on-dataframes",
    "href": "slides/06-slides.html#actions-on-dataframes",
    "title": "Lecture 6",
    "section": "Actions on DataFrames",
    "text": "Actions on DataFrames\nLike RDDs, DataFrames also have their own set of actions:\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\ncollect\nReturns an array that contains all the rows in the DataFrame to the driver\n\n\ncount\nReturns the number of rows in a DataFrame\n\n\nfirst\nReturns the first row in the DataFrame\n\n\nshow\nDisplays the top 20 rows in the DataFrame\n\n\ntake\nReturns the first n rows of the RDD"
  },
  {
    "objectID": "slides/06-slides.html#collect-caution-1",
    "href": "slides/06-slides.html#collect-caution-1",
    "title": "Lecture 6",
    "section": "collect CAUTION",
    "text": "collect CAUTION"
  },
  {
    "objectID": "slides/06-slides.html#limitations-on-dataframe",
    "href": "slides/06-slides.html#limitations-on-dataframe",
    "title": "Lecture 6",
    "section": "Limitations on DataFrame",
    "text": "Limitations on DataFrame\n\nCan only use DataFrame data types\nIf your unstructured data cannot be reformulated to adhere to some kind of schema, it would be better to use RDDs."
  },
  {
    "objectID": "slides/04-slides.html#agenda-and-goals-for-today",
    "href": "slides/04-slides.html#agenda-and-goals-for-today",
    "title": "Lecture 5",
    "section": "Agenda and Goals for Today",
    "text": "Agenda and Goals for Today\n\n\nLecture\n\nDistributed file systems\nModern file types\nWorking with large tabular data on a single node\n\nDuckDB\nPolars\n\n\n\nLab\n\nRun a similar task with Pandas, polars and duckdb"
  },
  {
    "objectID": "slides/04-slides.html#logistics-and-review",
    "href": "slides/04-slides.html#logistics-and-review",
    "title": "Lecture 5",
    "section": "Logistics and Review",
    "text": "Logistics and Review\n\n\nDeadlines\n\nAssignment 1: Python Skills Due Sept 5 11:59pm\nLab 2: Cloud Tooling Due Sept 5 6pm\nAssignment 2: Shell & Linux Due Sept 11 11:59pm\nLab 3: Parallel Computing Due Sept 12 6pm\nAssignment 3: Parallelization Due Sept 18 11:59pm\nLab 4: Docker and Lambda Due Sept 19 6pm\nAssignment 4: Containers Due Sept 25 11:59pm\nLab 5: DuckDB & Polars Due Sept 26 6pm\n\n\nLook back and ahead\n\nContinue to use Slack for questions!\nDocker (containerization)\nLambda functions\nComing up: Spark and project"
  },
  {
    "objectID": "slides/04-slides.html#raw-ingredients-of-storage-systems",
    "href": "slides/04-slides.html#raw-ingredients-of-storage-systems",
    "title": "Lecture 5",
    "section": "Raw ingredients of storage systems",
    "text": "Raw ingredients of storage systems\n\n\n\nDisk drives (magnetic HDDs or SSDs)\nRAM\nNetworking and CPU\nSerialization\nCompression\nCaching\n\n\n\n\n\n\nReis, J., Housley, M. (2022). Fundamentals of Data Engineering. United States: O’Reilly Media."
  },
  {
    "objectID": "slides/04-slides.html#single-machine-vs.-distributed-storage",
    "href": "slides/04-slides.html#single-machine-vs.-distributed-storage",
    "title": "Lecture 5",
    "section": "Single machine vs. distributed storage",
    "text": "Single machine vs. distributed storage\n\n\n\n\n\n\n\n\n\n\n\nSingle machine\n\nThey are commonly used for storing operating system files, application files, and user data files.\nFilesystems are also used in databases to store data files, transaction logs, and backups.\n\n\n\nDistributed storage\n\nA distributed filesystem is a type of filesystem that spans multiple computers.\nIt provides a unified view of files across all the computers in the system.\nHave existed before cloud\n\n\n\n\n\n\nReis, J., Housley, M. (2022). Fundamentals of Data Engineering. United States: O’Reilly Media."
  },
  {
    "objectID": "slides/04-slides.html#file-storage",
    "href": "slides/04-slides.html#file-storage",
    "title": "Lecture 5",
    "section": "File storage",
    "text": "File storage\nA file is a data entity with specific read, write, and reference characteristics used by software and operating systems.\n\n\n\n\n\n\nLocal disk\n\nOperating system–managed filesystems on local disk partition of SSD or magnetic disk:\n\nNTFS (Windows)\nHFS+ (MacOS)\next4 (Linux)() on a local disk partition of SSD or magnetic disk\n\n\n\n\nNetwork-attached (NAS)\n\nFile storage system to clients over a network\nIncluding redundancy and reliability, fine-grained control of resources, storage pooling across multiple disks for large virtual volumes, and file sharing across multiple machines\n\n\n\nCloud filesystems\n\nNot object store (more on that later)\nNot the virtual hard drive attached to a virtual machine\nFully managed filesystem which takes care of networking, managing disk clusters, failures, and configuration (Azure Files, Amazon Elastic Filesystem)\nBacked by Object Store\n\n\n\n\n\n\nReis, J., Housley, M. (2022). Fundamentals of Data Engineering. United States: O’Reilly Media."
  },
  {
    "objectID": "slides/04-slides.html#object-stores",
    "href": "slides/04-slides.html#object-stores",
    "title": "Lecture 5",
    "section": "Object stores",
    "text": "Object stores\nThe term object storage is somewhat confusing because object has several meanings in computer science. In this context, we’re talking about a specialized file-like construct. It could be any type of file: TXT, CSV, JSON, images, videos, audio, or pretty much any type of file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContains objects of all shapes and sizes.\nEvery object gets a unique identifier\nObjects are immutable; cannot be modifier in place (unlike local FS)\nDistributed by design\nMassively scalable REST API access"
  },
  {
    "objectID": "slides/04-slides.html#distributed-fs-vs-object-store",
    "href": "slides/04-slides.html#distributed-fs-vs-object-store",
    "title": "Lecture 5",
    "section": "Distributed FS vs Object Store",
    "text": "Distributed FS vs Object Store\n\n\n\n\n\n\n\n\n\nDistributed File System\nObject Storage\n\n\n\n\nOrganization\nFiles in hierarchical directories\nFlat organization (though there can be overlays to provide hierarchical files structure)\n\n\nMethod\nPOSIX File Operations\nREST API\n\n\nImmutability\nNone: Random writes anywhere in file\nImmutable: need to replace/append entire object\n\n\nScalability\nMillions of files\nBillions of objects\n\n\n\nBoth provide:\n\nFault tolerance\nAvailability and consistency"
  },
  {
    "objectID": "slides/04-slides.html#before-data-locality-for-hadoop",
    "href": "slides/04-slides.html#before-data-locality-for-hadoop",
    "title": "Lecture 5",
    "section": "Before: Data locality (for Hadoop)",
    "text": "Before: Data locality (for Hadoop)\n\n\n\n\n\n\n\nWhite, T. E. (2015). Hadoop: The Definitive Guide, 4th Edition. United States: O’Reilly Media, Incorporated."
  },
  {
    "objectID": "slides/04-slides.html#today-de-coupling-storage-from-compute",
    "href": "slides/04-slides.html#today-de-coupling-storage-from-compute",
    "title": "Lecture 5",
    "section": "Today: de-coupling storage from compute",
    "text": "Today: de-coupling storage from compute\n\n\n\n\n\n\n\nGopalan, R. (2022). The Cloud Data Lake. United States: O’Reilly Media."
  },
  {
    "objectID": "slides/04-slides.html#plain-text-csv-tdf-fwf",
    "href": "slides/04-slides.html#plain-text-csv-tdf-fwf",
    "title": "Lecture 5",
    "section": "Plain Text (CSV, TDF, FWF)",
    "text": "Plain Text (CSV, TDF, FWF)\n\n\n\n\n\n\nPay attention to encodings!\nLines end in linefeed, carriage-return, or both together depending on the OS that generated\nTypically, a single line of text contains a single record"
  },
  {
    "objectID": "slides/04-slides.html#json",
    "href": "slides/04-slides.html#json",
    "title": "Lecture 5",
    "section": "JSON",
    "text": "JSON\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nJSON files have two flavors: JSON Lines vs. JSON. Typically when we say data is in JSON format, we imply it’s JSON Lines which means that there is a single JSON object per line, and there are multiple lines.\n\n\n\n\n\n\n\nJSON Lines\nFour records, one per line. No ending comma.\n{\"id\":1, \"name\":\"marck\", \"last_name\":\"vaisman\"}\n{\"id\":2, \"name\":\"anderson\", \"last_name\":\"monken\"}\n{\"id\":3, \"name\":\"amit\", \"last_name\":\"arora\"}\n{\"id\":4, \"name\":\"abhijit\", \"last_name\":\"dasgupta\"}\n\n\nJSON\nFour records enclosed in a JSON Array\n[\n  {\"id\":1, \"name\":\"marck\", \"last_name\":\"vaisman\"},\n  {\"id\":2, \"name\":\"anderson\", \"last_name\":\"monken\"},\n  {\"id\":3, \"name\":\"amit\", \"last_name\":\"arora\"},\n  {\"id\":4, \"name\":\"abhijit\", \"last_name\":\"dasgupta\"},\n]"
  },
  {
    "objectID": "slides/04-slides.html#binary-files",
    "href": "slides/04-slides.html#binary-files",
    "title": "Lecture 5",
    "section": "Binary files",
    "text": "Binary files"
  },
  {
    "objectID": "slides/04-slides.html#issues-with-common-file-formats-particularly-csvs",
    "href": "slides/04-slides.html#issues-with-common-file-formats-particularly-csvs",
    "title": "Lecture 5",
    "section": "Issues with common file formats, particularly CSVs:",
    "text": "Issues with common file formats, particularly CSVs:\n\nStill ubiquitous and highly error prone (even in 2023)\nThe default delimiter is also one of the most familiar characters in the English language—the comma\nNot a uniform format\n\nDelimiter (comma, tab, semi-colon, custom)\nQuote characters (single or doble quote)\nEscaping to appropriately handle string data\n\nDoesn’t natively encode schema information\nNo direct support for nested structures\nEncoding and schema information must be configured in the target system to ensure appropriate ingestion\nAutodetection is a convenience feature provided in many cloud environments but is inappropriate for production ingestion, and can be painfully slow\nData engineers are often forced to work with CSV data and then build robust exception handling and error detection to ensure data quality on ingestion"
  },
  {
    "objectID": "slides/04-slides.html#apache-parquet",
    "href": "slides/04-slides.html#apache-parquet",
    "title": "Lecture 5",
    "section": "Apache Parquet",
    "text": "Apache Parquet\n\nFree and open-source column-oriented data storage format\nCreated by Twitter and Cloudera\nv1.0 released in July, 2013\nStores data in a columnar format (as opposed to row format) and is designed to realize excellent read and write performance\nParquet-encoded data builds in schema information and natively supports nested data\nParquet is portable\nHas become the standard for modern data warehouses and big data tools\nSupported by R and Python through Apache Arrow (more on that coming up!)"
  },
  {
    "objectID": "slides/04-slides.html#traditional-row-store",
    "href": "slides/04-slides.html#traditional-row-store",
    "title": "Lecture 5",
    "section": "Traditional row-store",
    "text": "Traditional row-store\n\n\n\n\n\n\n\n\n\n\n\n\n\nSay you wanted to answer the question “How many balls did we sell?, the engine must scan each and every row until the end!\n\n\nParquet file format, everything you need to know"
  },
  {
    "objectID": "slides/04-slides.html#column-store",
    "href": "slides/04-slides.html#column-store",
    "title": "Lecture 5",
    "section": "Column-store",
    "text": "Column-store\n\n\n\n\nParquet file format, everything you need to know\n\nParquet file format, everything you need to know"
  },
  {
    "objectID": "slides/04-slides.html#row-groups",
    "href": "slides/04-slides.html#row-groups",
    "title": "Lecture 5",
    "section": "Row groups",
    "text": "Row groups\n\n\n\n\n\n\nData is stored in row groups!\n\n\n\nOnly the required fields\n\n\n\n\n\n\nParquet file format, everything you need to know"
  },
  {
    "objectID": "slides/04-slides.html#metadata-compression-and-dictionary-encoding",
    "href": "slides/04-slides.html#metadata-compression-and-dictionary-encoding",
    "title": "Lecture 5",
    "section": "Metadata, compression, and dictionary encoding",
    "text": "Metadata, compression, and dictionary encoding\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParquet file format, everything you need to know"
  },
  {
    "objectID": "slides/04-slides.html#apache-arrow-for-in-memory",
    "href": "slides/04-slides.html#apache-arrow-for-in-memory",
    "title": "Lecture 5",
    "section": "Apache Arrow for in-memory",
    "text": "Apache Arrow for in-memory\n\nApache Arrow is a development platform for in-memory analytics. It contains a set of technologies that enable big data systems to process and move data fast. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware.\n\n\n\nTopol, M., McKinney, W. (2022). In-Memory Analytics with Apache Arrow: Perform Fast and Efficient Data Analytics on Both Flat and Hierarchical Structured Data. United Kingdom: Packt Publishing."
  },
  {
    "objectID": "slides/04-slides.html#before-arrow",
    "href": "slides/04-slides.html#before-arrow",
    "title": "Lecture 5",
    "section": "Before Arrow",
    "text": "Before Arrow\n\n\n\nTopol, M., McKinney, W. (2022). In-Memory Analytics with Apache Arrow: Perform Fast and Efficient Data Analytics on Both Flat and Hierarchical Structured Data. United Kingdom: Packt Publishing."
  },
  {
    "objectID": "slides/04-slides.html#after-arrow",
    "href": "slides/04-slides.html#after-arrow",
    "title": "Lecture 5",
    "section": "After Arrow",
    "text": "After Arrow\n\n\n\nTopol, M., McKinney, W. (2022). In-Memory Analytics with Apache Arrow: Perform Fast and Efficient Data Analytics on Both Flat and Hierarchical Structured Data. United Kingdom: Packt Publishing."
  },
  {
    "objectID": "slides/04-slides.html#arrow-compatibility",
    "href": "slides/04-slides.html#arrow-compatibility",
    "title": "Lecture 5",
    "section": "Arrow Compatibility",
    "text": "Arrow Compatibility\n\n\n\nTopol, M., McKinney, W. (2022). In-Memory Analytics with Apache Arrow: Perform Fast and Efficient Data Analytics on Both Flat and Hierarchical Structured Data. United Kingdom: Packt Publishing."
  },
  {
    "objectID": "slides/04-slides.html#arrow-performance",
    "href": "slides/04-slides.html#arrow-performance",
    "title": "Lecture 5",
    "section": "Arrow Performance",
    "text": "Arrow Performance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopol, M., McKinney, W. (2022). In-Memory Analytics with Apache Arrow: Perform Fast and Efficient Data Analytics on Both Flat and Hierarchical Structured Data. United Kingdom: Packt Publishing."
  },
  {
    "objectID": "slides/04-slides.html#use-arrow-to-readwrite-csvs-and-parquet",
    "href": "slides/04-slides.html#use-arrow-to-readwrite-csvs-and-parquet",
    "title": "Lecture 5",
    "section": "Use Arrow to read/write CSVs and Parquet",
    "text": "Use Arrow to read/write CSVs and Parquet\n\n\n\n\n\n\nPython\nUse the pyarrow library or straight from pandas\nimport pandas as pd\npd.read_csv(engine = 'pyarrow')\npd.read_parquet\n\nimport pyarrow.csv\npyarrow.csv.read_csv()\n\nimport pyarrow.parquet\npyarrow.parquet.read_table()\n\n\nR\nUse the arrow package\nlibrary(arrow)\n\nread_csv_arrow()\nread_parquet()\nread_json_arrow()\n\nwrite_csv_arrow()\nwrite_parquet()\n\n\n\nRecommendation: save your intermediate and analytical datasets as Parquet!"
  },
  {
    "objectID": "slides/04-slides.html#before-we-begin..",
    "href": "slides/04-slides.html#before-we-begin..",
    "title": "Lecture 5",
    "section": "Before we begin..",
    "text": "Before we begin..\n\nPandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\n\n\n\nPandas is slow, well yes but also, not so much if you use it the right way.\n\n\nApache Arrow and the “10 Things I Hate About pandas” (A 2017 post from the creator of Pandas..)\n50 times faster data loading for Pandas: no problem (but this is an old 2019 article..)\nIs Pandas really that slow?\n\n\nPandas 2.0 and the arrow revolution"
  },
  {
    "objectID": "slides/04-slides.html#polars-1",
    "href": "slides/04-slides.html#polars-1",
    "title": "Lecture 5",
    "section": "Polars",
    "text": "Polars\n\nPolars"
  },
  {
    "objectID": "slides/04-slides.html#why-is-polars-faster-than-pandas",
    "href": "slides/04-slides.html#why-is-polars-faster-than-pandas",
    "title": "Lecture 5",
    "section": "Why is Polars faster than Pandas?",
    "text": "Why is Polars faster than Pandas?\n\n\nPolars is written in Rust. Rust is a compiled language, Python is an interpreted language.\n\nCompiled language: you generate the machine code only once then run it, subsequent runs do not need the compilation step.\nInterpreted language: code has to be parsed, interpreted and converted into machine code every single time.\n\nParallelization: Vectorized operations that can be executed in parallel on multiple CPU cores.\nLazy evaluation: Polars supports two APIs lazy as well as eager evaluation (used by pandas). In lazy evaluation, a query is executed only when required. While in eager evaluation, a query is executed immediately.\nPolars uses Arrow as its in-memory representation of data. Similar to how pandas uses Numpy (although Pandas 2.0 does allow using Arrow as the backend in addition to Numpy).\n\n[Excerpt from this post from Ritchie Vink, author of Polars] Arrow provides the efficient data structures and some compute kernels, like a SUM, a FILTER, a MAX etc. Arrow is not a query engine. Polars is a DataFrame library on top of arrow that has implemented efficient algorithms for JOINS, GROUPBY, PIVOTs, MELTs, QUERY OPTIMIZATION, etc. (the things you expect from a DF lib).\nPolars could be best described as an in-memory DataFrame library with a query optimizer."
  },
  {
    "objectID": "slides/04-slides.html#ease-of-use",
    "href": "slides/04-slides.html#ease-of-use",
    "title": "Lecture 5",
    "section": "Ease of use",
    "text": "Ease of use\n\nFamiliar API for users of Pandas: there are differences in syntax polars != pandas but it is still a Dataframe API making it straightforward to perform common operations such as filtering, aggregating, and joining data. See migrating from Pandas\nReading data\n# must install s3fs -&gt; \"pip install s3fs\"\n\n# Using Polars\nimport polars as pl\npolars_df = pl.read_parquet(\"s3://nyc-tlc/trip data/yellow_tripdata_2023-06.parquet\")\n\n# using Pandas\nimport pandas as pd\npandas_df = pd.read_parquet(\"s3://nyc-tlc/trip data/yellow_tripdata_2023-06.parquet\")\nSelecting columns (see Pushdown optimization)\n# Using Polars\nselected_columns_polars = polars_df[['column1', 'column2']]\n\n# Using Pandas\nselected_columns_pandas = pandas_df[['column1', 'column2']]"
  },
  {
    "objectID": "slides/04-slides.html#ease-of-use-contd.",
    "href": "slides/04-slides.html#ease-of-use-contd.",
    "title": "Lecture 5",
    "section": "Ease of use (contd.)",
    "text": "Ease of use (contd.)\n\nFamiliar API for users of Pandas:\nFiltering data\n# Using Polars\nfiltered_polars = polars_df[polars_df['column1'] &gt; 10]\n\n# Using Pandas\nfiltered_pandas = pandas_df[pandas_df['column1'] &gt; 10]\nEven though you can write Polars code that looks like Pandas, it is better to write idiomatic Polars code that takes advantages of unique features Polars offers.\nMigrating from Apache Spark: Whereas the Spark DataFrame is analogous to a collection of rows, a Polars DataFrame is closer to a collection of columns."
  },
  {
    "objectID": "slides/04-slides.html#a-polars-dataframe-processing-pipeline-example",
    "href": "slides/04-slides.html#a-polars-dataframe-processing-pipeline-example",
    "title": "Lecture 5",
    "section": "A Polars DataFrame processing pipeline example",
    "text": "A Polars DataFrame processing pipeline example\nHere is an example that we will run as part of the lab in a little bit.\n\nThink how you would code this same pipeline in Pandas…\n\n\nPolars pipeline"
  },
  {
    "objectID": "slides/04-slides.html#further-reading",
    "href": "slides/04-slides.html#further-reading",
    "title": "Lecture 5",
    "section": "Further reading",
    "text": "Further reading\n\nPolars\nUser guide &lt;— MUST READ\nPolars GitHub repo\nPandas Vs Polars: a syntax and speed comparison\nTips & tricks for working with strings in Polars"
  },
  {
    "objectID": "slides/04-slides.html#duckdb-1",
    "href": "slides/04-slides.html#duckdb-1",
    "title": "Lecture 5",
    "section": "DuckDB",
    "text": "DuckDB\n\nDuckDB is an in-process SQL OLAP database management system.\n\n\nIt is like sqllite, but for analytics. What does this mean? It means that your database runs inside your process, there are no servers to manage, no remote system to connect to. Very snappy, easy to experiment with SQL like syntax.\n\n\nDuckDB does vectorized processing i.e. loads chunks of data into memory (tries to keep everything in the CPU’s L1 and L2 cache) and is thus able to handle datasets bigger than the amount of RAM available.\nDuckDB supports Python, R and a host of other languages.\n\n\n\nPaper on DuckDB by Hannes Mühleisen & Mark Raasveldt\nDuckDB: an Embeddable Analytical Database"
  },
  {
    "objectID": "slides/04-slides.html#duckdb---quick-introduction",
    "href": "slides/04-slides.html#duckdb---quick-introduction",
    "title": "Lecture 5",
    "section": "DuckDB - quick introduction",
    "text": "DuckDB - quick introduction\nDuckDB is an in-process SQL OLAP database management system\npip install duckdb\n\nDuck DB"
  },
  {
    "objectID": "slides/04-slides.html#key-features",
    "href": "slides/04-slides.html#key-features",
    "title": "Lecture 5",
    "section": "Key Features",
    "text": "Key Features\n\nColumnar Storage & Vectorized Query processing: DuckDB contains a columnar-vectorized query execution engine, where queries are still interpreted, but a large batch of values (a “vector”) are processed in one operation.\n\nMost analytical queries (think group by and summarize) or even data retrieval for training ML models require retrieving a subset of columns and now the entire row, columnar storage make this faster.\n\nIn-Memory Processing: all data needed for processing is brought within the process memory (recall that columnar storage format helps with this) making the queries run faster (no call to a database over the network).\nSQL Support: highly Postgres-compatible version of SQL1.\nACID Compliance: Transactional guarantees (ACID properties) through bulk-optimized Multi-Version Concurrency Control (MVCC).\n\nFriendlier SQL with DuckDB"
  },
  {
    "objectID": "slides/04-slides.html#use-cases-for-duckdb",
    "href": "slides/04-slides.html#use-cases-for-duckdb",
    "title": "Lecture 5",
    "section": "Use-cases for DuckDB",
    "text": "Use-cases for DuckDB\n\nData Warehousing\nBusiness Intelligence\nReal-time Analytics\nIoT Data Processing"
  },
  {
    "objectID": "slides/04-slides.html#duckdb-in-the-wild",
    "href": "slides/04-slides.html#duckdb-in-the-wild",
    "title": "Lecture 5",
    "section": "DuckDB in the wild",
    "text": "DuckDB in the wild\n\nHow We Silently Switched Mode’s In-Memory Data Engine to DuckDB To Boost Visual Data Exploration Speed\nWhy we built Rill with DuckDB\nLeveraging DuckDB for enhanced performance in dbt projects"
  },
  {
    "objectID": "slides/04-slides.html#how-might-you-think-about-duckdb---diy-version",
    "href": "slides/04-slides.html#how-might-you-think-about-duckdb---diy-version",
    "title": "Lecture 5",
    "section": "How might you think about DuckDB - DIY Version",
    "text": "How might you think about DuckDB - DIY Version\n\nDatalake and DuckDB\nUsing DuckDB in AWS Lambda\nModern Data Stack in a Box with DuckDB: DuckDB, Meltano, Dbt, Apache Superset"
  },
  {
    "objectID": "slides/04-slides.html#how-might-you-think-about-duckdb---fully-managed",
    "href": "slides/04-slides.html#how-might-you-think-about-duckdb---fully-managed",
    "title": "Lecture 5",
    "section": "How might you think about DuckDB - Fully-managed",
    "text": "How might you think about DuckDB - Fully-managed\n\n\n\n\n\nMotherDuck Architecture\n\n\n\n\nArchitecture and capabilities\nSeamlessly analyze data, whether it sits on your laptop, in the cloud or split between. Hybrid execution automatically plans each part of your query and determines where it’s best computed. If you use DuckDB with data lakes in s3, it’ll be much faster to run your analyses on MotherDuck.\n\nDuckDB Vs MotherDuck"
  },
  {
    "objectID": "slides/04-slides.html#setting-up-duckdb",
    "href": "slides/04-slides.html#setting-up-duckdb",
    "title": "Lecture 5",
    "section": "Setting Up DuckDB",
    "text": "Setting Up DuckDB\n\nConfiguration and Initialization: DuckDB is deeply integrated into Python and R for efficient interactive data analysis. DuckDB provides APIs for Java, C, C++, Julia, Swift, and others.\npip install duckdb\n# OR\nconda install python-duckdb -c conda-forge\nConnecting to DuckDB\nimport duckdb\n# directly query a Pandas DataFrame\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/anly503/datasets/main/EconomistData.csv\")\nduckdb.sql('SELECT * FROM df')"
  },
  {
    "objectID": "slides/04-slides.html#setting-up-duckdb-contd.",
    "href": "slides/04-slides.html#setting-up-duckdb-contd.",
    "title": "Lecture 5",
    "section": "Setting Up DuckDB (contd.)",
    "text": "Setting Up DuckDB (contd.)\n\nSupported data formats: DuckDB can ingest data from a wide variety of formats – both on-disk and in-memory. See the data ingestion page for more information.\nimport duckdb\nduckdb.read_csv('example.csv')                # read a CSV file into a Relation\nduckdb.read_parquet('example.parquet')        # read a Parquet file into a Relation\nduckdb.read_json('example.json')              # read a JSON file into a Relation\n\nduckdb.sql('SELECT * FROM \"example.csv\"')     # directly query a CSV file\nduckdb.sql('SELECT * FROM \"example.parquet\"') # directly query a Parquet file\nduckdb.sql('SELECT * FROM \"example.json\"')    # directly query a JSON file"
  },
  {
    "objectID": "slides/04-slides.html#querying-duckdb",
    "href": "slides/04-slides.html#querying-duckdb",
    "title": "Lecture 5",
    "section": "Querying DuckDB",
    "text": "Querying DuckDB\nEssential reading: Friendlier SQL with DuckDB, SQL Introduction\nBasic SQL Queries:\nimport duckdb\nimport pandas as pd\nbabynames = pd.read_parquet(\"https://github.com/anly503/datasets/raw/main/babynames.parquet.zstd\")\nduckdb.sql(\"select count(*)  from babynames where Name='John'\")\nAggregations and Grouping\nduckdb.sql(\"select State, Name, count(*) as count  from babynames group by State, Name order by State desc, count desc\")"
  },
  {
    "objectID": "slides/04-slides.html#querying-duckdb-contd.",
    "href": "slides/04-slides.html#querying-duckdb-contd.",
    "title": "Lecture 5",
    "section": "Querying DuckDB (contd.)",
    "text": "Querying DuckDB (contd.)\nEssential reading: From & Join clauses\nJoins and Subqueries\n# -- join two tables together\nduckdb.sql(\"SELECT * FROM table_name JOIN other_table ON (table_name.key = other_table.key\")\nWindow Functions\npowerplants = pd.read_csv(\"https://raw.githubusercontent.com/anly503/datasets/main/powerplants.csv\", parse_dates=[\"date\"])\nq = \"\"\"\nSELECT \"plant\", \"date\",\n    AVG(\"MWh\") OVER (\n        PARTITION BY \"plant\"\n        ORDER BY \"date\" ASC\n        RANGE BETWEEN INTERVAL 3 DAYS PRECEDING\n                  AND INTERVAL 3 DAYS FOLLOWING)\n        AS \"MWh 7-day Moving Average\"\nFROM powerplants\nORDER BY 1, 2;\n\"\"\"\nduckdb.sql(q)"
  },
  {
    "objectID": "slides/04-slides.html#using-the-duckdb-cli-shell",
    "href": "slides/04-slides.html#using-the-duckdb-cli-shell",
    "title": "Lecture 5",
    "section": "Using the DuckDB CLI & Shell",
    "text": "Using the DuckDB CLI & Shell\n\nInstall the DuckDB CLI (download link) OR use it in your browser via shell.duckdb.org/ for easy data exploration using just SQL.\nOnce installed you can import a local file into the shell and run queries.\n\nYou can download powerplants.csv from here.\n\n\nC:\\Users\\&lt;username&gt;\\Downloads\\duckdb_cli-windows-amd64&gt;duckdb\nv0.8.1 6536a77232\nEnter \".help\" for usage hints.\nConnected to a transient in-memory database.\nUse \".open FILENAME\" to reopen on a persistent database.\nD CREATE TABLE powerplants AS SELECT * FROM read_csv_auto('powerplants.csv');\nD DESCRIBE powerplants;\n┌─────────────┬─────────────┬─────────┬─────────┬─────────┬───────┐\n│ column_name │ column_type │  null   │   key   │ default │ extra │\n│   varchar   │   varchar   │ varchar │ varchar │ varchar │ int32 │\n├─────────────┼─────────────┼─────────┼─────────┼─────────┼───────┤\n│ plant       │ VARCHAR     │ YES     │         │         │       │\n│ date        │ DATE        │ YES     │         │         │       │\n│ MWh         │ BIGINT      │ YES     │         │         │       │\n└─────────────┴─────────────┴─────────┴─────────┴─────────┴───────┘\nD  SELECT * from powerplants where plant='Boston' and date='2019-01-02';\n┌─────────┬────────────┬────────┐\n│  plant  │    date    │  MWh   │\n│ varchar │    date    │ int64  │\n├─────────┼────────────┼────────┤\n│ Boston  │ 2019-01-02 │ 564337 │\n└─────────┴────────────┴────────┘\nD"
  },
  {
    "objectID": "slides/04-slides.html#profiling-in-duckdb",
    "href": "slides/04-slides.html#profiling-in-duckdb",
    "title": "Lecture 5",
    "section": "Profiling in DuckDB",
    "text": "Profiling in DuckDB\n\nQuery Optimization: Use the EXPLAIN & ANALYZE keywords to understand how your query is being executed (see Query Plan and the time being spent in individual steps of your query.\nD EXPLAIN ANALYZE SELECT * from powerplants where plant='Boston' and date='2019-01-02';\nDuckDB will use all the cores available on the underlying compute, but you can adjust it (scenario: your process is not the only application on that VM, you want to limit the amount of resources it gets). Full configuration available here.\nD select current_setting('threads');\n┌────────────────────────────┐\n│ current_setting('threads') │\n│           int64            │\n├────────────────────────────┤\n│                          8 │\n└────────────────────────────┘\nD SET threads=4;\nD select current_setting('threads');\n┌────────────────────────────┐\n│ current_setting('threads') │\n│           int64            │\n├────────────────────────────┤\n│                          4 │\n└────────────────────────────┘\nD"
  },
  {
    "objectID": "slides/04-slides.html#a-simple-example-of-using-duckdb-and-apache-arrow-using-nyc-taxi-dataset",
    "href": "slides/04-slides.html#a-simple-example-of-using-duckdb-and-apache-arrow-using-nyc-taxi-dataset",
    "title": "Lecture 5",
    "section": "A simple example of using DuckDB and Apache Arrow using NYC Taxi dataset",
    "text": "A simple example of using DuckDB and Apache Arrow using NYC Taxi dataset\nThis notebook reads the NYC taxi dataset files for the year 2021 (about ~29 million rows) and runs some analytics operation on this dataset. This dataset is too big to fit into memory.\n\nWe read the data from S3 using apache Arrow (pyarrow).\nThe zero-copy integration between DuckDB and Apache Arrow allows for rapid analysis of larger than memory datasets in Python and R using either SQL or relational APIs.\nWe create a DuckDB instance in memory and using the connection to this in-memory database We run some simple analytics operations using SQL syntax.\n\nAlso see https://duckdb.org/2021/12/03/duck-arrow.html"
  },
  {
    "objectID": "slides/04-slides.html#further-reading-1",
    "href": "slides/04-slides.html#further-reading-1",
    "title": "Lecture 5",
    "section": "Further reading",
    "text": "Further reading\n\nParallel Grouped Aggregation in DuckDB\nMeta queries\nProfiling queries in DuckDB\nDuckDB tutorial for beginners\nDuckDB CLI API\nUsing DuckDB in AWS Lambda\nRevisiting the Poor Man’s Data Lake with MotherDuck\nSupercharge your data processing with DuckDB\nFriendlier SQL with DuckDB\nBuilding and deploying data apps with DuckDB and Streamlit"
  },
  {
    "objectID": "slides/04-slides.html#github-classroom",
    "href": "slides/04-slides.html#github-classroom",
    "title": "Lecture 5",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link"
  },
  {
    "objectID": "slides/02-slides.html#look-back",
    "href": "slides/02-slides.html#look-back",
    "title": "Lecture 2",
    "section": "Look back",
    "text": "Look back\n\nGreat use of Slack\nBig data definition\nUsed the shell in Linux on a virtual machine through Codespaces"
  },
  {
    "objectID": "slides/02-slides.html#agenda-and-goals-for-today",
    "href": "slides/02-slides.html#agenda-and-goals-for-today",
    "title": "Lecture 2",
    "section": "Agenda and Goals for Today",
    "text": "Agenda and Goals for Today\n\nQuick tour of the cloud services that are used in the course\nExtended Lab:\n\nSetting up AWS accounts\nStarting VMs in the cloud and connecting to them"
  },
  {
    "objectID": "slides/02-slides.html#glossary",
    "href": "slides/02-slides.html#glossary",
    "title": "Lecture 2",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nLocal\nYour current workstation (laptop, desktop, etc.), wherever you start the terminal/console application.\n\n\nRemote\nAny machine you connect to via ssh or other means."
  },
  {
    "objectID": "slides/02-slides.html#working-on-a-single-machine",
    "href": "slides/02-slides.html#working-on-a-single-machine",
    "title": "Lecture 2",
    "section": "Working on a single machine",
    "text": "Working on a single machine\nYou are most likely using traditional data analysis tools, which are single threaded and run on a single machine."
  },
  {
    "objectID": "slides/02-slides.html#the-big-data-problem",
    "href": "slides/02-slides.html#the-big-data-problem",
    "title": "Lecture 2",
    "section": "The BIG DATA problem",
    "text": "The BIG DATA problem"
  },
  {
    "objectID": "slides/02-slides.html#is-moores-law-dead",
    "href": "slides/02-slides.html#is-moores-law-dead",
    "title": "Lecture 2",
    "section": "Is Moore’s Law Dead?",
    "text": "Is Moore’s Law Dead?"
  },
  {
    "objectID": "slides/02-slides.html#new-hardware",
    "href": "slides/02-slides.html#new-hardware",
    "title": "Lecture 2",
    "section": "New Hardware",
    "text": "New Hardware\nNeed\n\nThe demand for data processing will not be met by relying on the same technology.\nThe key to modern data processing is new semiconductors\n\nNot just squeezing more transistors per area\nNeed new compute architectures that are built and optimized for specialized functions\n\nSpecialized edge hardware for Edge Computing\nWhile many declare Moore’s Law to be broken or no longer valid, in reality it’s not the law that is broken but rather a heat problem.\n\nWhat\n\nGraphic Processing Units (GPUs)\nField Programmable Gate Arrays (FPGAs)\nData Processing Units (DPUs)\nPhotonic computing"
  },
  {
    "objectID": "slides/02-slides.html#so-we-cant-store-or-process-data-on-a-single-machine-what-do-we-do",
    "href": "slides/02-slides.html#so-we-cant-store-or-process-data-on-a-single-machine-what-do-we-do",
    "title": "Lecture 2",
    "section": "So, we can’t store or process data on a single machine, what do we do?",
    "text": "So, we can’t store or process data on a single machine, what do we do?"
  },
  {
    "objectID": "slides/02-slides.html#we-distribute",
    "href": "slides/02-slides.html#we-distribute",
    "title": "Lecture 2",
    "section": "We distribute",
    "text": "We distribute\nMore CPUs, more memory, more storage!"
  },
  {
    "objectID": "slides/02-slides.html#simple-we-use-the-cloud",
    "href": "slides/02-slides.html#simple-we-use-the-cloud",
    "title": "Lecture 2",
    "section": "Simple, we use the cloud",
    "text": "Simple, we use the cloud"
  },
  {
    "objectID": "slides/02-slides.html#cloud-computing-is-a-big-deal",
    "href": "slides/02-slides.html#cloud-computing-is-a-big-deal",
    "title": "Lecture 2",
    "section": "Cloud computing is a big deal!",
    "text": "Cloud computing is a big deal!\nBenefits\n\nProvides access to low-cost computing\nCosts are decreasing every year\nElastic\nPAAS works!\nMany other benefits…"
  },
  {
    "objectID": "slides/02-slides.html#what-is-the-claaaaaaawd-the-cloud",
    "href": "slides/02-slides.html#what-is-the-claaaaaaawd-the-cloud",
    "title": "Lecture 2",
    "section": "What is the claaaaaaawd (the cloud)",
    "text": "What is the claaaaaaawd (the cloud)"
  },
  {
    "objectID": "slides/02-slides.html#what-is-the-cloud",
    "href": "slides/02-slides.html#what-is-the-cloud",
    "title": "Lecture 2",
    "section": "What is the cloud?",
    "text": "What is the cloud?\n\\kloud\\ noun\nthe practice of storing regularly used computer data on multiple servers that can be accessed through the Internet\nUsing someone else’s computer(s)"
  },
  {
    "objectID": "slides/02-slides.html#nist-definition",
    "href": "slides/02-slides.html#nist-definition",
    "title": "Lecture 2",
    "section": "NIST Definition",
    "text": "NIST Definition"
  },
  {
    "objectID": "slides/02-slides.html#service-models",
    "href": "slides/02-slides.html#service-models",
    "title": "Lecture 2",
    "section": "Service Models",
    "text": "Service Models"
  },
  {
    "objectID": "slides/02-slides.html#the-evolution-of-the-cloud",
    "href": "slides/02-slides.html#the-evolution-of-the-cloud",
    "title": "Lecture 2",
    "section": "The evolution of the Cloud",
    "text": "The evolution of the Cloud\n\n\n\n\n\n\n\n\nYesterday\nToday\nTomorrow\n\n\n\n\nLimited number of tools and vendors\nMany tools and vendors to work with\nIntegrated tools and vendors\n\n\nOne platform - few devices\nMultiple platforms - many devices\nConnected platforms and devices\n\n\nData is scarce but manageable\nOverabundance of data\nData is used for important business decisions\n\n\nIT has major influence and control\nIT has limited influence and control\nIT is strategic to the business\n\n\nPeople only work when they are at work\nPeople work wherever they want\nPeople have access to what they need, wherever they are"
  },
  {
    "objectID": "slides/02-slides.html#what-does-the-cloud-look-like",
    "href": "slides/02-slides.html#what-does-the-cloud-look-like",
    "title": "Lecture 2",
    "section": "What does the cloud look like?",
    "text": "What does the cloud look like?"
  },
  {
    "objectID": "slides/02-slides.html#virtual-visit-to-a-microsoft-azure-data-center",
    "href": "slides/02-slides.html#virtual-visit-to-a-microsoft-azure-data-center",
    "title": "Lecture 2",
    "section": "Virtual Visit to a Microsoft Azure Data Center",
    "text": "Virtual Visit to a Microsoft Azure Data Center"
  },
  {
    "objectID": "slides/02-slides.html#microsoft-azure-data-center-in-boydton-va",
    "href": "slides/02-slides.html#microsoft-azure-data-center-in-boydton-va",
    "title": "Lecture 2",
    "section": "Microsoft Azure Data Center in Boydton, VA",
    "text": "Microsoft Azure Data Center in Boydton, VA"
  },
  {
    "objectID": "slides/02-slides.html#loudon-county-va-is-called-cloudon",
    "href": "slides/02-slides.html#loudon-county-va-is-called-cloudon",
    "title": "Lecture 2",
    "section": "Loudon County, VA is called “CLoudon”",
    "text": "Loudon County, VA is called “CLoudon”\n\nHow data centers power VA’s Loudon County: https://gcn.com/articles/2018/10/12/loudoun-county-data-centers.aspx\nThe heart of “The Cloud” is in Virginia: https://www.cbsnews.com/news/cloud-computing-loudoun-county-virginia/\nCBS Sunday Morning Visits the Home of the Internet in Loudoun County: https://biz.loudoun.gov/2017/10/30/cbs-sunday-morning-visits-loudoun/"
  },
  {
    "objectID": "slides/02-slides.html#of-the-worlds-internet-traffic-passes-through-loudon-county-va",
    "href": "slides/02-slides.html#of-the-worlds-internet-traffic-passes-through-loudon-county-va",
    "title": "Lecture 2",
    "section": "70% of the world’s internet traffic passes through Loudon County, VA",
    "text": "70% of the world’s internet traffic passes through Loudon County, VA"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Section\nInstructor\nDay\nLocation\n\n\n\n\n01\nAmit Arora\nW 9:30-12:00\nReiss 262\n\n\n02\nJeff Jacobs\nTu 9:30-12:00\nTBD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWednesday (Amit Arora)\nSession\nTuesday (Jeff Jacobs)\nSession\nNotes\n\n\n\n\n8/27/2025\n1\n9/2/2025\n1\nCourse overview - All sections combined\n\n\n9/3/2025\n2\n9/9/2025\n2\nCloud computing introduction\n\n\n9/10/2025\n3\n9/16/2025\n3\nParallelization concepts\n\n\n9/17/2025\n4\n9/23/2025\n4\nDuckDB, Polars, file formats\n\n\n9/24/2025\n5\n9/30/2025\n5\nData Warehouse (Athena, Presto, Snowflake)\n\n\n10/1/2025\n6\n10/7/2025\n6\nIntroduction to Spark, RDDs\n\n\n10/8/2025\n7\n10/14/2025\n7\nSpark DataFrames and Spark SQL\n\n\n10/15/2025\n8\n10/21/2025\n8\nSpark ML and Streaming\n\n\n10/22/2025\n9\n10/28/2025\n9\nApache Iceberg & Table Formats\n\n\n10/29/2025\n10\n11/4/2025\n10\nData Pipeline Orchestration (Airflow)\n\n\n11/5/2025\n11\n11/11/2025\n11\nVector Databases & RAG\n\n\n11/12/2025\n12\n11/18/2025\n12\nModern Data Stack & Governance\n\n\n11/19/2025\n13\n11/25/2025\n13\nServerless & Container Orchestration\n\n\n11/26/2025\nNo class - Thanksgiving\n12/2/2025\n14\nNo class Wednesday - Thanksgiving break\n\n\n12/3/2025\n14\n12/9/2025\nProjects\nFinal project presentations"
  },
  {
    "objectID": "project/project-on-azure.html",
    "href": "project/project-on-azure.html",
    "title": "Using Azure Machine Learning for your projects",
    "section": "",
    "text": "Tuesday Jul 22, 2025 at 11:11 am\nThe group AzureML environments are set up. Please read the important information contained in this note carefully."
  },
  {
    "objectID": "project/project-on-azure.html#some-considerations",
    "href": "project/project-on-azure.html#some-considerations",
    "title": "Using Azure Machine Learning for your projects",
    "section": "Some considerations",
    "text": "Some considerations\n\nUse interactive Spark sessions for initial development with a small subset of data. You should only use a 1-driver, 2-executor of 4-core machines (12 cores total). Once you are ready to scale out, you should use jobs. DO NOT SCALE OUT INTERACTIVELY. We will provide an example on how to run a Spark job within AzureML; it’s very similar to what you’ve done on AWS.\nThe cost of the Spark cluster (interactive or job) is $0.143 per core-hour (including driver). A 1-driver-node/2-worker-node cluster using 4-core machines will cost $0.143 x 12 = $1.716 per hour (prorated to the minute).\nThere are other costs in addition to Spark: each compute instance (~0.29/hr for the size mentioned above) plus approximately $0.65-$0.75/day in the platform services associated with AzureML (load balancer, storage, etc.). The platform fees are fixed, you cannot control that.\nYou have a limit of 100 running cores maximum for Spark (this does not include the compute instance, those are separate). At this limit, the maximum sized single cluster you can run is 1-driver/24-worker (4-core), but that means that this cluster costs $14.30 per hour! You should not need that large of a cluster. If you do, then something is not right.\nYou must cache appropriately to avoid multiple reads.\nOnce you get your data to a manageable size, do not use a spark cluster for doing Python only activities (i.e. visualization, duckdb, etc.). You can use your compute instance and we will show how to read files from the workspace blob store into Python."
  },
  {
    "objectID": "project/project-on-azure.html#log-into-your-teams-azure-machine-learning-azureml-workspace",
    "href": "project/project-on-azure.html#log-into-your-teams-azure-machine-learning-azureml-workspace",
    "title": "Using Azure Machine Learning for your projects",
    "section": "Log into your team’s Azure Machine Learning (AzureML) Workspace",
    "text": "Log into your team’s Azure Machine Learning (AzureML) Workspace\nNavigate to https://ml.azure.com and login with your GU credentials Click workspaces You should see your team’s workspace named project-group-##"
  },
  {
    "objectID": "project/project-on-azure.html#your-compute-instance",
    "href": "project/project-on-azure.html#your-compute-instance",
    "title": "Using Azure Machine Learning for your projects",
    "section": "Your compute instance",
    "text": "Your compute instance\nCompute instances were created for each team member. These compute instances are individual, and only the named person can use them. You need to use the compute instances for any github operation.\n\nDo this the first time you use a new compute instance\nThe first time you start your Compute Insance, you should run the following commands from the terminal of that machine. Change the values in &lt;&gt; to your own. The netid is without @georgetown.edu. You only need to run these commands once (but if you add another compute instance, you’ll need to do it again.)\naz upgrade\naz extension remove -n azure-cli-ml\naz extension remove -n ml\naz extension add -n ml -y\ngit config --global --add safe.directory \"/home/azureuser/cloudfiles/code/Users/&lt;NETID&gt;/*\"\ngit config --global user.email \"&lt;YOUR-EMAIL&gt;\"\ngit config --global user.name \"&lt;YOUR-NAME&gt;\""
  },
  {
    "objectID": "project/project-on-azure.html#data-location",
    "href": "project/project-on-azure.html#data-location",
    "title": "Using Azure Machine Learning for your projects",
    "section": "Data location",
    "text": "Data location\nThe Azure Blob location that has the project data is: wasbs://reddit-project@dsan6000fall2024.blob.core.windows.net/&lt;DIRECTORY&gt;/.\n\nWe’ve added the Reddit data for this year’s project which spans the June-2023 to July-2024 in the 202306-202407 directory\nWe’ve also added data from prior years that spans Jan-2021 to March-2023 in the 202101-202303 directory\n\nWithin each dataset directory the structure is similar. There is a comments and submissions subdirectory, and then the data is partitioned by year and month.\n\nThe 202306-202407 dataset uses the yyyy=####/mm=## partitioning schema\nThe 202101-202303 dataset uses the year=####/month=## partitioning schema\n\nNote: There are differences in the both datasets with the partitioning schema names, the actual individual parquet file names, and the field names. If you do end up using both sets and want to stack them together (noting there is a three month gap), you’ll have to process them individually and generate a common schema. d2c-450d-93b7-96eeb3699b22"
  },
  {
    "objectID": "project/project-on-azure.html#job-example",
    "href": "project/project-on-azure.html#job-example",
    "title": "Using Azure Machine Learning for your projects",
    "section": "Job example",
    "text": "Job example\nClick here to download a zip file with an example on how to run an unattended Spark job in Azure Machine Learning."
  },
  {
    "objectID": "project/06-project.html",
    "href": "project/06-project.html",
    "title": "Website",
    "section": "",
    "text": "Tuesday Jul 22, 2025 at 11:11 am\nThe output of the project will be delivered through a self-contained website in the docs/ subdirectory, having index.html as the starting point. You will build the website incrementally over the milestones.\nThe code to build the website will be in the website-source/ directory.",
    "crumbs": [
      "Project contents",
      "6: Website"
    ]
  },
  {
    "objectID": "project/06-project.html#structure",
    "href": "project/06-project.html#structure",
    "title": "Website",
    "section": "Structure",
    "text": "Structure\nThe website must have the following structure:\n\nLanding and project description page - docs/index.html - You are populating this site during the final submission\nExploratory data analysis page - docs/eda.html - You are populating this site during milestone 1\nNLP page - docs/nlp.html - You are populating this site during milestone 2\nML page - docs/ml.html - You are populating this site during milestone 3\nConclusion page - docs/conclusion.html - You are populating this site during the final submission",
    "crumbs": [
      "Project contents",
      "6: Website"
    ]
  },
  {
    "objectID": "project/06-project.html#requirements",
    "href": "project/06-project.html#requirements",
    "title": "Website",
    "section": "Requirements",
    "text": "Requirements\n\nYour website must be accessible via an index.html file in the docs/folder. You can build the website using the tool(s) of your preference. Choose however you would like to build your website (using quarto, Rmarkdown, editing HTML directly, any other framework, etc.)\nYou can use CSS frameworks, such as Bootstrap, Materialize, or Distill and include external libraries (jQuery, leaflet.js, moment.js, etc.). Layers such as NVD3, Vega-lite, Highcharts, etc. are allowed. Many of these have wrapper packages in R and Python. For example, the package altair (in both R and Python) wraps Vega-lite, and the package plotly (in both R and Python), among other packages, wraps D3. Other packages you may use are any of the htmlwidgets packages in R, bokeh or holoviz (and it’s accompanying ecosystem) in Python, as well as specialized packages for geospatial (leaflet, tmap in R, folium in Python) and networks (igraph, NetworkX, bokeh, plotly).\nYou do not need to make the site public as we will look at it from the repository. You may make it public (just the website sub-directory) using your preferred method.\nNo custom backends (Node.js, Python, etc) and database systems, such as Postgres or MySQL.\nThe communication needs to be effective with clean aesthetics, but you do not need to get too fancy.\nYou may include the html exports of notebooks\n\n\n\n\n\n\n\nWarning\n\n\n\n\nNote that a raw Jupyter notebook rendered in HTML is not an appropriate webpage for your final submission. The code, computing and guts of your work should not be visible, and the output should be well-formatted and understandable to a non-technical reader. You must link to the appropriate technical files (notebooks, scripts) so that inquisitive minds (and instructors) can go.\nAll data visualizations and tables should be properly labelled, with captions if necessary, and should stand alone without reading through your narrative content. If you are referring to your table or figure in your narrative, it must be of the form Table 1 or Figure 1, and the corresponding table/figure should then have a caption which includes the appropriate table or figure number. See this page if you are using Quarto as your website generator. For other technologies, follow the formats outlined in this page.",
    "crumbs": [
      "Project contents",
      "6: Website"
    ]
  },
  {
    "objectID": "project/04-project.html",
    "href": "project/04-project.html",
    "title": "Milestone 3: Machine Learning",
    "section": "",
    "text": "Monday Feb 26, 2024 at 8:10 am\nYou should read through the entire milestone instructions before beginning your work! Don’t start the cluster until you are ready.\nCode and data saved locally on the cluster will be lost when the cluster terminates. Remember to save intermediate and analytical datasets. Your deliverable artifacts (charts, summary csv files, etc.) must be saved in the data/ folder of this repo.",
    "crumbs": [
      "Project contents",
      "4: ML"
    ]
  },
  {
    "objectID": "project/04-project.html#table-of-contents",
    "href": "project/04-project.html#table-of-contents",
    "title": "Milestone 3: Machine Learning",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nMilestone description\nAnalysis Requirements\nMilestone web page requirements\nSubmission Instructions",
    "crumbs": [
      "Project contents",
      "4: ML"
    ]
  },
  {
    "objectID": "project/04-project.html#setup",
    "href": "project/04-project.html#setup",
    "title": "Milestone 3: Machine Learning",
    "section": "Setup",
    "text": "Setup\n\nCreate a sub-directory named ml in the code/ folder within the repository.\nCreate one or more Notebooks in the ml sub-directory as needed.",
    "crumbs": [
      "Project contents",
      "4: ML"
    ]
  },
  {
    "objectID": "project/04-project.html#milestone-description",
    "href": "project/04-project.html#milestone-description",
    "title": "Milestone 3: Machine Learning",
    "section": "Milestone description",
    "text": "Milestone description\nIn this milestone, you will develop Python and Pyspark notebooks to perform the following tasks:\n\nConduct machine learning on your big data\nEvaluate the performance of your models\nInterpret your results for your analytical questions\nCommunicate your results on your group website\n\nWe expect you to put significant effort into this.\n\n\n\n\n\n\nOutput file sizes\n\n\n\nREMEMBER!!! All the output you are making MUST be small data. Can you make a graph of all 1 million+ rows of spark data? NO! You MUST take that big data and collapse it into reasonable data to put into a table or a graph. You should never collect more than ~10,000 rows.",
    "crumbs": [
      "Project contents",
      "4: ML"
    ]
  },
  {
    "objectID": "project/04-project.html#requirements",
    "href": "project/04-project.html#requirements",
    "title": "Milestone 3: Machine Learning",
    "section": "Requirements",
    "text": "Requirements\n\nPreparing data\n\nPrepare your data for modeling. Conduct any remaining feature transformations and apply additional textual processing steps to have the needed variables in your data.\nConduct at least 2 ML transformations for your data by applying string indexer, vectorizer, or other transformers.\nSplit data into testing and training data if you are running supervised models.\n\n\n\nBuilding machine learning models\n\nUse machine learning to answer at least two different business questions (more is always fine!) from your data. Within each analysis, you will compare the performance of two or more models types by selecting at least two different hyperparameter sets OR at least two different pretrained models. The choice of hyperparameter/model comparison depends on your analysis.\nAt least one of these model analyses must leverage Spark ML models where you conduct the training, testing, and validation yourself. The other analysis can use pre-trained models for your inference tasks.\n\n\n\nEvaluate model performance\n\nRun model evaluation metrics (ROC AUC, Confusion Matrix, Accuracy, F1-score, R2, etc.) on your models and/or hyperparameter options.\nEvaluate your models using at least two different metrics and compare and interpret your results, for each ML analysis.\n\n\n\nSummarize your ML work\n\nPrepare at least 1 appropriate table and at least 1 chart for each ML analysis. You do not need one for every model / hyperparameter set.\n\n\n\nSaving model output and using it for inference\n\nYou must save at least one ML model.\nDemonstrate that you can save your best ML model: load the prepared model into your notebook and start making predictions without any additional training. This can be done in a separate notebook.",
    "crumbs": [
      "Project contents",
      "4: ML"
    ]
  },
  {
    "objectID": "project/04-project.html#pipelines",
    "href": "project/04-project.html#pipelines",
    "title": "Milestone 3: Machine Learning",
    "section": "Pipelines",
    "text": "Pipelines\nApply at least one pipeline to your ML analysis so that you can compare hyperparameters and model options without having to re-run large parts of code.",
    "crumbs": [
      "Project contents",
      "4: ML"
    ]
  },
  {
    "objectID": "project/04-project.html#milestone-web-page-requirements",
    "href": "project/04-project.html#milestone-web-page-requirements",
    "title": "Milestone 3: Machine Learning",
    "section": "Milestone web page requirements",
    "text": "Milestone web page requirements\nIn this milestone, you will only be populating content in the ML page code/ml.html with the following sections:\n\nExecutive summary\n\nWrite 1-2 paragraphs on your ML accomplishments. You can include up to 2 images or tables. Your writing must be excellent and persuasive. 3 sentences total will not cut it. The expectation is to write at least 8 sentences. This is the section to describe the high-level results and the most important information only! This summary must be NON-TECHNICAL! Think about how to touch on your business goals without going into much detail.\n\n\n\nAnalysis report\n\nWrite a data analysis style report of 4-6 text paragraphs (each paragraph at least 4 sentences). This is where you will discuss your ML prep, execution, and evaluation. During that prose, you will present all your tables and figures. You can save the images or take snips from your Jupyter notebook. This part is for you to write up the analysis work you have already done. Be creative, be awesome!! Show off to a future employer your amazing data science skills!\n\nThe flow of the report is up to you. Maybe you want to split the text by business goal or organize the prose into a data journey story.\nYou must reference all the business goals that you have accomplished in this report. Did your technical proposal change as you started working with the data?\nIf you made changes to your business goals, discuss your analytical justification for changing your plans. It is OK that plans change, though it is important to describe why.\nYour visualizations must follow the visualization best practices you learned in the visualization class. There should be titles, axis labels, legends as needed, etc.\n\n\nInclude links to all your ML coding notebooks. Make sure you put your exported notebooks into a public place so your audience can see your notebooks.",
    "crumbs": [
      "Project contents",
      "4: ML"
    ]
  },
  {
    "objectID": "project/04-project.html#submission-instructions",
    "href": "project/04-project.html#submission-instructions",
    "title": "Milestone 3: Machine Learning",
    "section": "Submission instructions",
    "text": "Submission instructions\n\nCommit and push your files\nExport any of the notebooks you created as IPython and/or html files, and add them to your repository from your local machine before you tag the release\nTag the submission release commit with the v0.3-ml tag by the due date\nMake sure to push to GitHub",
    "crumbs": [
      "Project contents",
      "4: ML"
    ]
  },
  {
    "objectID": "project/02-project.html",
    "href": "project/02-project.html",
    "title": "Milestone 2: Add external data and perform NLP",
    "section": "",
    "text": "Monday Feb 26, 2024 at 8:10 am",
    "crumbs": [
      "Project contents",
      "2: NLP"
    ]
  },
  {
    "objectID": "project/02-project.html#table-of-contents",
    "href": "project/02-project.html#table-of-contents",
    "title": "Milestone 2: Add external data and perform NLP",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nSetup for NLP\nMilestone description\nAnalysis Requirements\nMilestone web page requirements\nSubmission Instructions",
    "crumbs": [
      "Project contents",
      "2: NLP"
    ]
  },
  {
    "objectID": "project/02-project.html#setup",
    "href": "project/02-project.html#setup",
    "title": "Milestone 2: Add external data and perform NLP",
    "section": "Setup",
    "text": "Setup\n\nInstall the SparkNLP package to your Spark environment. Refer to the labs for examples of installing SparkNLP into a Sagemaker or Azure Spark environment.\nCreate a sub-directory named nlp in the code/ folder within the repository.\nCreate one or more Notebooks in the nlp sub-directory as needed.",
    "crumbs": [
      "Project contents",
      "2: NLP"
    ]
  },
  {
    "objectID": "project/02-project.html#milestone-description",
    "href": "project/02-project.html#milestone-description",
    "title": "Milestone 2: Add external data and perform NLP",
    "section": "Milestone description",
    "text": "Milestone description\nIn this milestone, you will develop Python and Pyspark notebooks to perform the following tasks:\n\nExpand your analysis of Reddit data\nConduct NLP processing work\nMerge/overlay your external data, including EDA\nStart to answer some of your analytical questions.\n\nWe expect you to put significant effort into this.\n\n\n\n\n\n\nOutput data sizes\n\n\n\nREMEMBER!!! All the output you are making MUST be small data. Can you make a graph of all 1 million+ rows of spark data? NO! You MUST take that big data and collapse it into reasonable data to put into a table or a graph. You should never collect more than ~10,000 rows.",
    "crumbs": [
      "Project contents",
      "2: NLP"
    ]
  },
  {
    "objectID": "project/02-project.html#requirements",
    "href": "project/02-project.html#requirements",
    "title": "Milestone 2: Add external data and perform NLP",
    "section": "Requirements",
    "text": "Requirements\n\nAdd external data\n\nAcquire, clean, and merge your external data source(s) onto your Reddit data. Produce appropriate charts/tables showing the distribution of your external data alongside your Reddit data. Make sure you include proper citation(s)/attribution(s) for your external data.\n\n\n\nConduct your natural language processing work\n\nConduct basic data text checks/analysis on your data. What are the most common words overall or over time? What is the distribution of text lengths? What are important words according to TF-IDF?\nIdentify important keywords for your Reddit data and use regex searches to create at least two dummy variables to identify comments on particular topics.\nClean your text data using johnsnowlabs sparkNLP. Think about a few standard procedures to use: stop words, stemming, lemmatizing, removing unusual characters, matching synonyms, etc. You must use at least five NLP cleaning procedures.\n\n\n\nBuild a sentiment model\n\nBuild at least one sentiment model using the sparkNLP framework. Pick a pre-trained model or train your own model. You can start with a simple pos/neg/neu sentiment. Maybe you will want to build your own textual classification model…. that is completely up to you and your topical interests and technical goals. You must report a table of summary statistics from any model(s) leveraged.\n\n\n\nVisualize\n\nProduce at least 3 interesting graphs about your resulting dataset. Think about the dimensions that are interesting for your Reddit data. There are millions of choices. Make sure your graphs are connected to your analytical questions.\n\n\n\nSummary tables\n\nProduce at least 3 interesting tables about your resulting dataset. You can decide how to split up your data into categories, time slices, etc. There are infinite ways you can make summary statistics. Be unique, creative, and interesting!\n\n\n\nSave output\n\nSave your output datasets in parquet format. This will allow you to more quickly work on your next assignment focused on ML without having to re-run any of your transformations.",
    "crumbs": [
      "Project contents",
      "2: NLP"
    ]
  },
  {
    "objectID": "project/02-project.html#milestone-web-page-requirements",
    "href": "project/02-project.html#milestone-web-page-requirements",
    "title": "Milestone 2: Add external data and perform NLP",
    "section": "Milestone web page requirements",
    "text": "Milestone web page requirements\nIn this milestone, you will only be populating content in the NLP page docs/nlp.html with the following sections:\n\nExecutive summary\n\nWrite 1-2 paragraphs on your NLP accomplishments. You can include up to 2 images or tables. This is the section to describe the high-level results and the most important information only! This summary must be NON-TECHNICAL! Think about how to touch on your business goals without going into much detail.\n\n\n\nAnalysis report\n\nWrite a data analysis style report of 4-6 text paragraphs (each paragraph at least 4 sentences). This is where you will discuss your NLP work and present all your interesting tables and figures. You can save the images or take snips from your notebook. This part is for you to write up the analysis work you have already done. Be creative, be awesome!! Show off to a future employer your amazing data science skills!\n\nThe flow of the report is up to you. Maybe you want to split the text by business goal or organize the prose into a data journey story.\nYou must reference all the business goals that you have accomplished in this report. Did your technical proposal change as you started working with the data?\nIf you made changes to your business goals, discuss your analytical justification for changing your plans. It is OK that plans change, though it is important to describe why.\nYour visualizations must follow the visualization best practices you learned in the visualization class. There must be titles, axis labels, legends as needed, etc.\n\n\nInclude links to all your NLP coding notebooks and sources for your external data so that your audience can look at your work. Make sure you put your exported notebooks into a public place so your audience can see your notebooks.",
    "crumbs": [
      "Project contents",
      "2: NLP"
    ]
  },
  {
    "objectID": "project/02-project.html#submission-instructions",
    "href": "project/02-project.html#submission-instructions",
    "title": "Milestone 2: Add external data and perform NLP",
    "section": "Submission instructions",
    "text": "Submission instructions\n\nCommit and push your files\nExport any of the notebooks you created as IPython and/or html files, and add them to your repository from your local machine before you tag the release\nTag the submission release commit with the v0.2-nlp tag by the due date\nMake sure to push to GitHub",
    "crumbs": [
      "Project contents",
      "2: NLP"
    ]
  },
  {
    "objectID": "labs/week-tbd-labs.html#part-1--lambda",
    "href": "labs/week-tbd-labs.html#part-1--lambda",
    "title": "Docker",
    "section": "Part-1- Lambda",
    "text": "Part-1- Lambda"
  },
  {
    "objectID": "labs/week-tbd-labs.html#hello-world-lambda",
    "href": "labs/week-tbd-labs.html#hello-world-lambda",
    "title": "Docker",
    "section": "“Hello World” Lambda",
    "text": "“Hello World” Lambda\nLaunch AWS Academy and get to the AWS Console. Find the Lambda service from the search bar. \nThe dashboard shows the Lambda functions that have been made, some metrics on Lambda usage. Click on the orange Create Function button.\n\nHere you have to fill out the details for your Lambda function. There are several parts to set up.\n\nYou will leave the default option Author from scratch so that you can code directly from the Lambda service.\nSet your Function name as test-lambda.\nChoose your Runtime as Python 3.11\nClick on the Change default execution role dropdown, then select Use an existing role option, and finally pick the existing role LabRole.\n\n\n\nUnder Advanced Settings check the Enable Function URL checkbox and select None for Auth type. This will create an HTTPS endpoint that you can access from your web browser or cURL command without authentication (in a real world scenario the authentication piece is usually handled by an API Gateway).\n\n\n\nNow click on the orange Create function button.\n\nYou now have your environment for Lambda! In the upper function overview tab, you can select a variety of triggers and destinations for the Lambda. We will leave these alone for now. You can explore both on your own time to see the options.\n\nLet’s start with the basic test of the “Hello World” code that was provided in the Python code. Click on the blue Test button.\n\nThis will launch a popup to configure your event. You can submit a JSON payload to the test that will mimic input data that the Lambda function can process. Start off by setting Event name to mytest. Then you can leave the Event JSON for now, but you will come back to it for future iterations of experimentation. Click on the orange save button.\n\nClick on the blue Test button again. If you click on the arrow then you can choose to change or make a new test environment like you did on the previous step.\n\nYour test will execute, and the results will be shown. Several pieces of info are important:\n\nName of the test that was conducted\nResponse object that the function returned\nFunction logs that include the duration of the function, billed direction, memory max (for pricing), and actual memory used\nStatus in the upper right\n\n\nYou are now ready to invoke your newly deployed Lambda function through your web browser or through cURL. Copy the function URL and paste it in your browser’s address bar.\n\nNow try invoking the Lambda from a cURL command (this requires cURL to be available on your machine).\n\nOPTIONAL - If you wanted to set your Lambda to run on a regular schedule, like a crontab, you would add a trigger with the Add trigger button in the Function Overview and select EventBridge (CloudWatch Events). The Trigger add would look like this for setting a job to run every day at 10:15am UTC."
  },
  {
    "objectID": "labs/week-tbd-labs.html#exploring-the-lambda-file-system---lab-submission-component",
    "href": "labs/week-tbd-labs.html#exploring-the-lambda-file-system---lab-submission-component",
    "title": "Docker",
    "section": "Exploring the Lambda File System - LAB SUBMISSION COMPONENT",
    "text": "Exploring the Lambda File System - LAB SUBMISSION COMPONENT\nIn this section, you will a make use of AWS Lambda to see how serverless infrastructure works. For this assignment, you will need to submit a set of JSON files along with other required files.\nIn AWS Lambda, for an output to be generated, you will may use json.dumps to dump a dictionary to the body value in the return statement. This is one of many methods you can use! We encourage you to experiment with few different methods and choose what works the best given your case. A screenshot is attached below for reference. Also note that indent=2 argument for json.dumps, it comes in handy for producting a pretty printed output.\nA dictionary is basically defined as a key:value pair, this is also the building block of json format.\nEach time you make a change to the code, you will have to click on the Deploy button and then the blue Test button.\n\nNote the use of the dict constructor for creating the dictionary. This is an alternate, more readable (arguably slower though) way of creating a Python dictionary. Also note that indent=2 argument for json.dumps, it comes in handy for producting a pretty printed output.\n\n\n\nUse the pathlib library and its iterdir() method in Python to view the contents of the root directory (the / is referred to as the root directory). Make a new key in the Lambda called root in the return JSON and send the contents of the root directory. This might take a few tries! How do you deal with objects that need to become strings?\n\n\nHint!: Think how you can pass objects in a string? For this you can use something like this:\n\n\nf'string here {object}'\n\nReturn the contents of the event input variable to the lambda_handler function as additional item in the return JSON as event.\nReturn the python version using the executable() method in the sys library. The key should be py_version.\nReturn the current username using the subprocess library and the whoami shell command. The key should be username.\nNote that all the new keys you are adding to the response should be nested as part of the body. So essentially, the response contains of two keys: a statusCode which specifies if the call successed or failed (a statusCode value other than 200 indicates a failure) and a body key which has the contents of the response.\n\nAfter these have been executed, copy the function URL and put it in lambda-test-url.json\nThe response from the server should be like this:\n{ \"statusCode\": 200,\n  \"body\": {\n    \"message\": \"Hello from Lambda!\",\n    \"root\": \"...\"\n    \"event\": \"...\",\n    \"py_version\": \"...\",\n    \"username\": \"...\"\n  }\n}"
  },
  {
    "objectID": "labs/week-tbd-labs.html#part-2--cloud9",
    "href": "labs/week-tbd-labs.html#part-2--cloud9",
    "title": "Docker",
    "section": "Part-2- Cloud9",
    "text": "Part-2- Cloud9"
  },
  {
    "objectID": "labs/week-tbd-labs.html#creating-cloud9-environment",
    "href": "labs/week-tbd-labs.html#creating-cloud9-environment",
    "title": "Docker",
    "section": "Creating Cloud9 Environment",
    "text": "Creating Cloud9 Environment\n\nSearch for cloud9 in the search bar of your AWS console as shown in the figure below.\n\n\n\nOnce on the Cloud9 splash screen, click on the orange button Create environment.\n\n\n\nEnter a Name for your environment. Leave the description blank. The figure below shows sample text you could use. Once you enter your name, scroll down to the next section.\n\n\n\nThere are a few options here. You have to make a few changes.\n\nThe Instance type section is to select how large an instance for Cloud9. Select the t3.small instance type\nThe Platform section is for selecting the operating system for your new instance. Leave as the default\nThe Timeout option is set so your instance will hibernate after 30 minutes so you are not charged for the instance 24/7. This is a major problem for cloud services because you can run up a bill quite quickly! Leave as the default\nIn Network Settings, Connection is how to connect to your instance. Select Secure Shell (SSH)\nFinally, click the orange button Create\n\n\n\n\nYou will be sent to the environments page of the Cloud9 service. Your environment is now building. In the table below, for your environment (the row), click on the Open button in Cloud9 IDE column. In the screenshot, we named it cloud9-env.\n\n\n\nThe environment will be configured for you. This takes a few minutes.\n\n\nOnce the environment setup screen goes away then you are ready to use Cloud9. If you get a warning message, just click “OK”."
  },
  {
    "objectID": "labs/week-tbd-labs.html#part-3--docker-lambda",
    "href": "labs/week-tbd-labs.html#part-3--docker-lambda",
    "title": "Docker",
    "section": "Part-3- Docker-Lambda",
    "text": "Part-3- Docker-Lambda"
  },
  {
    "objectID": "labs/week-tbd-labs.html#setting-up-basic-docker-images-in-cloud9",
    "href": "labs/week-tbd-labs.html#setting-up-basic-docker-images-in-cloud9",
    "title": "Docker",
    "section": "Setting up Basic Docker Images in Cloud9",
    "text": "Setting up Basic Docker Images in Cloud9\n\nThis lab will take a lot of time! You will need atleast 5-6 hours on average to work on this lab. Please make sure you start this assignment as soon as possible\n\nDocker image building in Cloud9 is easy since the docker package is already set up. You just have to write some code and run Linux commands!\n\nIn Cloud9, start off by cloning your git repository from either the source control button on the lefthand sidebar or through the terminal.\nIn the root of your repository, create three empty text files in that folder called Dockerfile, app.py, and requirements.txt.\nThe results should look like below and have the symbols change automatically:\n\n\n\nOpen up the Dockerfile and add the following text (note the # lines are comments just like python!)\n\n# syntax=docker/dockerfile:1\n\n# adapted from https://www.philschmid.de/aws-lambda-with-custom-docker-image\n# https://docs.aws.amazon.com/lambda/latest/dg/python-image.html\nFROM python:3.11-slim-buster\n\nCMD [\"python\", \"-c\", \"import platform; print(f\\\"version: {platform.python_version()}\\\")\"]\n\nGo to the terminal and change directories to the location of your Dockerfile. Run the command docker build ./ -t test\n\n\n\nRun the command docker run test to see if your Dockerfile worked!"
  },
  {
    "objectID": "labs/week-tbd-labs.html#lambda-docker-imagelab-demonstration",
    "href": "labs/week-tbd-labs.html#lambda-docker-imagelab-demonstration",
    "title": "Docker",
    "section": "Lambda Docker Image(Lab-Demonstration)",
    "text": "Lambda Docker Image(Lab-Demonstration)\nNote that this Dockerfile is invoking your requirements.txt file to install any packages from pip and the app.py lambda_handler function to run the python code.\n\nNow you might think how does this requirements.txt file work? Each library which needs to be installed will be listed here. Think of Docker as a virtual environment where you can install any package you need and then you would list them in the requirements.txt file. A small example of this is given in the following screenshot below. When the requirements.txt file has been changed, you will have to build and redeploy the docker image.\n\n\n\n\nrequirements.txt\n\n\n\nUse the new Dockerfile contents below for your Dockerfile.\n\nA few examples of how to build a docker file along with some documentation is given below in this link: https://spacelift.io/blog/dockerfile. You can scroll down to see how a docker commands work and what they do. This will be useful in making this a relatively simple task.\n# syntax=docker/dockerfile:1\n\n# adapted from https://www.philschmid.de/aws-lambda-with-custom-docker-image\n# https://docs.aws.amazon.com/lambda/latest/dg/python-image.html\nFROM public.ecr.aws/lambda/python:3.11\n\n##### copy requirements file and install necessary packages\n\n# ***CODE TO DO***\n# ADD the requirements.txt into the ${LAMBDA_TASK_ROOT} directory in the container\n\nRUN pip3 install -r ${LAMBDA_TASK_ROOT}/requirements.txt --target \"${LAMBDA_TASK_ROOT}\"\n\n##### Copy function code to docker container\n\n# ***CODE TO DO***\n# ADD the app.py file into the ${LAMBDA_TASK_ROOT} directory in the container\n\n##### SET THE COMMAND OF THE CONTAINER FOR THE LAMBDA HANDLER\n# app (name of py file)\n# handler (name of function to execute for lambda job)\nCMD [ \"app.lambda_handler\" ]\n\nNote that the ADD and COPY commands in Docker for this instance are similar. The ADD function is more advanced and can auto-extract compressed files into the image. Please use the given python version for this assignment. This assignment was mainly designed to be used with python 3.11. If you a version of python which is lower, we cannot say if it would be compatible.\nSet up your python file app.py with a function called lambda_handler that accepts the event and context arguments. Wait, we have already done this in basic Lambda! Copy your function from the Lambda service. This will ensure that the response is the same through basic Lambda and through the Docker Lambda.\nSince you made changes to the Dockerfile and your app.py files, you need to build a new Docker image. Run the command docker build ./ -t lambda-test so that you name the image something new.\nThis has to be done every time you make changes to the app.py file or the Dockerfile.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe syntax of docker build is as follows:\ndocker build PATH -t 'CONTAINER NAME'\n\n#Container name can be changed in this instance, but lambda-test is preferred.\n#  -t is a flag which tags the container with a name \n#  In the above command, ./ is the path where the container would be built. \n#  Can you recall where does ./ lead to?\n\n\n\n\nTry running the command docker images to see the images you have in your local environment.\n\n\n\n“Running” the python script requires two steps because the Lambda container is built as a listening service that will execute when there is a payload provided to it.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe syntax of docker run is as follows:\n\ndocker run -p PORT CONTAINER NAME\n\n#-p flag specifies which port needs to be used for the container to start running.\n# CONTAINER NAME can be anything, we defined it to be lambda-test in this scenario.\n\n\n\nRun the command docker run -p 8080:8080 lambda-test to set up the service on your first terminal tab. This will run the service and listen for triggers. Next, click on the green plus icon and choose New Terminal to launch a new bash terminal.\n\n\n\nIn this second terminal, run the command curl -XPOST \"http://localhost:8080/2015-03-31/functions/function/invocations\" -d '{\"payload\":\"hello world!\"}'. This should return the same response as what you saw in the Lambda service. Also, go back to the first terminal tab to see the summary of execution message."
  },
  {
    "objectID": "labs/week-tbd-labs.html#python-setup",
    "href": "labs/week-tbd-labs.html#python-setup",
    "title": "Docker",
    "section": "Python Setup",
    "text": "Python Setup\nReturn the price of any stock symbol that is submitted through the payload value for Lambda. For example, the goal is to get the DOW stock price if I run the command: http://localhost:8080/2015-03-31/functions/function/invocations\" -d '{\"payload\":\"DOW\"}'\n\nThe url has to be dynamic based on the input stock symbol: https://finance.yahoo.com/quote/DOW\nUse the requests and beautifulsoup packages to build the function. Note you will need to add these libraries to the requirements.txt file.\nStart your app.py file with this start code.\n\nimport os\nimport json\nimport requests\nimport traceback\nfrom bs4 import BeautifulSoup\n\nurl = f\"https://finance.yahoo.com/quote/DOW\"\n\n# need headers to get pull from yahoo finance\nheader = {'Connection': 'keep-alive',\n          'Expires': '-1',\n          'Upgrade-Insecure-Requests': '1',\n          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) \\\n           AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36'\n          }\n\nresponse = requests.get(url, headers=header)\nsoup = BeautifulSoup(response.text, \"html.parser\")\nprice = soup.find(\"fin-streamer\", {'data-field':\"regularMarketPrice\", 'data-symbol' : stock.upper()}).text\n\nprint(f\"price={price}\")"
  },
  {
    "objectID": "labs/week-tbd-labs.html#coding-requirements",
    "href": "labs/week-tbd-labs.html#coding-requirements",
    "title": "Docker",
    "section": "Coding Requirements:",
    "text": "Coding Requirements:\n\nAdd a try-except framework if any part of your code errors. Use the command error=traceback.format_exc() to capture the error. What do you return when the function errors instead?\nEnsure that if your function does not receive an input or if it receives an invalid stock ticker that it returns a 404 status code. For no input use the message \"No stock provided\" and for an invalid ticker choose \"Invalid stock provided\"\nMake the url dynamic to the input stock symbol specified\nIntegrate your code into the Lambda framework - event input and response output\nEnsure the response object for a successful request looks like {\"statusCode\" : 200, \"body\" : {\"stock\" : \"A\", \"price\" : \"#####\"}}.\n\n\nPlease use indent = 2 when dumping the response to make sure the response is easily readable\n\n\nHint #1: Try developing using the python console in Cloud9 before integrating into your app.py file. You don’t want to have to build a Docker image every code change, right?\n\nThis process is mainly only for prototyping. You would do this by writing the entire python code first in app.py file, then running the following command in the terminal :\n\n\npython app.py --payload {'payload': 'stock'}\nHint #2: Once you put the code into the Lambda framework, you will have to build and run to complete a development integration.\nHint #3: Implement a basic logger function to see where you might be going wrong? is ther a certain way to pass an input string to the event handler function? Try printing the event out to the console.\n\n\nUse the following test inputs to confirm your function can handle all the errors gracefully: APPL, AAPL, appl, DOW, dow. Unknown tickers need to be handled and case of the ticker should not matter.\nSubmit the bash history where this function is implemented in the file lambda-local-test.txt.\nOnce your code is ready to go with Lambda, add, commit, and push the files (app.py, Dockerfile, requirements.txt) to GitHub."
  },
  {
    "objectID": "labs/week-tbd-labs.html#posting-docker-image-to-ecr",
    "href": "labs/week-tbd-labs.html#posting-docker-image-to-ecr",
    "title": "Docker",
    "section": "Posting Docker Image to ECR",
    "text": "Posting Docker Image to ECR\nECR stands for Elastic Container Registry.\n\nRun the command aws ecr create-repository --repository-name docker-lambda to make a new repo in the elastic container registry to store your new containers.\nRun the commands to grab info on your AWS account and region.\naws_region=$(aws configure get region)\naws_account_id=$(aws sts get-caller-identity --query 'Account' --output text)\nRun the following command to configure your authentication to talk to the ECR service. Note how we use BASH variable with the $ so that you don’t have to manually enter your region or account id.\naws ecr get-login-password \\\n--region $aws_region \\\n| docker login \\\n--username AWS \\\n--password-stdin $aws_account_id.dkr.ecr.$aws_region.amazonaws.com\nTag the image in the ECR registry by running the command docker tag lambda-docker-build $aws_account_id.dkr.ecr.$aws_region.amazonaws.com/docker-lambda\n\nThe final docker-lambda is referring to the new repository you just built a few commands ago.\n\nPush the image to docker by running the command docker push $aws_account_id.dkr.ecr.$aws_region.amazonaws.com/docker-lambda\n\n\nRead more about pushing a Docker image to ECR here."
  },
  {
    "objectID": "labs/week-tbd-labs.html#docker-setup-in-lambda",
    "href": "labs/week-tbd-labs.html#docker-setup-in-lambda",
    "title": "Docker",
    "section": "Docker Setup in Lambda",
    "text": "Docker Setup in Lambda\nGo back to the Lambda dashboard by going to this link. Make a new function by clicking on the orange Create function button.\n\nYou must select the Container image option that is the third item on the top row of options for Lambda.\nName your function container-test\nSet your Execution role like we did earlier so that you use LabRole\nClick on the Browse images button to find the container you just uploaded!\n\n\n\nA popup will launch and you have to select the repository (“docker lambda”) and then your image, which will be called “latest” by default. Click on the orange Select image button.\n\n\nNow you see the same overview page for the Lambda. Since this is a container image and not simple code, we cannot actually preview anything. Just click on the Test tab.\n\nSet a name for your test aapl-test and change the event JSON to look like {\"payload\" : \"AAPL\"}. Once you are satisfied, click on the Save button and then the orange Test button.\n\nThe result of your test will be shown in a green box, and just click on the Details arrow to see the summary. Note that the stock price came back successfully. The billed duration in the example is 2578 ms, with “Init duration” contributing 709.68 ms and the code execution contributing 1867.85 ms. The results are rounded to the nearest millisecond, but are calculated at the 10 microsecond level, WOW!"
  },
  {
    "objectID": "labs/index.html",
    "href": "labs/index.html",
    "title": "Labs",
    "section": "",
    "text": "Lab\nThe lab for the class would involve a hands-on coding assignment provided through GitHub Classroom. You will start the lab in-class, myself and the TAs would be helping you with any questions with the lab and then you would need to turn in the lab by checking in your code and results in the GitHub repo (you will have until next class for this, but usually you would be able to do this much sooner).",
    "crumbs": [
      "Labs",
      "Overview",
      "Labs"
    ]
  },
  {
    "objectID": "labs/11-labs.html",
    "href": "labs/11-labs.html",
    "title": "Lambda & Docker",
    "section": "",
    "text": "DSAN 6000 Fall 2025",
    "crumbs": [
      "Labs",
      "Labs content",
      "11: Serverless & DevOps"
    ]
  },
  {
    "objectID": "labs/11-labs.html#hello-world-lambda",
    "href": "labs/11-labs.html#hello-world-lambda",
    "title": "Lambda & Docker",
    "section": "“Hello World” Lambda",
    "text": "“Hello World” Lambda\nLaunch AWS Academy and get to the AWS Console. Find the Lambda service from the search bar. \nThe dashboard shows the Lambda functions that have been made, some metrics on Lambda usage. Click on the orange Create Function button.\n\nHere you have to fill out the details for your Lambda function. There are several parts to set up.\n\nYou will leave the default option Author from scratch so that you can code directly from the Lambda service.\nSet your Function name as my-first-lambda.\nChoose your Runtime as Python 3.9\nClick on the Change default execution role dropdown, then select Use an existing role option, and finally pick the existing role LabRole. Once you have done these four things, click on the orange Create function button.\n\n\nYou now have your environment for Lambda! In the upper function overview tab, you can select a variety of triggers and destinations for the Lambda. We will leave these alone for now. You can explore both on your own time to see the options.\nLet’s start with the basic test of the “Hello World” code that was provided in the Python code. Click on the orange Test button.\n\nThis will launch a popup to configure your event. You can submit a JSON payload to the test that will mimic input data that the Lambda function can process. Start off by setting Event name to mytest. Then you can leave the Event JSON for now, but you will come back to it for future iterations of experimentation. Click on the orange save button.\n\nClick on the orange test button again. If you click on the arrow then you can choose to change or make a new test environment like you did on the previous step.\n\nYour test will execute, and the results will be shown. Several pieces of info are important:\n\nName of the test that was conducted\nResponse object that the function returned\nFunction logs that include the duration of the function, billed direction, memory max (for pricing), and actual memory used\nStatus in the upper right\n\n\nOPTIONAL - If you wanted to set your Lambda to run on a regular schedule, like a crontab, you will need to add a trigger using EventBridge (CloudWatch Events). The Trigger add would look like this for setting a job to run every day at 10:15am UTC.",
    "crumbs": [
      "Labs",
      "Labs content",
      "11: Serverless & DevOps"
    ]
  },
  {
    "objectID": "labs/11-labs.html#exploring-the-lambda-file-system---todo",
    "href": "labs/11-labs.html#exploring-the-lambda-file-system---todo",
    "title": "Lambda & Docker",
    "section": "Exploring the Lambda File System - TODO",
    "text": "Exploring the Lambda File System - TODO\nIn this section, you will store three screenshots in your Word doc to show the Lambda responses from a variety of code changes to your Lambda handler function.\nEach time you make a change to the code, you will have to click on the Deploy button and then the orange Test button.\n\n\n1. Use the os or subprocess library in python to view the contents of the root directory. Make a new key in the return dictionary and send as the value the contents of the root directory.\n\n\n2. Return the contents of the event input variable to the lambda_handler function as additional item in the return dictionary\n\n\n3. Return the contents of the context input variable to the lambda_handler function as additional item in the return dictionary. This might take a few tries! How do you deal with objects that need to become strings?",
    "crumbs": [
      "Labs",
      "Labs content",
      "11: Serverless & DevOps"
    ]
  },
  {
    "objectID": "labs/11-labs.html#creating-cloud9-environment",
    "href": "labs/11-labs.html#creating-cloud9-environment",
    "title": "Lambda & Docker",
    "section": "Creating Cloud9 Environment",
    "text": "Creating Cloud9 Environment\n\nSearch for cloud9 in the search bar of your AWS console as shown in the figure below.\n\n\n\nOnce on the Cloud9 splash screen, click on the orange button Create environment.\n\n\n\nEnter a Name and description for your environment. The figure below shows sample text you could use. Once you enter your name and description click the orange button Next step.\n\n\n\nThere are a few options here. You can leave all of the defaults. Click the orange Next step button.\n\nThe Environment type section lets you pick if you want to spin up a new EC2 machine or connect to existing resources.\nThe Instance type section is to select how large an instance for Cloud9. The small t2.micro instance is fine for Cloud9.\nThe Platform section is for selecting the operating system for your new instance.\nThe Cost-saving setting option is set so your instance will hibernate after 30 minutes so you are not charged for the instance 24/7. This is a major problem for cloud services because you can run up a bill quite quickly!\nThe IAM role is for managing permissions to AWS resources like S3. Cloud9 setup will make a new role automatically.\n\n\n\n\nThis screen shows the summary of the selections made for naming and configuring the environment. Click the orange Create environment button.\n\n\n\nThe environment will be configured for you. This takes a few minutes.\n\n\nOnce the environment setup screen goes away then you are ready to use Cloud9. If you get a warning message, just click “OK”.",
    "crumbs": [
      "Labs",
      "Labs content",
      "11: Serverless & DevOps"
    ]
  },
  {
    "objectID": "labs/11-labs.html#setting-up-basic-docker-images-in-cloud9",
    "href": "labs/11-labs.html#setting-up-basic-docker-images-in-cloud9",
    "title": "Lambda & Docker",
    "section": "Setting up Basic Docker Images in Cloud9",
    "text": "Setting up Basic Docker Images in Cloud9\nDocker image building in Cloud9 is easy since the docker package is already set up. You just have to write some code and run Linux commands!\n\nStart off by making a new folder on the lefthand folder sidebar. Call it something simple like docker-lambda-env.\nOnce you have the folder created, create three files in that folder called Dockerfile, app.py, and requirements.txt.\nThe results should look like the below and have the symbols change automatically:\n\n\n\nOpen up the Dockerfile and add the following text (note the # lines are comments just like python!)\n\n# syntax=docker/dockerfile:1\n\n# adapted from https://www.philschmid.de/aws-lambda-with-custom-docker-image\n# https://docs.aws.amazon.com/lambda/latest/dg/python-image.html\nFROM python:3.9-slim-buster\n\nCMD [\"python\", \"-c\", \"import platform; print(f\\\"version: {platform.python_version()}\\\")\"]\n\nGo to the terminal and change directories to the location of your Dockerfile. Run the command docker build ./ -t test\n\n\n\nRun the command docker run test to see if your Dockerfile worked!",
    "crumbs": [
      "Labs",
      "Labs content",
      "11: Serverless & DevOps"
    ]
  },
  {
    "objectID": "labs/11-labs.html#lambda-docker-image",
    "href": "labs/11-labs.html#lambda-docker-image",
    "title": "Lambda & Docker",
    "section": "Lambda Docker Image",
    "text": "Lambda Docker Image\n\nUse the new Dockerfile contents below for your Dockerfile. Note that this Dockerfile is invoking your requirements.txt file to install any packages from pip and the app.py lambda_handler function to run the python code.\n\n# syntax=docker/dockerfile:1\n\n# adapted from https://www.philschmid.de/aws-lambda-with-custom-docker-image\n# https://docs.aws.amazon.com/lambda/latest/dg/python-image.html\nFROM public.ecr.aws/lambda/python:3.9\n\n# copy requirements file and install necessary packages\nADD requirements.txt ${LAMBDA_TASK_ROOT}\nRUN pip3 install -r ${LAMBDA_TASK_ROOT}/requirements.txt --target \"${LAMBDA_TASK_ROOT}\"\n\n# Copy function code to docker container\nCOPY app.py ${LAMBDA_TASK_ROOT}\n\n# app (name of py file)\n# handler (name of function to execute for lambda job)\nCMD [ \"app.lambda_handler\" ]\n\nNote that the ADD and COPY commands in Docker for this instance are similar. The ADD function is more advanced and can auto-extract compressed files into the image.\nSet up your python file app.py with a function called lambda_handler that accepts the event and context arguments. Wait, we have already done this in basic Lambda! Copy your function from the Lambda service. This will ensure that the response is the same through basic Lambda and through the Docker Lambda.\n\n\n\nSince you made changes to the Dockerfile and your app.py files, you need to build a new Docker image. Run the command docker build ./ -t lambda-test.\n\n\n\n“Running” the python script requires two steps because the Lambda container is built as a listening service that will execute when there is a payload provided to it.\n\n\nRun the command docker run -p 8080:8080 lambda-test to set up the service on your first terminal tab. This will run the service and listen for triggers. Next, click on the green plus icon and choose New Terminal to launch a new bash terminal.\n\n\n\nIn this second terminal, run the command curl -XPOST \"http://localhost:8080/2015-03-31/functions/function/invocations\" -d '{\"payload\":\"hello world!\"}'. This should return the same response as what you saw in the Lambda service. Also, go back to the first terminal tab to see the summary of execution message.",
    "crumbs": [
      "Labs",
      "Labs content",
      "11: Serverless & DevOps"
    ]
  },
  {
    "objectID": "labs/11-labs.html#python-setup",
    "href": "labs/11-labs.html#python-setup",
    "title": "Lambda & Docker",
    "section": "Python Setup",
    "text": "Python Setup\nReturn the price of any stock symbol that is submitted through the payload value for Lambda. For example, I would get the DOW stock price if I ran the command: http://localhost:8080/2015-03-31/functions/function/invocations\" -d '{\"payload\":\"DOW\"}'\n\nThe url has to be dynamic based on the input stock symbol: https://finance.yahoo.com/quote/DOW\nUse the requests and beautifulsoup packages to build the function. Note you will need to add these libraries to the requirements.txt file.\nThe starter code looks like this:\n\n# import libraries\nimport requests\nfrom bs4 import BeautifulSoup\n\n# set url\nurl = f\"https://finance.yahoo.com/quote/DOW\"\n\n# get the url page results\nresponse = requests.get(url)\n\n# try to parse Beautiful Soup\ntry:\n    soup = BeautifulSoup(response.text, \"html.parser\")\nexcept Exception as e: # handle error gracefully\n    return {\n        'statusCode': 200,\n        'body': json.dumps(f'Here is the error message: {e}'),\n        } # send the error message back to the user\n\n# find the price\nprice = soup.find(\"fin-streamer\", {'data-test':\"qsp-price\"}).text\n\nprint(price)\n\nCoding Goals:\n\nAdd try-except framework if the find does not work. What do you return instead?\nMake the url dynamic to the input stock symbol specified\nIntegrate your code into the Lambda framework - event input and response output\n\nHint #1: Try developing using the python console in Cloud9 before integrating into your app.py file.\nHint #2: Once you put the code into the Lambda framework, you will have to build and run to run a development integration.\n\n\nUse the following test inputs to confirm your function can handle all the errors gracefully: APPL, AAPL, appl, DOW, dow.\n\n\nTake a screenshot of your terminal with all 5 test cases and their result and place into your Word doc\n\n\nOnce your code is ready to go with Lambda, add, commit, and push the files (app.py, Dockerfile, requirements.txt) to GitHub. Easiest way to do this is by downloading and uploading through the GitHub website.",
    "crumbs": [
      "Labs",
      "Labs content",
      "11: Serverless & DevOps"
    ]
  },
  {
    "objectID": "labs/11-labs.html#posting-docker-image-to-ecr",
    "href": "labs/11-labs.html#posting-docker-image-to-ecr",
    "title": "Lambda & Docker",
    "section": "Posting Docker Image to ECR",
    "text": "Posting Docker Image to ECR\nECR stands for Elastic Container Registry.\n\nRun the command aws ecr create-repository --repository-name docker-lambda to make a new repo in the elastic container registry to store your new containers.\nRun the command $(aws ecr get-login --no-include-email --region us-east-1) to grab the login information for your AWS account and store on the Cloud9 EC2 instance. This is bad practice for important accounts, but this account is just for experimenting!\nRun the command cat /home/ec2-user/.docker/config.json to see the contents of the authentication file. Copy the address that looks similar to 565177075063.dkr.ecr.us-east-1.amazonaws.com\nTag the image with the ECR registry by running the command docker tag lambda-test [[THE URL YOU FOUND IN THE LAST STEP]]/docker-lambda\n\nThe example looks like docker tag lambda-test 565177075063.dkr.ecr.us-east-1.amazonaws.com/docker-lambda\nNote that you have to use your own account id, not the one in the example text!\nThe final docker-lambda is referring to the new repository you just built a few commands ago.\n\nPush the image to docker by running the command docker push 565177075063.dkr.ecr.us-east-1.amazonaws.com/docker-lambda\n\n\nRead more about pushing a Docker image to ECR here.",
    "crumbs": [
      "Labs",
      "Labs content",
      "11: Serverless & DevOps"
    ]
  },
  {
    "objectID": "labs/11-labs.html#docker-setup-in-lambda",
    "href": "labs/11-labs.html#docker-setup-in-lambda",
    "title": "Lambda & Docker",
    "section": "Docker Setup in Lambda",
    "text": "Docker Setup in Lambda\nGo back to the Lambda dashboard by going to this link: https://us-east-1.console.aws.amazon.com/lambda/home?region=us-east-1#/discover. Make a new function by clicking on the orange Create function button.\n\nYou must select the Container image option that is the third item on the top row of options for Lambda.\nName your function container-test\nSet your Execution role like we did earlier so that you use LabRole\nClick on the Browse images button to find the container you just uploaded!\n\n\n\nA popup will launch and you have to select the repository (“docker lambda”) and then your image, which will be called “latest” by default. Click on the orange Select image button.\n\n\nNow you see the same overview page for the Lambda. Since this is a container image and not simple code, we cannot actually preview anything. Just click on the Test tab.\n\nSet a name for your test aapl-test and change the event JSON to look like {\"payload\" : \"AAPL\"}. Once you are satisfied, click on the Save button and then the orange Test button.\n\nThe result of your test will be shown in a green box, and just click on the Details arrow to see the summary. Note that the stock price came back successfully. The billed duration in the example is 2578 ms, with “Init duration” contributing 709.68 ms and the code execution contributing 1867.85 ms. The results are rounded to the nearest millisecond, but are calculated at the 10 microsecond level, WOW!\n\n\nTake a screenshot of the success output from the test you made in Lambda into your Word doc\n\n\nEXTRA CREDIT - 1 points. Modify your app.py to accept multiple stocks comma separated like {“payload” : “AAPL,DOW,MSFT”}",
    "crumbs": [
      "Labs",
      "Labs content",
      "11: Serverless & DevOps"
    ]
  },
  {
    "objectID": "labs/11-labs.html#github-classroom",
    "href": "labs/11-labs.html#github-classroom",
    "title": "Lambda & Docker",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link",
    "crumbs": [
      "Labs",
      "Labs content",
      "11: Serverless & DevOps"
    ]
  },
  {
    "objectID": "labs/09-labs.html",
    "href": "labs/09-labs.html",
    "title": "Lab 9",
    "section": "",
    "text": "DSAN 6000 Fall 2025",
    "crumbs": [
      "Labs",
      "Labs content",
      "9: Spark NLP"
    ]
  },
  {
    "objectID": "labs/09-labs.html#github-classroom",
    "href": "labs/09-labs.html#github-classroom",
    "title": "Lab 9",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link",
    "crumbs": [
      "Labs",
      "Labs content",
      "9: Spark NLP"
    ]
  },
  {
    "objectID": "labs/07-labs.html#the-right-data-tool",
    "href": "labs/07-labs.html#the-right-data-tool",
    "title": "Lab 7 - SparkSQL",
    "section": "The right data tool…",
    "text": "The right data tool…\n\nSometimes you only need simple solutions to problems. Perfection is the enemy of the good. We’ll walk through a few options when considering a big data job.\nHere are a few options when your data is local:\n\nUsing a single process in R/python\nWriting an R/python function to run in parallel on a single node\nUsing Python cluster tools like Ray or Dask\nUsing PySpark on a single node to development with multiple workers\n\nHere are a few options when your data is distributed\n\nUsing Dask on a cluster\nUsing PySpark on a cluster\n\nHere are a few options for task-specific needs:\n\nHarnessing a specialized solution like GPU data science with RAPIDS. This software is developed so you can conduct your entire data science pipeline on a GPU.\nTensorFlow distributed netural network training with Spark!\nLarge language model processing with Spark!",
    "crumbs": [
      "Labs",
      "Labs content",
      "7: Spark SQL & Dataframes"
    ]
  },
  {
    "objectID": "labs/07-labs.html#rdd-vs-pysparksql-dataframe",
    "href": "labs/07-labs.html#rdd-vs-pysparksql-dataframe",
    "title": "Lab 7 - SparkSQL",
    "section": "RDD vs PySparkSQL DataFrame",
    "text": "RDD vs PySparkSQL DataFrame\nRDD (resilient distributed dataset) is an immutable collection of records (rows) of data\n\ngreat for unstructured data\ndoing low-level transformations\n\nPySpark DataFrame is organized into a table with rows and columns. This is just like the format when working in a relational database like Hive.\n\nTwo dimensional table format, great for “standard” datasets\nColumn format means consistent metadata and data typing\nQuery optimization\nSchema is created automatically!",
    "crumbs": [
      "Labs",
      "Labs content",
      "7: Spark SQL & Dataframes"
    ]
  },
  {
    "objectID": "labs/07-labs.html#common-actions-in-r-python-pandas-and-pyspark",
    "href": "labs/07-labs.html#common-actions-in-r-python-pandas-and-pyspark",
    "title": "Lab 7 - SparkSQL",
    "section": "Common Actions in R, Python Pandas, and PySpark",
    "text": "Common Actions in R, Python Pandas, and PySpark\nAnything you can do, I can do in parallel!\nTDS big data options\nhead(starwars)\n\nReading in data\n\nR\nlibrary(arrow)\nlibrary(dplyr)\n\nstarwars &lt;- arrow::read_parquet('starwars.parquet')\n\n\nPandas\nimport os\nimport pandas as pd\nstarwars = pd.read_parquet('starwars.parquet')\n\n\nPySpark\nimport pyspark\nfrom pyspark.sql.functions import udf, lit, col\nfrom pyspark.sql import SparkSession\n\n# configuration for workers\nconfig = pyspark.SparkConf().setAll([('spark.executor.memory', '10g'),\n                             ('spark.executor.cores', '2'),\n                             ('spark.cores.max', '16'),])\n# launch cluster connection\nsc = pyspark.SparkContext(conf = config)\n\n# set up pyspark session\nspark = pyspark.SparkSession.builder.appName('my-test').getOrCreate()\n\nstarwars = spark.read.load('starwars.parquet')\n\n\n\nSelecting data variables\n\nR\nstarwars_select &lt;- starwars %&gt;% select(name, height, mass)\n\n\nPandas\nstarwars_select = starwars[['name','height','mass']]\n\n\nPySparkSQL\nstarwars_select = starwars.select(['name','height','mass'])\n\n\n\nFiltering data rows\n\nR\nstarwars_filter &lt;- starwars %&gt;% filter(height &gt; 110, \n                                       homeworld == \"Tatooine\")\n\n\nPandas\nstarwars_filter = starwars[(starwars.height &gt; 110) & \n                        (starwars.homeworld == \"Tatooine\")]\n\n\nPySpark\nstarwars_filter = starwars[(col('height') &gt; 110) &\n                    (col('homeworld') == \"Tatooine\")]\n\n\n\nManipulating data\n\nR\nstarwars &lt;- starwars %&gt;% \n    mutate(tatooine_dummy = if_else(homeworld == 'Tatooine',\n                                    TRUE,\n                                    FALSE))\n\n\nPandas\nstarwars['tatooine_dummy'] = starwars.apply(\n                                  lambda x: True if x.homeworld == 'Tatooine' \n                                        else False, \n                                                axis = 1)\n\n\nPySpark\nfrom pyspark.sql.types import BooleanType\n@udf(returnType=BooleanType())\ndef dummy_tatooine(x):\n    if x == 'Tatooine':\n        return True\n    else:\n        return False\n\nstarwars = starwars.withColumn('tatooine_dummy', \n                dummy_tatooine(col('homeworld')))\n\n\n\nView the head of the data\n\nR\nstarwars %&gt;% head(5)\n\n\nPandas\nstarwars.head(5)\n\n\nPySpark\nstarwars.take(5) # RDD version\n\nstarwars.show(5) # SQL version\n\n\n\nGroup-by mean data\n\nR\nstarwars %&gt;% group_by(species) %&gt;% summarize(mean_height = mean(height))\n\n\nPandas\nstarwars.groupby('species')['height'].mean()\n\n\nPySpark\nstarwars.groupBy('species').mean('height').collect()\n\n\n\nTallest character from each species\n\nR\nstarwars %&gt;% group_by(species) %&gt;% filter(height = max(height))\n\n\nPandas\nstarwars.iloc[starwars.groupby('species').height.idxmax().tolist(),:]\n\nstarwars.sort_values('height',ascending=False).groupby('species').first()\n\n\nPySpark\ntemp_df = starwars.groupBy('species').agg(f.max('height').alias('height'))\nstarwars.groupBy.join(temp_df,on='height',how='leftsemi').show()\n\nfrom pyspark.sql import Window\nw = Window.partitionBy('species')\nstarwars.withColumn('maxheight', f.max('height').over(w))\\\n    .where(f.col('height') == f.col('maxheight'))\\\n    .drop('maxheight')\\\n    .show()",
    "crumbs": [
      "Labs",
      "Labs content",
      "7: Spark SQL & Dataframes"
    ]
  },
  {
    "objectID": "labs/07-labs.html#collecting-data",
    "href": "labs/07-labs.html#collecting-data",
    "title": "Lab 7 - SparkSQL",
    "section": "Collecting Data",
    "text": "Collecting Data\nBe extra careful when using the .collect() function. If you have massive amounts of data, then your spark driver is going to have trouble.\n\nIn general, always run a .count() function to check the number of rows before running .collect(). Alternatively, you can run the command .show(5) or .take(5) to only see the first few rows of data. You never want to bring 10s of millions of rows to your local session. Let the big data live in big data land.",
    "crumbs": [
      "Labs",
      "Labs content",
      "7: Spark SQL & Dataframes"
    ]
  },
  {
    "objectID": "labs/07-labs.html#github-classroom",
    "href": "labs/07-labs.html#github-classroom",
    "title": "Lab 7 - SparkSQL",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link",
    "crumbs": [
      "Labs",
      "Labs content",
      "7: Spark SQL & Dataframes"
    ]
  },
  {
    "objectID": "labs/05-labs.html",
    "href": "labs/05-labs.html",
    "title": "Lab 5",
    "section": "",
    "text": "DSAN 6000 Fall 2025",
    "crumbs": [
      "Labs",
      "Labs content",
      "5: Data Warehouse"
    ]
  },
  {
    "objectID": "labs/05-labs.html#github-classroom",
    "href": "labs/05-labs.html#github-classroom",
    "title": "Lab 5",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link",
    "crumbs": [
      "Labs",
      "Labs content",
      "5: Data Warehouse"
    ]
  },
  {
    "objectID": "labs/03-labs.html",
    "href": "labs/03-labs.html",
    "title": "Lab 3",
    "section": "",
    "text": "DSAN 6000 Fall 2025",
    "crumbs": [
      "Labs",
      "Labs content",
      "3: Parallelization"
    ]
  },
  {
    "objectID": "labs/03-labs.html#github-classroom",
    "href": "labs/03-labs.html#github-classroom",
    "title": "Lab 3",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link",
    "crumbs": [
      "Labs",
      "Labs content",
      "3: Parallelization"
    ]
  },
  {
    "objectID": "labs/01-labs.html",
    "href": "labs/01-labs.html",
    "title": "Lab 1",
    "section": "",
    "text": "DSAN 6000 Fall 2025",
    "crumbs": [
      "Labs",
      "Labs content",
      "1: Course overview"
    ]
  },
  {
    "objectID": "labs/01-labs.html#goals",
    "href": "labs/01-labs.html#goals",
    "title": "Lab 1",
    "section": "Goals",
    "text": "Goals\n\nCreating your public/private ssh key pair and knowing where to find the files\nLearning to use GitHub Codespaces IDE\nLearning the Linux Shell\nBASH Exercise",
    "crumbs": [
      "Labs",
      "Labs content",
      "1: Course overview"
    ]
  },
  {
    "objectID": "labs/01-labs.html#windows-users",
    "href": "labs/01-labs.html#windows-users",
    "title": "Lab 1",
    "section": "Windows Users",
    "text": "Windows Users\nWindows users will be using the Windows Powershell: &lt;https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/powershell&gt;\nWindows Powershell is most likely installed if you have Windows 10. If you don’t have Powershell, take a look at this article: https://www.howtogeek.com/336775/how-to-enable-and-use-windows-10s-built-in-ssh-commands/ that explains how to install it.\nYou can find Powershell by typing “Powershell” into the search bar:\n\nOnce Powershell is running, this is your terminal:\n\n\nAdditional Powershell Configuration (you must do this!)\nYou need to perform this step **only once** to be able to use agent forwarding which is explained further in the lab.\n\nExit Powershell if running\nStart a new Powershell session using run as Administrator\nEnter the following command (you can cut/paste from here):\n\n\nGet-Service -Name ssh-agent | Set-Service -StartupType Manual\n\nExit Powershell. You should not need to run as administrator going forward.",
    "crumbs": [
      "Labs",
      "Labs content",
      "1: Course overview"
    ]
  },
  {
    "objectID": "labs/01-labs.html#mac-and-linux-users",
    "href": "labs/01-labs.html#mac-and-linux-users",
    "title": "Lab 1",
    "section": "Mac and Linux Users",
    "text": "Mac and Linux Users\nFor Mac and Linux users, you will open up the Terminal.\n\nMacs and Linux have a built in Terminal.\nOr, you can use iTerm app: &lt;https://www.iterm2.com/&gt;\n\nIf you are on Linux (but not on Mac), you can open the terminal by using Ctrl-Alt-T.",
    "crumbs": [
      "Labs",
      "Labs content",
      "1: Course overview"
    ]
  },
  {
    "objectID": "labs/01-labs.html#ssh-keypair-setup",
    "href": "labs/01-labs.html#ssh-keypair-setup",
    "title": "Lab 1",
    "section": "SSH Keypair Setup",
    "text": "SSH Keypair Setup\nWhen you want to connect to a remote machine, the method is called “Secure Shell”. This creates a connection between the local machine (where your terminal window lives) and the “remote” machine (where the commands you will send actually execute). In order for the local and remote machines to authenticate (trust) each other, we have to create a special password-like files called a keypair. It is called a keypair because there is a public version and a private version. Read more about SSH Keys here.\nNOTE: You only need to create your ssh public/private keypair one time only. If you already have a public/private keypair on your laptop let us know.\n\nOpen a terminal (on your laptop) if not already open. By default, every time you open a terminal it will open in your home directory.\nAt the command prompt run the following command: ssh-keygen -t rsa -b 2048 and press enter\nYou will see this prompt, just press enter\n\n\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/User/.ssh/id_rsa):\n\nYou will see this prompt, just press enter\n\n\nCreated directory '/home/User/.ssh'.\nEnter passphrase (empty for no passphrase):\n\nYou will see this prompt, just press enter\n\n\nEnter same passphrase again:\n\nYou will see these messages (your randomart will look different) and your keypair has been created.\n\n\nYour identification has been saved in /home/User_name/.ssh/id_rsa.\nYour public key has been saved in /home/User_name/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:xPJtMLmJSO73x/NQo3qMpqF6r6St4ONmshS8QZqfmHA User_name@WinDev1802Eval\n\nThe key's randomart image is:\n+---[RSA 2048]----+\n|                 |\n|       . .       |\n| .  . . *        |\n|+. o . = *       |\n|++E o . S o o    |\n|.=+o     . o .   |\n|+oo o o  +o      |\n|+= +.o oo.*.     |\n|*+=++ooooo o.    |\n+----[SHA256]-----+",
    "crumbs": [
      "Labs",
      "Labs content",
      "1: Course overview"
    ]
  },
  {
    "objectID": "labs/01-labs.html#see-your-key-files",
    "href": "labs/01-labs.html#see-your-key-files",
    "title": "Lab 1",
    "section": "See your key files",
    "text": "See your key files\n\nOpen a terminal if not already open\nChange to your .ssh directory\n\nThis is a hidden directory so if you list your files using ls you won’t see it. For seeing all files, use ls -la.\nTo change into the .ssh directory type cd .ssh\n\nType pwd to print your current working directory.\n\nWindows users in Powershell will see:\n\n\nPS C:\\\\Users\\\\your_name\\.ssh&gt; pwd\n\nPath\n----\nC:\\\\Users\\\\your_name\\.ssh\n\n\nPS C:\\\\Users\\\\your_name\\.ssh&gt;\n\nMac users will see:\n\n\npwd\n/Users/myusername/.ssh\n\nLinux users will see:\n\n\n\\$ pwd\n\n/home/myusername/.ssh\n\nNext, we need to open the new key file we just made.\n\nType ls to list the files in the directory.\n\nWhat is displayed may look different. You will not have a config file unless you have already created one.\n\nType ls -la to list all the files in the directory, even the hidden ones.\n\nWhat is displayed may look different. You will not have a config file unless you have already created one.",
    "crumbs": [
      "Labs",
      "Labs content",
      "1: Course overview"
    ]
  },
  {
    "objectID": "labs/01-labs.html#get-your-public-key-info",
    "href": "labs/01-labs.html#get-your-public-key-info",
    "title": "Lab 1",
    "section": "Get your public key info",
    "text": "Get your public key info\n\nThe file id_rsa is your private key and this file will not leave your computer.\nThe file id_rsa.pub is the public key, whose contents we will upload to cloud services so you authenticate.\nThe known_hosts is a file that gets generated as you connect to different remote systems.\n\nThis is useful so you know you are connecting to the same server as previous times.\n\n\n\n\\$ ls -la\n\ntotal 32\n\ndrwxr-xr-x  6  your_name staff   192 May 29 20:39 .\ndrwxr-xr-x+ 75 your_name staff  2400 May 30 13:35 ..\n-rw-r--r--  1  your_name staff   181 May 29 15:50 config\n-r--------  1  your_name staff  3243 May 29 15:50 id_rsa\n-rw-r--r--  1  your_name staff   742 May 29 15:50 id_rsa.pub\n-rw-r--r--  1  your_name staff   363 May 29 20:42 known_hosts\n\nView the contents of your public_key file by running the command cat id_rsa.pub\n\nWhat is shown is a sample public key, yours will be different\n\n\n\n\\$ cat id_rsa.pub\n\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCnKuIRXwZu0JZH0/Q2XNrYYTaJT7bMtXGhGQaSSOZs6MhQ4SkSbHiygO7RauQf741buLnASzY27GKMMMml6InwfxJWrF60KhNK0r869POQkuZa9v9/cmYcEIzmAJe1xRPABEZ2yfbTG9Wq4sg9cU0mwt1Bx7wiN4QNf0Bak62EC8JWTbcKLduuzO1zabIb5xW9gfR9b4K3HwmqRLl18S8bNsfYQZfvtlwd0mCWQUeuEGbDOgqh//nLIj6DeXdyxbD5xrz79iOAuAK2nXAjNCEtKpxNGQr2Py7aWQjlH+U5laDEHVg4hzmBY7yoZ5eC3Ye45yPqpQA1y8JrbXVhPJRP User\\@WinDev1802Eval",
    "crumbs": [
      "Labs",
      "Labs content",
      "1: Course overview"
    ]
  },
  {
    "objectID": "labs/01-labs.html#pubkey",
    "href": "labs/01-labs.html#pubkey",
    "title": "Lab 1",
    "section": "Extracting your public key",
    "text": "Extracting your public key\n\nOpen a text editor (Notepad on Windows or Textpad on Mac, NOT MICROSOFT WORD) and select the output of your terminal with all the text from the ssh-rsa beginning all the way to the end, and paste it in your text editor as-is. We will use this in the next step.\n\nYou can also just copy/paste from your terminal screen.\nOn a Mac, you can also copy the contents of the id_rsa.pub file using\n\n\n\npbcopy &lt; id_rsa.pub",
    "crumbs": [
      "Labs",
      "Labs content",
      "1: Course overview"
    ]
  },
  {
    "objectID": "labs/01-labs.html#adding-ssh-key-to-github",
    "href": "labs/01-labs.html#adding-ssh-key-to-github",
    "title": "Lab 1",
    "section": "Adding SSH Key to GitHub",
    "text": "Adding SSH Key to GitHub\n\na) Create a GitHub Account if you do not already have one\nGo to www.github.com to create a GitHub account if you do not already have one. Your username has to be globally unique, and the email address you use to register GitHub can be any email address you own.\n\n\nb) Upload your Public key to GitHub\n\nLog into to your GitHub account if you are not already logged in\nClick on your profile icon on the top-right of the screen and select Settings from the dropdown\nClick on SSH and GPG keys from the left hand menu\nClick on the New SSH key button on the top-right\nGive your key a name. This is just a name and is meaningful to you.\nPaste the contents of the public key in the Key box. Leave the “Key Type” dropdown as “Authentication Key”.\nClick the Add SSH Key button\n\n\n\nc) Test that your ssh key works with GitHub\n\nOpen a terminal if not already open on your laptop\nAt the command prompt, type ssh -T git@github.com and press enter to test. If it works, you will see something like this, with your GitHub username:\n\n\nThe authenticity of host 'github.com (192.30.253.112)' can't be established.\nRSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8.\nAre you sure you want to continue connecting (yes/no)? yes\nWarning: Permanently added 'github.com,192.30.253.112' (RSA) to the list of known hosts.\nHi wahalulu! You've successfully authenticated, but GitHub does not provide shell access.\nYou are now ready to use ssh authentication with GitHub.",
    "crumbs": [
      "Labs",
      "Labs content",
      "1: Course overview"
    ]
  },
  {
    "objectID": "labs/01-labs.html#create-a-personal-access-token-on-github",
    "href": "labs/01-labs.html#create-a-personal-access-token-on-github",
    "title": "Lab 1",
    "section": "Create a Personal Access Token on GitHub",
    "text": "Create a Personal Access Token on GitHub\n\nLog into to your GitHub account if you are not already logged in\nClick on your profile icon on the top-right of the screen and select Settings from the dropdown\nClick Developer settings\nClick the Personal access tokens tab\nClick the Generate new token button\nEnter a token description (you can call it big-data-class)\nSelect the repo permission, and then click the Generate token button\n\n\n\nCopy the token and save it in a text file. You will need this token later on in the semester and if you lose it you will need to re-generate a token",
    "crumbs": [
      "Labs",
      "Labs content",
      "1: Course overview"
    ]
  },
  {
    "objectID": "labs/01-labs.html#github-codespaces-ide",
    "href": "labs/01-labs.html#github-codespaces-ide",
    "title": "Lab 1",
    "section": "GitHub Codespaces IDE",
    "text": "GitHub Codespaces IDE\nCodespaces is an integrated developer environment (IDE), which provides you with a VS Code environment with a cloud computing backend. The repository is automatically loaded into your environment and you do not need any additional authentication steps to push to your repo. Read more here about Codespaces.\nIn this lab, we use Codespaces to get you familiar with the Linux terminal.\nYou will not use Codespaces for most of your work this semester. If you were just exploring code or making minor changes to a project, Codespaces could be a simple solution.",
    "crumbs": [
      "Labs",
      "Labs content",
      "1: Course overview"
    ]
  },
  {
    "objectID": "labs/01-labs.html#launching-codespaces",
    "href": "labs/01-labs.html#launching-codespaces",
    "title": "Lab 1",
    "section": "Launching Codespaces",
    "text": "Launching Codespaces\nOpen your Git repo for the lab. Launch Codespaces from your repo by clicking on the green code button, then select “codespaces” then click “Create codespace on main”.\n\nWait for the workspace to be created. It will set up the computing instance with a screen like this.",
    "crumbs": [
      "Labs",
      "Labs content",
      "1: Course overview"
    ]
  },
  {
    "objectID": "labs/01-labs.html#linux-terminal-in-codespaces",
    "href": "labs/01-labs.html#linux-terminal-in-codespaces",
    "title": "Lab 1",
    "section": "Linux Terminal in Codespaces",
    "text": "Linux Terminal in Codespaces\nYou now have full VS Code capabilities. The terminal in Cloud9 is the lower window. You are connected to a cloud computing instance and are able to run BASH commands (along with other programming languages) there.\n\nCheck out the cheat sheet on Linux commands here.",
    "crumbs": [
      "Labs",
      "Labs content",
      "1: Course overview"
    ]
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Lectures and labs",
    "section": "",
    "text": "Each class is divided into two parts: a lecture followed by a lab. Some class optionally have a reading component also that needs to be completed before class.\n\nLecture\nDuring the lecture, I will be going over the slides for that lecture, this would usually include a quick review of the previous class, feedback on a recently graded assignment and then cover the topic of the day.\nThe lecture would usually last about 90 minutes, followed by 5 to 10 minutes break.\n\n\nLab\nThe lab for the class would involve a hands-on coding assignment provided through GitHub Classroom. You will start the lab in-class, myself and the TAs would be helping you with any questions with the lab and then you would need to turn in the lab by checking in your code and results in the GitHub repo (you will have until next class for this, but usually you would be able to do this much sooner).",
    "crumbs": [
      "Content",
      "Overview",
      "Lectures and labs"
    ]
  },
  {
    "objectID": "content/13-content.html",
    "href": "content/13-content.html",
    "title": "DSAN 6000",
    "section": "",
    "text": "Ray, RAPIDS and DuckDB",
    "crumbs": [
      "Content",
      "Course content",
      "13: Ray, RAPIDS & DuckDB"
    ]
  },
  {
    "objectID": "content/13-content.html#about-todays-class",
    "href": "content/13-content.html#about-todays-class",
    "title": "DSAN 6000",
    "section": "",
    "text": "Ray, RAPIDS and DuckDB",
    "crumbs": [
      "Content",
      "Course content",
      "13: Ray, RAPIDS & DuckDB"
    ]
  },
  {
    "objectID": "content/13-content.html#readings",
    "href": "content/13-content.html#readings",
    "title": "DSAN 6000",
    "section": "Readings",
    "text": "Readings\nNo prescribed readings for this lecture, but you should have completed Lab 0 - Background Skills before this class.",
    "crumbs": [
      "Content",
      "Course content",
      "13: Ray, RAPIDS & DuckDB"
    ]
  },
  {
    "objectID": "content/13-content.html#slides",
    "href": "content/13-content.html#slides",
    "title": "DSAN 6000",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys.",
    "crumbs": [
      "Content",
      "Course content",
      "13: Ray, RAPIDS & DuckDB"
    ]
  },
  {
    "objectID": "content/13-content.html#lab",
    "href": "content/13-content.html#lab",
    "title": "DSAN 6000",
    "section": "Lab",
    "text": "Lab\nThe lab for today’s lesson is available online as an HTML file.",
    "crumbs": [
      "Content",
      "Course content",
      "13: Ray, RAPIDS & DuckDB"
    ]
  },
  {
    "objectID": "content/13-content.html#assignment",
    "href": "content/13-content.html#assignment",
    "title": "DSAN 6000",
    "section": "Assignment",
    "text": "Assignment\nNo assignment for this class.",
    "crumbs": [
      "Content",
      "Course content",
      "13: Ray, RAPIDS & DuckDB"
    ]
  },
  {
    "objectID": "content/11-content.html",
    "href": "content/11-content.html",
    "title": "DSAN 6000",
    "section": "",
    "text": "Lambda & Docker.",
    "crumbs": [
      "Content",
      "Course content",
      "11: Serverless & DevOps"
    ]
  },
  {
    "objectID": "content/11-content.html#about-todays-class",
    "href": "content/11-content.html#about-todays-class",
    "title": "DSAN 6000",
    "section": "",
    "text": "Lambda & Docker.",
    "crumbs": [
      "Content",
      "Course content",
      "11: Serverless & DevOps"
    ]
  },
  {
    "objectID": "content/11-content.html#readings",
    "href": "content/11-content.html#readings",
    "title": "DSAN 6000",
    "section": "Readings",
    "text": "Readings\nNo prescribed readings for this lecture.",
    "crumbs": [
      "Content",
      "Course content",
      "11: Serverless & DevOps"
    ]
  },
  {
    "objectID": "content/11-content.html#slides",
    "href": "content/11-content.html#slides",
    "title": "DSAN 6000",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys.",
    "crumbs": [
      "Content",
      "Course content",
      "11: Serverless & DevOps"
    ]
  },
  {
    "objectID": "content/11-content.html#lab",
    "href": "content/11-content.html#lab",
    "title": "DSAN 6000",
    "section": "Lab",
    "text": "Lab\nThe lab for today’s lesson is available online as an HTML file.",
    "crumbs": [
      "Content",
      "Course content",
      "11: Serverless & DevOps"
    ]
  },
  {
    "objectID": "content/11-content.html#assignment",
    "href": "content/11-content.html#assignment",
    "title": "DSAN 6000",
    "section": "Assignment",
    "text": "Assignment\nNo assignment for this class.",
    "crumbs": [
      "Content",
      "Course content",
      "11: Serverless & DevOps"
    ]
  },
  {
    "objectID": "content/09-content.html",
    "href": "content/09-content.html",
    "title": "DSAN 6000",
    "section": "",
    "text": "Spark 4: NLP with Spark.",
    "crumbs": [
      "Content",
      "Course content",
      "9: Spark NLP"
    ]
  },
  {
    "objectID": "content/09-content.html#about-todays-class",
    "href": "content/09-content.html#about-todays-class",
    "title": "DSAN 6000",
    "section": "",
    "text": "Spark 4: NLP with Spark.",
    "crumbs": [
      "Content",
      "Course content",
      "9: Spark NLP"
    ]
  },
  {
    "objectID": "content/09-content.html#readings",
    "href": "content/09-content.html#readings",
    "title": "DSAN 6000",
    "section": "Readings",
    "text": "Readings\nNo prescribed readings for this lecture.",
    "crumbs": [
      "Content",
      "Course content",
      "9: Spark NLP"
    ]
  },
  {
    "objectID": "content/09-content.html#slides",
    "href": "content/09-content.html#slides",
    "title": "DSAN 6000",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys.",
    "crumbs": [
      "Content",
      "Course content",
      "9: Spark NLP"
    ]
  },
  {
    "objectID": "content/09-content.html#lab",
    "href": "content/09-content.html#lab",
    "title": "DSAN 6000",
    "section": "Lab",
    "text": "Lab\nThe lab for today’s lesson is available online as an HTML file.",
    "crumbs": [
      "Content",
      "Course content",
      "9: Spark NLP"
    ]
  },
  {
    "objectID": "content/09-content.html#assignment",
    "href": "content/09-content.html#assignment",
    "title": "DSAN 6000",
    "section": "Assignment",
    "text": "Assignment\nNo assignment for this class.",
    "crumbs": [
      "Content",
      "Course content",
      "9: Spark NLP"
    ]
  },
  {
    "objectID": "content/07-content.html",
    "href": "content/07-content.html",
    "title": "DSAN 6000",
    "section": "",
    "text": "Spark 2: Spark DataFrames and SparkSQL.",
    "crumbs": [
      "Content",
      "Course content",
      "7: Spark SQL & Dataframes"
    ]
  },
  {
    "objectID": "content/07-content.html#about-todays-class",
    "href": "content/07-content.html#about-todays-class",
    "title": "DSAN 6000",
    "section": "",
    "text": "Spark 2: Spark DataFrames and SparkSQL.",
    "crumbs": [
      "Content",
      "Course content",
      "7: Spark SQL & Dataframes"
    ]
  },
  {
    "objectID": "content/07-content.html#readings",
    "href": "content/07-content.html#readings",
    "title": "DSAN 6000",
    "section": "Readings",
    "text": "Readings\nNo prescribed readings for this lecture.",
    "crumbs": [
      "Content",
      "Course content",
      "7: Spark SQL & Dataframes"
    ]
  },
  {
    "objectID": "content/07-content.html#slides",
    "href": "content/07-content.html#slides",
    "title": "DSAN 6000",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys.",
    "crumbs": [
      "Content",
      "Course content",
      "7: Spark SQL & Dataframes"
    ]
  },
  {
    "objectID": "content/07-content.html#lab",
    "href": "content/07-content.html#lab",
    "title": "DSAN 6000",
    "section": "Lab",
    "text": "Lab\nThe lab for today’s lesson is available online as an HTML file.",
    "crumbs": [
      "Content",
      "Course content",
      "7: Spark SQL & Dataframes"
    ]
  },
  {
    "objectID": "content/07-content.html#assignment",
    "href": "content/07-content.html#assignment",
    "title": "DSAN 6000",
    "section": "Assignment",
    "text": "Assignment\nGitHub Classroom Link",
    "crumbs": [
      "Content",
      "Course content",
      "7: Spark SQL & Dataframes"
    ]
  },
  {
    "objectID": "content/05-content.html",
    "href": "content/05-content.html",
    "title": "DSAN 6000",
    "section": "",
    "text": "Scaling up data analytics with Dask.",
    "crumbs": [
      "Content",
      "Course content",
      "5: Data Warehouse"
    ]
  },
  {
    "objectID": "content/05-content.html#about-todays-class",
    "href": "content/05-content.html#about-todays-class",
    "title": "DSAN 6000",
    "section": "",
    "text": "Scaling up data analytics with Dask.",
    "crumbs": [
      "Content",
      "Course content",
      "5: Data Warehouse"
    ]
  },
  {
    "objectID": "content/05-content.html#readings",
    "href": "content/05-content.html#readings",
    "title": "DSAN 6000",
    "section": "Readings",
    "text": "Readings\nReadings for this lecture (to be completed before this class):\n- Rocklin",
    "crumbs": [
      "Content",
      "Course content",
      "5: Data Warehouse"
    ]
  },
  {
    "objectID": "content/05-content.html#slides",
    "href": "content/05-content.html#slides",
    "title": "DSAN 6000",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys.",
    "crumbs": [
      "Content",
      "Course content",
      "5: Data Warehouse"
    ]
  },
  {
    "objectID": "content/05-content.html#lab",
    "href": "content/05-content.html#lab",
    "title": "DSAN 6000",
    "section": "Lab",
    "text": "Lab\nThe lab for today’s lesson is available online as an HTML file.",
    "crumbs": [
      "Content",
      "Course content",
      "5: Data Warehouse"
    ]
  },
  {
    "objectID": "content/05-content.html#assignment",
    "href": "content/05-content.html#assignment",
    "title": "DSAN 6000",
    "section": "Assignment",
    "text": "Assignment\nGitHub Classroom Link",
    "crumbs": [
      "Content",
      "Course content",
      "5: Data Warehouse"
    ]
  },
  {
    "objectID": "content/03-content.html",
    "href": "content/03-content.html",
    "title": "DSAN 6000",
    "section": "",
    "text": "Scaling up on a single machine with Python multiprocessing.",
    "crumbs": [
      "Content",
      "Course content",
      "3: Parallelization"
    ]
  },
  {
    "objectID": "content/03-content.html#about-todays-class",
    "href": "content/03-content.html#about-todays-class",
    "title": "DSAN 6000",
    "section": "",
    "text": "Scaling up on a single machine with Python multiprocessing.",
    "crumbs": [
      "Content",
      "Course content",
      "3: Parallelization"
    ]
  },
  {
    "objectID": "content/03-content.html#readings",
    "href": "content/03-content.html#readings",
    "title": "DSAN 6000",
    "section": "Readings",
    "text": "Readings\nReadings for this lecture (to be completed before this class):\nWolohan Ch. 1,2,5",
    "crumbs": [
      "Content",
      "Course content",
      "3: Parallelization"
    ]
  },
  {
    "objectID": "content/03-content.html#slides",
    "href": "content/03-content.html#slides",
    "title": "DSAN 6000",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys.",
    "crumbs": [
      "Content",
      "Course content",
      "3: Parallelization"
    ]
  },
  {
    "objectID": "content/03-content.html#lab",
    "href": "content/03-content.html#lab",
    "title": "DSAN 6000",
    "section": "Lab",
    "text": "Lab\nParallelization with Python and multiprocessing. The lab for today’s lesson is available online as an HTML file.",
    "crumbs": [
      "Content",
      "Course content",
      "3: Parallelization"
    ]
  },
  {
    "objectID": "content/03-content.html#assignment",
    "href": "content/03-content.html#assignment",
    "title": "DSAN 6000",
    "section": "Assignment",
    "text": "Assignment\nGitHub Classroom Link",
    "crumbs": [
      "Content",
      "Course content",
      "3: Parallelization"
    ]
  },
  {
    "objectID": "content/01-content.html",
    "href": "content/01-content.html",
    "title": "DSAN 6000",
    "section": "",
    "text": "Course overview, big data concepts, cloud computing and evolution of cloud technologies.",
    "crumbs": [
      "Content",
      "Course content",
      "1: Course overview"
    ]
  },
  {
    "objectID": "content/01-content.html#about-todays-class",
    "href": "content/01-content.html#about-todays-class",
    "title": "DSAN 6000",
    "section": "",
    "text": "Course overview, big data concepts, cloud computing and evolution of cloud technologies.",
    "crumbs": [
      "Content",
      "Course content",
      "1: Course overview"
    ]
  },
  {
    "objectID": "content/01-content.html#readings",
    "href": "content/01-content.html#readings",
    "title": "DSAN 6000",
    "section": "Readings",
    "text": "Readings\nNo prescribed readings for this lecture, but you should have completed Lab 0 - Background Skills before this class.",
    "crumbs": [
      "Content",
      "Course content",
      "1: Course overview"
    ]
  },
  {
    "objectID": "content/01-content.html#slides",
    "href": "content/01-content.html#slides",
    "title": "DSAN 6000",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys.",
    "crumbs": [
      "Content",
      "Course content",
      "1: Course overview"
    ]
  },
  {
    "objectID": "content/01-content.html#lab",
    "href": "content/01-content.html#lab",
    "title": "DSAN 6000",
    "section": "Lab",
    "text": "Lab\nThe lab for today’s lesson is available online as an HTML file.",
    "crumbs": [
      "Content",
      "Course content",
      "1: Course overview"
    ]
  },
  {
    "objectID": "content/01-content.html#assignment",
    "href": "content/01-content.html#assignment",
    "title": "DSAN 6000",
    "section": "Assignment",
    "text": "Assignment\nGitHub Classroom Link",
    "crumbs": [
      "Content",
      "Course content",
      "1: Course overview"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "content/02-content.html",
    "href": "content/02-content.html",
    "title": "DSAN 6000",
    "section": "",
    "text": "Introduction to cloud services to be used throughout the semester.",
    "crumbs": [
      "Content",
      "Course content",
      "2: Cloud services"
    ]
  },
  {
    "objectID": "content/02-content.html#about-todays-class",
    "href": "content/02-content.html#about-todays-class",
    "title": "DSAN 6000",
    "section": "",
    "text": "Introduction to cloud services to be used throughout the semester.",
    "crumbs": [
      "Content",
      "Course content",
      "2: Cloud services"
    ]
  },
  {
    "objectID": "content/02-content.html#readings",
    "href": "content/02-content.html#readings",
    "title": "DSAN 6000",
    "section": "Readings",
    "text": "Readings\nReadings for this lecture (to be completed before this class):\n- Laberis - What is Cloud\n- Rittinghouse - The Evolution of Cloud",
    "crumbs": [
      "Content",
      "Course content",
      "2: Cloud services"
    ]
  },
  {
    "objectID": "content/02-content.html#slides",
    "href": "content/02-content.html#slides",
    "title": "DSAN 6000",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys.",
    "crumbs": [
      "Content",
      "Course content",
      "2: Cloud services"
    ]
  },
  {
    "objectID": "content/02-content.html#lab",
    "href": "content/02-content.html#lab",
    "title": "DSAN 6000",
    "section": "Lab",
    "text": "Lab\nAccessing cloud services, starting and connecting to a virtual machine, S3 commands. The lab for today’s lesson is available online as an HTML file.",
    "crumbs": [
      "Content",
      "Course content",
      "2: Cloud services"
    ]
  },
  {
    "objectID": "content/02-content.html#assignment",
    "href": "content/02-content.html#assignment",
    "title": "DSAN 6000",
    "section": "Assignment",
    "text": "Assignment\nGitHub Classroom Link",
    "crumbs": [
      "Content",
      "Course content",
      "2: Cloud services"
    ]
  },
  {
    "objectID": "content/04-content.html",
    "href": "content/04-content.html",
    "title": "DSAN 6000",
    "section": "",
    "text": "Scaling out: MapReduce, Hadoop, distributed filesystems, Hadoop Streaming.",
    "crumbs": [
      "Content",
      "Course content",
      "4: DuckDB, Polars and file formats"
    ]
  },
  {
    "objectID": "content/04-content.html#about-todays-class",
    "href": "content/04-content.html#about-todays-class",
    "title": "DSAN 6000",
    "section": "",
    "text": "Scaling out: MapReduce, Hadoop, distributed filesystems, Hadoop Streaming.",
    "crumbs": [
      "Content",
      "Course content",
      "4: DuckDB, Polars and file formats"
    ]
  },
  {
    "objectID": "content/04-content.html#readings",
    "href": "content/04-content.html#readings",
    "title": "DSAN 6000",
    "section": "Readings",
    "text": "Readings\nReadings for this lecture (to be completed before this class):\n- Wolohan Ch.7\n- Ghemawat et.al - The Google File System\n- Dean, Ghemawat - MapReduce",
    "crumbs": [
      "Content",
      "Course content",
      "4: DuckDB, Polars and file formats"
    ]
  },
  {
    "objectID": "content/04-content.html#slides",
    "href": "content/04-content.html#slides",
    "title": "DSAN 6000",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys.",
    "crumbs": [
      "Content",
      "Course content",
      "4: DuckDB, Polars and file formats"
    ]
  },
  {
    "objectID": "content/04-content.html#lab",
    "href": "content/04-content.html#lab",
    "title": "DSAN 6000",
    "section": "Lab",
    "text": "Lab\nStarting a cluster, running a Hadoop job with EMR on AWS. The lab for today’s lesson is available online as an HTML file.",
    "crumbs": [
      "Content",
      "Course content",
      "4: DuckDB, Polars and file formats"
    ]
  },
  {
    "objectID": "content/04-content.html#assignment",
    "href": "content/04-content.html#assignment",
    "title": "DSAN 6000",
    "section": "Assignment",
    "text": "Assignment\nGitHub Classroom Link",
    "crumbs": [
      "Content",
      "Course content",
      "4: DuckDB, Polars and file formats"
    ]
  },
  {
    "objectID": "content/06-content.html",
    "href": "content/06-content.html",
    "title": "DSAN 6000",
    "section": "",
    "text": "Spark 1: Introduction to Spark, Spark RDDs.",
    "crumbs": [
      "Content",
      "Course content",
      "6: Intro to Apache Spark"
    ]
  },
  {
    "objectID": "content/06-content.html#about-todays-class",
    "href": "content/06-content.html#about-todays-class",
    "title": "DSAN 6000",
    "section": "",
    "text": "Spark 1: Introduction to Spark, Spark RDDs.",
    "crumbs": [
      "Content",
      "Course content",
      "6: Intro to Apache Spark"
    ]
  },
  {
    "objectID": "content/06-content.html#readings",
    "href": "content/06-content.html#readings",
    "title": "DSAN 6000",
    "section": "Readings",
    "text": "Readings\nReadings for this lecture (to be completed before this class):\n- Damji et.al Ch.1,2,3",
    "crumbs": [
      "Content",
      "Course content",
      "6: Intro to Apache Spark"
    ]
  },
  {
    "objectID": "content/06-content.html#slides",
    "href": "content/06-content.html#slides",
    "title": "DSAN 6000",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys.",
    "crumbs": [
      "Content",
      "Course content",
      "6: Intro to Apache Spark"
    ]
  },
  {
    "objectID": "content/06-content.html#lab",
    "href": "content/06-content.html#lab",
    "title": "DSAN 6000",
    "section": "Lab",
    "text": "Lab\nThe lab for today’s lesson is available online as an HTML file.",
    "crumbs": [
      "Content",
      "Course content",
      "6: Intro to Apache Spark"
    ]
  },
  {
    "objectID": "content/06-content.html#assignment",
    "href": "content/06-content.html#assignment",
    "title": "DSAN 6000",
    "section": "Assignment",
    "text": "Assignment\nGitHub Classroom Link",
    "crumbs": [
      "Content",
      "Course content",
      "6: Intro to Apache Spark"
    ]
  },
  {
    "objectID": "content/08-content.html",
    "href": "content/08-content.html",
    "title": "DSAN 6000",
    "section": "",
    "text": "Spark 3: Machine Learning with SparkML.",
    "crumbs": [
      "Content",
      "Course content",
      "8: Spark ML"
    ]
  },
  {
    "objectID": "content/08-content.html#about-todays-class",
    "href": "content/08-content.html#about-todays-class",
    "title": "DSAN 6000",
    "section": "",
    "text": "Spark 3: Machine Learning with SparkML.",
    "crumbs": [
      "Content",
      "Course content",
      "8: Spark ML"
    ]
  },
  {
    "objectID": "content/08-content.html#readings",
    "href": "content/08-content.html#readings",
    "title": "DSAN 6000",
    "section": "Readings",
    "text": "Readings\nNo prescribed readings for this lecture.",
    "crumbs": [
      "Content",
      "Course content",
      "8: Spark ML"
    ]
  },
  {
    "objectID": "content/08-content.html#slides",
    "href": "content/08-content.html#slides",
    "title": "DSAN 6000",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys.",
    "crumbs": [
      "Content",
      "Course content",
      "8: Spark ML"
    ]
  },
  {
    "objectID": "content/08-content.html#lab",
    "href": "content/08-content.html#lab",
    "title": "DSAN 6000",
    "section": "Lab",
    "text": "Lab\nThe lab for today’s lesson is available online as an HTML file.",
    "crumbs": [
      "Content",
      "Course content",
      "8: Spark ML"
    ]
  },
  {
    "objectID": "content/08-content.html#assignment",
    "href": "content/08-content.html#assignment",
    "title": "DSAN 6000",
    "section": "Assignment",
    "text": "Assignment\nNo assignment for this class.",
    "crumbs": [
      "Content",
      "Course content",
      "8: Spark ML"
    ]
  },
  {
    "objectID": "content/10-content.html",
    "href": "content/10-content.html",
    "title": "DSAN 6000",
    "section": "",
    "text": "Spark 5: Spark Streaming.",
    "crumbs": [
      "Content",
      "Course content",
      "10: Spark Streaming"
    ]
  },
  {
    "objectID": "content/10-content.html#about-todays-class",
    "href": "content/10-content.html#about-todays-class",
    "title": "DSAN 6000",
    "section": "",
    "text": "Spark 5: Spark Streaming.",
    "crumbs": [
      "Content",
      "Course content",
      "10: Spark Streaming"
    ]
  },
  {
    "objectID": "content/10-content.html#readings",
    "href": "content/10-content.html#readings",
    "title": "DSAN 6000",
    "section": "Readings",
    "text": "Readings\nNo prescribed readings for this lecture.",
    "crumbs": [
      "Content",
      "Course content",
      "10: Spark Streaming"
    ]
  },
  {
    "objectID": "content/10-content.html#slides",
    "href": "content/10-content.html#slides",
    "title": "DSAN 6000",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys.",
    "crumbs": [
      "Content",
      "Course content",
      "10: Spark Streaming"
    ]
  },
  {
    "objectID": "content/10-content.html#lab",
    "href": "content/10-content.html#lab",
    "title": "DSAN 6000",
    "section": "Lab",
    "text": "Lab\nThe lab for today’s lesson is available online as an HTML file.",
    "crumbs": [
      "Content",
      "Course content",
      "10: Spark Streaming"
    ]
  },
  {
    "objectID": "content/10-content.html#assignment",
    "href": "content/10-content.html#assignment",
    "title": "DSAN 6000",
    "section": "Assignment",
    "text": "Assignment\nNo assignment for this class.",
    "crumbs": [
      "Content",
      "Course content",
      "10: Spark Streaming"
    ]
  },
  {
    "objectID": "content/12-content.html",
    "href": "content/12-content.html",
    "title": "DSAN 6000",
    "section": "",
    "text": "Data Engineering.",
    "crumbs": [
      "Content",
      "Course content",
      "12: Data engineering"
    ]
  },
  {
    "objectID": "content/12-content.html#about-todays-class",
    "href": "content/12-content.html#about-todays-class",
    "title": "DSAN 6000",
    "section": "",
    "text": "Data Engineering.",
    "crumbs": [
      "Content",
      "Course content",
      "12: Data engineering"
    ]
  },
  {
    "objectID": "content/12-content.html#readings",
    "href": "content/12-content.html#readings",
    "title": "DSAN 6000",
    "section": "Readings",
    "text": "Readings\nNo prescribed readings for this lecture.",
    "crumbs": [
      "Content",
      "Course content",
      "12: Data engineering"
    ]
  },
  {
    "objectID": "content/12-content.html#slides",
    "href": "content/12-content.html#slides",
    "title": "DSAN 6000",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys.",
    "crumbs": [
      "Content",
      "Course content",
      "12: Data engineering"
    ]
  },
  {
    "objectID": "content/12-content.html#lab",
    "href": "content/12-content.html#lab",
    "title": "DSAN 6000",
    "section": "Lab",
    "text": "Lab\nThe lab for today’s lesson is available online as an HTML file.",
    "crumbs": [
      "Content",
      "Course content",
      "12: Data engineering"
    ]
  },
  {
    "objectID": "content/12-content.html#assignment",
    "href": "content/12-content.html#assignment",
    "title": "DSAN 6000",
    "section": "Assignment",
    "text": "Assignment\nNo assignment for this class.",
    "crumbs": [
      "Content",
      "Course content",
      "12: Data engineering"
    ]
  },
  {
    "objectID": "content/14-content.html",
    "href": "content/14-content.html",
    "title": "DSAN 6000",
    "section": "",
    "text": "Project discussion and open session.",
    "crumbs": [
      "Content",
      "Course content",
      "14: Feature Store, Vector DB & Misc."
    ]
  },
  {
    "objectID": "content/14-content.html#about-todays-class",
    "href": "content/14-content.html#about-todays-class",
    "title": "DSAN 6000",
    "section": "",
    "text": "Project discussion and open session.",
    "crumbs": [
      "Content",
      "Course content",
      "14: Feature Store, Vector DB & Misc."
    ]
  },
  {
    "objectID": "content/14-content.html#readings",
    "href": "content/14-content.html#readings",
    "title": "DSAN 6000",
    "section": "Readings",
    "text": "Readings\nNo prescribed readings for this lecture, but you should have completed Lab 0 - Background Skills before this class.",
    "crumbs": [
      "Content",
      "Course content",
      "14: Feature Store, Vector DB & Misc."
    ]
  },
  {
    "objectID": "content/14-content.html#slides",
    "href": "content/14-content.html#slides",
    "title": "DSAN 6000",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys.",
    "crumbs": [
      "Content",
      "Course content",
      "14: Feature Store, Vector DB & Misc."
    ]
  },
  {
    "objectID": "content/14-content.html#lab",
    "href": "content/14-content.html#lab",
    "title": "DSAN 6000",
    "section": "Lab",
    "text": "Lab\nThe lab for today’s lesson is available online as an HTML file.",
    "crumbs": [
      "Content",
      "Course content",
      "14: Feature Store, Vector DB & Misc."
    ]
  },
  {
    "objectID": "content/14-content.html#assignment",
    "href": "content/14-content.html#assignment",
    "title": "DSAN 6000",
    "section": "Assignment",
    "text": "Assignment\nNo assignment for this class.",
    "crumbs": [
      "Content",
      "Course content",
      "14: Feature Store, Vector DB & Misc."
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Big Data Analytics & Cloud Computing\n        ",
    "section": "",
    "text": "“Learn how to analyze Big Data using cloud computing and technologies such as Concurrency, Spark and others in this hands-on, practical workshop-style course.”\n        \n        \n            DSAN 6000 • Fall 2025Amit Arora, Jeff JacobsGraduate School of Arts and Science, Georgetown University\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\n\n\nInstructors\n\n   Amit Arora\n   Jeff Jacobs\n   Online\n   Schedule an appointment\n\n\n\nCourse details\n\n   Wednesdays (Prof Amit), Tuesdays (Prof Jeff)\n   August 27 – December 20, 2025\n   9:30 AM–12:00 PM\n   Reiss 262 (W, Amit), TBD (Tu, Jeff)\n   Slack (Join)\n\n\n\nContacting us\nE-mail and Slack are the best ways to get in contact with me. I will try to respond to all course-related e-mails and Slack messages within 24 hours (really), but also remember that life can be busy and chaotic for everyone (including me!), so if I don’t respond right away, don’t worry!"
  },
  {
    "objectID": "labs/02-labs.html",
    "href": "labs/02-labs.html",
    "title": "Lab 2 Amazon Web Services (AWS) Setup Instructions",
    "section": "",
    "text": "DSAN 6000 Fall 2025\nFollow these instructions step-by-step to setup your AWS environment. The screenshots may look a bit different than what you are seeing, but the flow is the same.",
    "crumbs": [
      "Labs",
      "Labs content",
      "2: Cloud services"
    ]
  },
  {
    "objectID": "labs/02-labs.html#login-into-the-aws-console",
    "href": "labs/02-labs.html#login-into-the-aws-console",
    "title": "Lab 2 Amazon Web Services (AWS) Setup Instructions",
    "section": "Login into the AWS Console",
    "text": "Login into the AWS Console\nThe AWS Console is your entry point into the AWS cloud.\n\nClick on the AWS link alongside the ⬤. #\nA new tab will open in your browser, this is the AWS Console. #\nNote the URL in your browser’s address bar, it will start with the name of the AWS region (such as us-east-1) in which your cloud resources are hosted.\nNote the username on the top right hand corner, this is your Federated Identity. Also note that the you did not have to provide any credentials (username/password) to login into the AWS console. How did this happen? #\n\n\nLogging into the AWS Console at a later time\nTo access the AWS Console in the future, login to https://www.awsacademy.com/LMS_Login, go to Learner Lab -&gt; Modules -&gt; Start Lab.\n\n\n\n\n\n\nNote\n\n\n\nIf you already had an AWS account prior to logging into AWS Academy you would need to login into the AWS Educate AWS account via an Incognito Browser Window.",
    "crumbs": [
      "Labs",
      "Labs content",
      "2: Cloud services"
    ]
  },
  {
    "objectID": "labs/02-labs.html#part-3-creating-the-user",
    "href": "labs/02-labs.html#part-3-creating-the-user",
    "title": "Lab 2 Amazon Web Services (AWS) Setup Instructions",
    "section": "Part-3: Creating the User",
    "text": "Part-3: Creating the User\n\nNow that you have created the domain, It is time to create a user in that domain. Easiest way to do this is to navigate to the Studio page on the sidebar and Clicking “Create user profile” \nName your user, as a matter of convention set the username as your NET ID. Click on next.\n\n\n\nFollow the screenshot.\n\n\n\nDisable all canvas permissions and Paste the ARN of your LabRole. Scroll Down\n\n\n\nClick on Next \nScroll down \nScroll down, click on next \nUser should be created now.",
    "crumbs": [
      "Labs",
      "Labs content",
      "2: Cloud services"
    ]
  },
  {
    "objectID": "labs/02-labs.html#launching-sagemaker-studio",
    "href": "labs/02-labs.html#launching-sagemaker-studio",
    "title": "Lab 2 Amazon Web Services (AWS) Setup Instructions",
    "section": "Launching Sagemaker Studio",
    "text": "Launching Sagemaker Studio\n\nClick on Domains on the sidebar. Select your domain and click on User Profiles.\n\n\n\nSelect launch and Click on Studio\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is called a managed service, you get to work on the notebook without having to deal with the creation steps for the underlying infrastructure. You can easily change this configuration as per the requirements of the analytics/machine learning job you are running.\n\n\n\nThis should open up\n\n\n\nSelect JupyterLab and Click on Create JupyterLab Space \nA Jupyter Notebook session should open up",
    "crumbs": [
      "Labs",
      "Labs content",
      "2: Cloud services"
    ]
  },
  {
    "objectID": "labs/02-labs.html#shutting-down-sagemaker-spaces",
    "href": "labs/02-labs.html#shutting-down-sagemaker-spaces",
    "title": "Lab 2 Amazon Web Services (AWS) Setup Instructions",
    "section": "Shutting down SageMaker Spaces",
    "text": "Shutting down SageMaker Spaces\nIt is important to shutdown SageMaker Studio when not in use so that you do not get billed for it when you are not using it.\n\nNavigate back to Sagemaker Studio, Select Jupyter Lab, Click on Stop \n\n\n\n\n\n\n\nImportant\n\n\n\nAt the end of this lab:\n\nMake sure you shutdown SageMaker Studio.\nMake sure you shutdown EC2 VM.\nMake sure you have ended the lab in AWS Educate by pressing the End Lab button.",
    "crumbs": [
      "Labs",
      "Labs content",
      "2: Cloud services"
    ]
  },
  {
    "objectID": "labs/02-labs.html#optional-using-vscode-instead-of-jupyter-notebooks",
    "href": "labs/02-labs.html#optional-using-vscode-instead-of-jupyter-notebooks",
    "title": "Lab 2 Amazon Web Services (AWS) Setup Instructions",
    "section": "(Optional) Using VSCode Instead of Jupyter Notebooks",
    "text": "(Optional) Using VSCode Instead of Jupyter Notebooks\nSageMaker Studio Has added support for VSCode and the steps to run it are below:\n\nNavigate back to Sagemaker Studio, Select Code Editor, Click on Create Code Editor Space \nFollow the screenshot and click on Create Space \nNow select the instance type you want to use. Click on Run Space. Wait for a while then click on Open Code Editor.\n\n\n\nNow you can start using VS Code as you do on your local machine.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis service also has to be shutdown as jupyter notebook. Since you only have 50\\(, not shutting a notebook down, can cost upto 20\\) a day for instances such as ml.g4dn.xlarge. It is very important to shutdown the space after working on it. If you spend 50$ on this account, YOU WILL LOSE ALL DATA. NO, you cannot recover the data after the account is shutdown. Therefore it is recommended to make regular commits to github as you work on your assignments or labs.\n\n\n\nTo stop Code Editor Space, Go back to Sagemaker Studio, Click on Code editor.",
    "crumbs": [
      "Labs",
      "Labs content",
      "2: Cloud services"
    ]
  },
  {
    "objectID": "labs/02-labs.html#github-classroom",
    "href": "labs/02-labs.html#github-classroom",
    "title": "Lab 2 Amazon Web Services (AWS) Setup Instructions",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link",
    "crumbs": [
      "Labs",
      "Labs content",
      "2: Cloud services"
    ]
  },
  {
    "objectID": "labs/04-labs.html",
    "href": "labs/04-labs.html",
    "title": "Lab 4",
    "section": "",
    "text": "DSAN 6000 Fall 2025",
    "crumbs": [
      "Labs",
      "Labs content",
      "4: DuckDB, Polars and file formats"
    ]
  },
  {
    "objectID": "labs/04-labs.html#github-classroom",
    "href": "labs/04-labs.html#github-classroom",
    "title": "Lab 4",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link",
    "crumbs": [
      "Labs",
      "Labs content",
      "4: DuckDB, Polars and file formats"
    ]
  },
  {
    "objectID": "labs/06-labs.html",
    "href": "labs/06-labs.html",
    "title": "Lab 6",
    "section": "",
    "text": "DSAN 6000 Fall 2025",
    "crumbs": [
      "Labs",
      "Labs content",
      "6: Intro to Apache Spark"
    ]
  },
  {
    "objectID": "labs/06-labs.html#github-classroom",
    "href": "labs/06-labs.html#github-classroom",
    "title": "Lab 6",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link",
    "crumbs": [
      "Labs",
      "Labs content",
      "6: Intro to Apache Spark"
    ]
  },
  {
    "objectID": "labs/08-labs.html",
    "href": "labs/08-labs.html",
    "title": "Lab 8",
    "section": "",
    "text": "DSAN 6000 Fall 2025",
    "crumbs": [
      "Labs",
      "Labs content",
      "8: Spark ML"
    ]
  },
  {
    "objectID": "labs/08-labs.html#github-classroom",
    "href": "labs/08-labs.html#github-classroom",
    "title": "Lab 8",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link",
    "crumbs": [
      "Labs",
      "Labs content",
      "8: Spark ML"
    ]
  },
  {
    "objectID": "labs/10-labs.html",
    "href": "labs/10-labs.html",
    "title": "Lab 10",
    "section": "",
    "text": "DSAN 6000 Fall 2025",
    "crumbs": [
      "Labs",
      "Labs content",
      "10: Spark Streaming"
    ]
  },
  {
    "objectID": "labs/10-labs.html#github-classroom",
    "href": "labs/10-labs.html#github-classroom",
    "title": "Lab 10",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link",
    "crumbs": [
      "Labs",
      "Labs content",
      "10: Spark Streaming"
    ]
  },
  {
    "objectID": "labs/13-labs.html",
    "href": "labs/13-labs.html",
    "title": "Lab 13",
    "section": "",
    "text": "DSAN 6000 Fall 2025",
    "crumbs": [
      "Labs",
      "Labs content",
      "13: Ray, RAPIDS & DuckDB"
    ]
  },
  {
    "objectID": "labs/13-labs.html#github-classroom",
    "href": "labs/13-labs.html#github-classroom",
    "title": "Lab 13",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link - Part 1",
    "crumbs": [
      "Labs",
      "Labs content",
      "13: Ray, RAPIDS & DuckDB"
    ]
  },
  {
    "objectID": "labs/login-to-saxanet.html",
    "href": "labs/login-to-saxanet.html",
    "title": "DSAN 6000",
    "section": "",
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet. #"
  },
  {
    "objectID": "project/01-project.html",
    "href": "project/01-project.html",
    "title": "Milestone 1: Frame your analysis and EDA",
    "section": "",
    "text": "You should thoroughly read through the entire assignment before beginning your work! Don’t start costly cloud resources until you are ready.\nCode and data saved locally on the cluster will be lost when the cluster terminates. If you want to keep data, you must store it in a blob storage location. You must store the results (i.e. any plots, csv files) in the data folder of this repo.",
    "crumbs": [
      "Project contents",
      "1: EDA"
    ]
  },
  {
    "objectID": "project/01-project.html#first-time-setup",
    "href": "project/01-project.html#first-time-setup",
    "title": "Milestone 1: Frame your analysis and EDA",
    "section": "First time setup",
    "text": "First time setup\nTHIS SECTION IS IN PROGRESS AS INFRASTRUCTURE IS BEING FINALIZED\n\nIf all of the following steps are successful then it means your environment is set up correctly and you are all set to begin your project.\n\nSet up your git credentials.\nCreate a shared cluster for your team.\nClone this repo.\nCreate a notebook, call it project_eda.\nFollow the example notebook steps to interact with the data in your preferred computing environment.\nCheck the changed files (this would be the project_eda notebook) in the repo.\nExport the project_eda notebook to IPython Notebook and check in the project_eda.ipynb file in the repo.",
    "crumbs": [
      "Project contents",
      "1: EDA"
    ]
  },
  {
    "objectID": "project/01-project.html#submission-details",
    "href": "project/01-project.html#submission-details",
    "title": "Milestone 1: Frame your analysis and EDA",
    "section": "Submission Details",
    "text": "Submission Details\nIn this assignment, you will start working with your Reddit data, decide on the scope of your project, and outline your 10 business goals.\nYou will use the file project_starter_script.py to get your Reddit data from Azure Blob Storage, note that you do not need to copy the data to your local environment. The project_starter_script.py notebook also contains sample code for a basic EDA example and saves the results in the local repo so that it can be checked in. It also has an example of saving intermediate big data.\nYou will develop EDA notebook(s) using PySpark. All your big data analysis must be in PySpark. In this assignment, you will examine the dataset, make transformations of the data, produce summary statistics and graphs of the data, and answer some of your business goals that only require exploratory work. You may choose to put all your work into one notebook or you may choose to separate it. Either is fine!\n\n\n\n\n\n\nOutput data sizes\n\n\n\nREMEMBER!!! All the output you are making MUST MUST MUST be small data. Can you make a graph of all 1 million+ rows of spark data? NO! You MUST MUST MUST take that big data and collapse it into reasonable data to put into a table or a graph. You should never collect more than ~10,000 rows back to you.\n\n\n\nMinimum requirements:\n\nMake a project plan for your Reddit data with 10 topics that you plan on exploring.\nPropose 10 different avenues of analysis for your data.\nAny good data science project can be broken into at least 10 topics. These topics should vary in complexity to include exploratory (2-3), NLP (3-5), and ML (3-5) ideas. Each entry of your 10 must include the “business goal” as well as the “technical proposal” for finding the answers. We want to see the “Executive Summary” view of the questions as well as the “Data Science” plans for making it happen. The business goal cannot have any technical language and must be accessible to an audience without a data science background.\n\nExample question based on the data science subreddit https://www.reddit.com/r/datascience/Links to an external site.:\n\n\nBusiness goal: Determine the most popular programming languages and the most effective programming languages used to conduct geospatial data analysis.\n\n\nTechnical proposal: Use NLP to identify posts that mention geospatial terms and one or more programming languages. Conduct counts of which programming languages are mentioned the most along with these geospatial terms. Analyze counts over time to check for major deviations and identify the leaders. Conduct sentiment analysis of the posts to assign positive or negative values to programming languages. Present findings for volume metrics and sentiment analysis for the top 5 programming languages to answer the “popular” and “effective” insights for geospatial analysis.\n\nEach business goal must be 1-2 sentences while each technical proposal must be at least 3 sentences. There must be enough details about your plans so you can get feedback. Include these business requirements in a markdown cell in the project_eda notebook.\nConduct your exploratory data analysis.\n\n\nReport on the basic info about your dataset. What are the interesting columns? What is the schema? How many rows do you have? etc. etc.\nConduct basic data quality checks! Make sure there are no missing values, check the length of the comments, and remove rows of data that might be corrupted. Even if you think all your data is perfect, you still need to demonstrate that with your analysis.\nProduce at least 5 interesting graphs about your dataset. Think about the dimensions that are interesting for your Reddit data! There are millions of choices. Make sure your graphs are connected to your business questions.\nProduce at least 3 interesting summary tables about your dataset. You can decide how to split up your data into categories, time slices, etc. There are infinite ways you can make summary statistics. Be unique, creative, and interesting!\nUse data transformations to make AT LEAST 3 new variables that are relevant to your business questions. We cannot be more specific because this depends on your project and what you want to explore!\nImplement regex searches for specific keywords of interest to produce dummy variables and then make statistics that are related to your business questions. Note, that you DO NOT have to do textual cleaning of the data at this point. The next assignment on NLP will focus on the textual cleaning and analysis aspect.\nFind some type of external data to join onto your Reddit data. Don’t know what to pick? Consider a time-related dataset. Stock prices, game details over time, active users on a platform, sports scores, covid cases, etc., etc. While you may not need to join this external data with your entire dataset, you must have at least one analysis that connects to external data. You do not have to join the external data and analyze it yet, just find it.\nIf you are planning to make any custom datasets that are derived from your Reddit data, make them now. These datasets might be graph-focused, or maybe they are time series focused, it is completely up to you!\n\nCreate a website using your project_eda notebook. This could be as simple as publishing your notebook on GitHub pages.\n\nHere is a simple example that describes how to publish a notebook to GitHub.\nWhen you publish to GitHub, it will give you a warning that this repo is private but the published website would be Public, which is OK.\n\nWe expect you to put significant effort into this assignment. This assignment only requires the Jupyter notebooks.",
    "crumbs": [
      "Project contents",
      "1: EDA"
    ]
  },
  {
    "objectID": "project/01-project.html#submitting-the-assignment",
    "href": "project/01-project.html#submitting-the-assignment",
    "title": "Milestone 1: Frame your analysis and EDA",
    "section": "Submitting the Assignment",
    "text": "Submitting the Assignment\nYou will follow the submission process for all labs and assignments:\n\nCommit and push your files\nExport any of the notebooks you created as IPython and/or html files, and add them to your repository from your local machine before you tag the release\nTag the submission release commit with the v0.1-eda tag by the due date\nMake sure to push to GitHub\nSPECIFIC TO THIS DELIVERABLE - submit the URL for your public-facing website to the assignment on Canvas.\n\nMake sure you commit only the files requested, and push your repository to GitHub!\nThe files to be committed and pushed to the repository for this assignment are:\n\nREADME.md\n.gitignore\nLICENSE\ncode/project_starter_script.py\ncode/project_eda.ipynb\nimg/*\ndata/*\n\nMake sure that your project_eda notebook includes both a list of the business problems you are solving and the charts and summary tables as described above in the minimum requirements section.",
    "crumbs": [
      "Project contents",
      "1: EDA"
    ]
  },
  {
    "objectID": "project/01-project.html#grading-rubric",
    "href": "project/01-project.html#grading-rubric",
    "title": "Milestone 1: Frame your analysis and EDA",
    "section": "Grading Rubric",
    "text": "Grading Rubric\nMany of the assignments you will work on are open-ended. Grading is generally holistic, meaning that there will not always be specific point value for individual elements of a deliverable. Each deliverable submission is unique and will be compared to all other submissions.\n\nIf a deliverable exceeds the requirements and expectations, that is considered A level work.\nIf a deliverable just meets the requirements and expectations, that is considered A-/B+ level work.\nIf a deliverable does not meet the requirements, that is considered B or lesser level work.\n\nAll deliverables must meet the following general requirements, in addition to the specific requirements of each deliverable:\nIf your submission meets or exceeds the requirements, is creative, is well thought-out, has proper presentation and grammar, and is at the graduate student level, then the submission will get full credit. Otherwise, partial credit will be given and deductions may be made for any of the following reasons:\nPoints will be deducted for any of the following reasons:\n\nAny instruction is not followed\nThere are missing sections of the deliverable\nThe overall presentation and/or writing is sloppy\nThere are no comments in your code\nThere are files in the repository other than those requested\nThere are absolute filename links in your code\nThe repository structure is altered in any way\nFiles are named incorrectly (wrong extensions, wrong case, etc.)",
    "crumbs": [
      "Project contents",
      "1: EDA"
    ]
  },
  {
    "objectID": "project/03-project.html",
    "href": "project/03-project.html",
    "title": "Peer Feedback Milestone: Get and receive feedback on your EDA Milestone",
    "section": "",
    "text": "Monday Feb 26, 2024 at 8:10 am\nIMPORTANT: The first step in the feedback process is submitting your work to be reviewed. This MUST be completed as part of your EDA deliverable by submitting the URL of your public-facing website. Submitting the URL is part of EDA and must be completed by the EDA Milestone deadline.\nIn this deliverable, your group will collectively provide feedback to another group’s EDA deliverable. This will consist of a shared document (such as Google Doc) that is exported and submitted to the other team and Canvas.",
    "crumbs": [
      "Project contents",
      "3: Peer feedback"
    ]
  },
  {
    "objectID": "project/03-project.html#table-of-contents",
    "href": "project/03-project.html#table-of-contents",
    "title": "Peer Feedback Milestone: Get and receive feedback on your EDA Milestone",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nWhat to Look for in the EDA Deliverable\nRequired Content and Structure of Feedback\nSubmission Details",
    "crumbs": [
      "Project contents",
      "3: Peer feedback"
    ]
  },
  {
    "objectID": "project/03-project.html#what-to-look-for-in-the-eda-deliverable",
    "href": "project/03-project.html#what-to-look-for-in-the-eda-deliverable",
    "title": "Peer Feedback Milestone: Get and receive feedback on your EDA Milestone",
    "section": "What to Look for in the EDA Deliverable",
    "text": "What to Look for in the EDA Deliverable\nIn preparation for your review of the other group, review the EDA deliverable requirements.\nYou will conduct a broad review of the project, including the topic, the goals, the technical proposals, and the initial EDA work.\n\nComment on the overall topic and its relevance and achievability\nReview the 10 planned goals for the project\n\nAre the complex enough? Are they too easy?\nHow well connected are the planned pieces of analysis?\nEnsure that business goals have no technical language and are compelling for a non-technical audience\nAre the technical proposals reasonable?\nIs there a discussion about the external dataset that will be used? Does it make sense to include this dataset given the project goals?\n\nEDA work\n\nDo the basic data quality checks make sense? Is there anything missing?\nAre there 5 graphs and do they fit well with the project goals?\nAre there 3 tables and do they connect to the project goals?\nAre there three data transformations and a regex search? Are these steps completed well?\n\nDeliverable level of excellence\n\nQuality of visualizations\nQuality of tables\nQuality of writing and grammar\nQuality of website. Is it well formatted? Are all the notebooks available to the audience?",
    "crumbs": [
      "Project contents",
      "3: Peer feedback"
    ]
  },
  {
    "objectID": "project/03-project.html#required-content-and-structure-of-feedback",
    "href": "project/03-project.html#required-content-and-structure-of-feedback",
    "title": "Peer Feedback Milestone: Get and receive feedback on your EDA Milestone",
    "section": "Required Content and Structure of Feedback",
    "text": "Required Content and Structure of Feedback\nYour Word document must have the following sections. These sections are based on the description of the requirements for the deliverable in the prior section. You must write in full sentences and explain your feedback. This feedback is meant to improve the project!\n\nOverall impressions\nProject plan\nEDA work\nQuality of content\nLessons learned\n\n\nOverall impressions\nProvide your feedback on the overall project and any comments on how to improve the success of this group. Provide both constructive criticism and positive feedback.\n\n\nProject plan\nWalk through each goal and provide feedback on the goals and technical proposals.\n\n\nEDA work\nHow can the group improve their analysis? What would you like to see that is not already there?\n\n\nQuality of content\nIf there are any issues with the quality of the website/writing/tables/visualization, outline them here. Ensure you are clear on what element you are providing feedback on.\n\n\nLessons learned\nClose with positive feedback for the group. What did you learn from reviewing their project? Are there styles of analysis or topics that inspired you to do something similar in your project?",
    "crumbs": [
      "Project contents",
      "3: Peer feedback"
    ]
  },
  {
    "objectID": "project/03-project.html#submission-details",
    "href": "project/03-project.html#submission-details",
    "title": "Peer Feedback Milestone: Get and receive feedback on your EDA Milestone",
    "section": "Submission Details",
    "text": "Submission Details\n\nWrite the feedback document in a shared space according to the required specifications. Ensure that your whole group agrees with and contributes to the content.\nEmail all members of the group you are reviewing a copy of your feedback document, cc’ing the instructor email dsan6000-instructors@georgetown.edu. Get the group members from the announcement of groups here\nSubmit the Word document to Canvas in this assignment\n\nALL ACTIONS MUST BE COMPLETED BY THE DUE DATE. NO EXCEPTIONS.",
    "crumbs": [
      "Project contents",
      "3: Peer feedback"
    ]
  },
  {
    "objectID": "project/03-project.html#next-steps",
    "href": "project/03-project.html#next-steps",
    "title": "Peer Feedback Milestone: Get and receive feedback on your EDA Milestone",
    "section": "Next Steps",
    "text": "Next Steps\n\nAcknowledge the feedback you received from your reviewer group.\nMeet with your group on how you plan to respond to the feedback and incorporate it into your project.\nFuture deliverables will require you to adopt and/or respond to the feedback received from this peer feedback milestone and the instructors.",
    "crumbs": [
      "Project contents",
      "3: Peer feedback"
    ]
  },
  {
    "objectID": "project/05-project.html#table-of-contents",
    "href": "project/05-project.html#table-of-contents",
    "title": "Milestone 4: Final Submission",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nMilestone 4: Final Deliverable\n\nTable of Contents\nSetup\nMilestone description\nAnalysis Requirements\nFinal milestone web page requirements\n\nReviewing and editing your existing website\nIntroduction page\nNLP and ML pages\nDiscussion page\nConclusion page\nEDA page\n\nExecutive summary\nAnalysis report\n\n\nSubmission instructions",
    "crumbs": [
      "Project contents",
      "5: Final"
    ]
  },
  {
    "objectID": "project/05-project.html#setup",
    "href": "project/05-project.html#setup",
    "title": "Milestone 4: Final Submission",
    "section": "Setup",
    "text": "Setup\nSince you are making updates to your code you do not need to make any new subdirectories. Be sure to indicate on your website or in your Git repo which parts of your project have been updated.",
    "crumbs": [
      "Project contents",
      "5: Final"
    ]
  },
  {
    "objectID": "project/05-project.html#milestone-description",
    "href": "project/05-project.html#milestone-description",
    "title": "Milestone 4: Final Submission",
    "section": "Milestone description",
    "text": "Milestone description\nIn this assignment you will complete your project, making sure that it is ready for primetime. This means that your project website has been improved since your intermediate assignments and will wow any future employers.\nYou will populate the following pages: - docs/index.html - docs/conclusion.html - docs/discussion.html\nYou will update and improve the following pages: - docs/eda.html - docs/nlp.html - docs/ml.html\nRemember, all your big data analysis must be in PySpark unless an exception has been made. You may choose to put all your work into one notebook or you may choose to keep separate notebooks for your work. It might be the case that you have to do very little additional coding for your final deliverable. This really depends on the status of your existing project work and how much more effort you have to put into building it out to be great! More refinement is always better! Iteration always helps! Keep improving the project!",
    "crumbs": [
      "Project contents",
      "5: Final"
    ]
  },
  {
    "objectID": "project/05-project.html#analysis-requirements",
    "href": "project/05-project.html#analysis-requirements",
    "title": "Milestone 4: Final Submission",
    "section": "Analysis Requirements",
    "text": "Analysis Requirements\n\nExpand on your analysis from the intermediate assignments in some manner. Do you add an additional data transformation? Try one more model? Try a different NLP text method? It does not have to be groundbreaking at this point, but think about how you can make minor updates to improve the work further.\nMake sure to indicate in an appendix on your conclusion page what updates you made to your analysis since your intermediate deliverables.",
    "crumbs": [
      "Project contents",
      "5: Final"
    ]
  },
  {
    "objectID": "project/05-project.html#final-milestone-web-page-requirements",
    "href": "project/05-project.html#final-milestone-web-page-requirements",
    "title": "Milestone 4: Final Submission",
    "section": "Final milestone web page requirements",
    "text": "Final milestone web page requirements\n\nReviewing and editing your existing website\n\nSpell check, grammar check, read for clarity. Make sure all your pages are well-written and free of errors!\nSubmit all your notebooks used in the project to the Git repo. You must also have all your notebooks available on your website. Make sure you have these notebooks available in html format that is easily viewable by a web user. Points will be deducted for linking to ipynb files that must be downloaded.\nCan your audience access all of your code files?\nJust the ipynb is not a valid webpage. It is really hard to navigate. You need to save your outputs and create the webpages, suitable for a non-data scientist audience.\nA public website is required. Your team MUST have a self-contained website that can be opened locally upon download and navigable by any web browser. You can assume an available internet connection (in case you are calling JS libraries).\nIf you do choose to publish your website publicly, you must update the README to include its URL.\n\n\n\nIntroduction page\n\nYou will write up an introduction for your entire project of at least 4 text paragraphs and 2-5 informative graphs/images/tables.\nThis is a NON-TECHNICAL introduction. There should be no technical language, equations, etc. Talk about what the project, its goals, and why it matters!\nAll your analytical goals/questions and technical proposals must be listed out at the end as an appendix to your introduction.\nAdapt your project plan analytical goals/questions with any changes you have made to your project since you started.\nIt is OK if you did not get to all of your analytical goals/questions.\nIf you already wrote up your introduction as part of an earlier deliverable, great! Then that is less work for you now.\n\n\n\nNLP and ML pages\n\nYou made these pages in earlier deliverables. This means you only have to make updates and improvements to these two pages.\nImprove usability of your pages by fixing issues with html, placement, blurry graphics, etc.\nStrengthen the analytical justification of the decisions you made in your project. This text is critical so the audience understands why you followed the path of work that you did.\n\n\n\nFeedback discussion page\n\nYou must organize the feedback you have received on the project into the following categories:\n\nProject plans\nEDA work\nNLP work\nML work\nWebsite/results\n\nProvide bullet points summarizing each piece of meaningful feedback received from your instructors and your peers. After you outline the feedback received, you must write 1-3 sentences describing how you have incorporated the feedback into your project or explain why you have chosen not to.\n\n\n\nConclusion page\n\nYou will write up 3-5 text paragraphs with 2-5 informative graphs/images/tables that summarize and conclude on all the existing work you have done in the project.\nYou must have a section that discusses the planned next steps in the project. It does not mean that you will actually implement these next steps, but it is important to outline your vision for how you would move the project forward if you had another month to conduct your work.\nMake sure you are not repeating yourself from the methods pages.\nThe introduction and conclusion should serve as book ends to your project. If those pages are read without any of the middle pages, one should still be able to understand the purpose of the project, its impact, and your next steps.\n\n\n\nEDA page\nYour website/eda.html will be populated with the following sections:\n\nExecutive summary\n\nWrite 1-2 paragraphs on your EDA accomplishments. You can include up to 2 images or tables. Your writing must be excellent and persuasive. 3 sentences total will not cut it. The expectation is to write at least 8 sentences. This is the section to describe the high-level results and the most important information only! This summary must be NON-TECHNICAL! Think about how to touch on your business goals without going into much detail.\n\n\n\nAnalysis report\n\nWrite a data analysis style report of 4-6 text paragraphs (each paragraph at least 4 sentences). This is where you will discuss your data ingestion, data cleaning, and data evaluation. During that prose, you will present all your tables and figures. You can save the images or take snips from your Jupyter notebook. This part is for you to write up the analysis work you have already done. Be creative, be awesome!! Show off to a future employer your amazing data science skills!\n\nThe flow of the report is up to you. Maybe you want to split the text by business goal or organize the prose into a data journey story.\nYou must reference all the business goals that you have accomplished in this report. Did your technical proposal change as you started working with the data?\nIf you made changes to your business goals, discuss your analytical justification for changing your plans. It is OK that plans change, though it is important to describe why.\nYour visualizations must follow the visualization best practices you learned in ANLY 503. There should be titles, axis labels, legends as needed, etc.\n\n\nInclude links to all your EDA coding notebooks and sources for your external data so that your audience can look at your work.\n\nipynb files MUST show the requisite outputs, they will be penalized even if the code is present. We need to see the outputs from your coding cells in addition to your website.",
    "crumbs": [
      "Project contents",
      "5: Final"
    ]
  },
  {
    "objectID": "project/05-project.html#submission-instructions",
    "href": "project/05-project.html#submission-instructions",
    "title": "Milestone 4: Final Submission",
    "section": "Submission instructions",
    "text": "Submission instructions\n\nCommit and push your files\nExport any of the notebooks you created as IPython and/or html files, and add them to your repository from your local machine before you tag the release\nTag the submission release commit with the v1.0-final tag by the due date\nMake sure to push to GitHub",
    "crumbs": [
      "Project contents",
      "5: Final"
    ]
  },
  {
    "objectID": "project/index.html#the-reddit-archive-dataset",
    "href": "project/index.html#the-reddit-archive-dataset",
    "title": "Project",
    "section": "The Reddit Archive dataset",
    "text": "The Reddit Archive dataset\nYour team will work with a subset of the Reddit Archive data. This dataset used to be publicly available until Reddit made changes to its API terms of service. The time period that you will be working with is before the change in API terms.\nThe data you will use spans January 2022 to March 2023. There are two datasets that we are making available to you:\n\nsubmissions: 412 GB of plain-text json files representing 109 million entries\ncomments: 918 GB of plain-text json files representing 701 entries\n\nFor ease of use, both datasets have been pre-processed:\n\nThe number of original fields/columns was reduced\nThe original Unix timestamp values were converted to actual timestamps\nThe original text JSON files were saved as parquet\n\nThe available parquet dataset sizes are 14 GB for submissions and 95 GB for comments. For a sneak peek of the original data, you can download a sample.\nThis is a very rich dataset and it is up to you to decide how you are going to use it. You will select one or more topics of interest to explore and use the Reddit data and one or more external datasets to perform a meaningful analysis.\nWe recommend you read this paper which talks about the dataset, and you may want to look at several papers written about or using the Reddit data for inspiration.",
    "crumbs": [
      "Overview",
      "Project"
    ]
  },
  {
    "objectID": "project/index.html#tools",
    "href": "project/index.html#tools",
    "title": "Project",
    "section": "Tools",
    "text": "Tools\nYou will use a Serverless Spark Cluster in Azure or a Spark Processing Job in Sagemaker to analyze, transform, and process your data and create one or more analytical datasets. We will provide specific instructions on how to access the data in both Azure and AWS.\nDevelop your Spark code similar to how it has been discussed in class:\n\nIterate quickly in a local notebook\nScale up to a small cluster to ensure your code still works\nProductionalize with a larger cluster or remote processing job to run your full pipeline.\n\nSmaller derivative data sets can be processed locally if needed for plotting or reporting.",
    "crumbs": [
      "Overview",
      "Project"
    ]
  },
  {
    "objectID": "project/index.html#objectives",
    "href": "project/index.html#objectives",
    "title": "Project",
    "section": "Objectives",
    "text": "Objectives\nIn every project as data scientists, in addition to doing modeling and machine learning work, you will also be responsible (either individually or as part of a team) for providing the following as part of a project:\n\nFindings: what does the data say?\nConclusions: what is your interpretation of the data?\nRecommendations: what can be done to address the question/problem at hand\n\nYour analyses in this project will focus on the first two above. Your work must provide the audience with an understanding of the topic you are analyzing, presenting, and discussing. The objective is to find a topic of interest, work with the data, and present it to an audience that may not know very much about the subject using a data-driven approach.",
    "crumbs": [
      "Overview",
      "Project"
    ]
  },
  {
    "objectID": "project/index.html#milestones",
    "href": "project/index.html#milestones",
    "title": "Project",
    "section": "Milestones",
    "text": "Milestones\nThe project will be executed over several milestones, and each one has a specific set of requirements. There are four major milestones and a feedback deliverable (click on each one for the appropriate instructions and description):\n\nMilestone 1: Define the questions and Exploratory Data Analysis\nMilestone 2: NLP and external data overlay\nPeer Feedback: Give and receive peer feedback\nMilestone 3: Machine Learning\nMilestone 4: Final delivery [TO BE RELEASED SOON]\n\nAll of your work will be done within the team GitHub repository, and each milestone will be tagged with a specific release tag by the due date.\n\nMilestone 1 (EDA): will be tagged v0.1-eda\nMilestone 2 (NLP): will be tagged v0.2-nlp\nMilestone 3 (ML): will be tagged v0.3-ml\nMilestone 4 (Final): will be tagged v1.0-final",
    "crumbs": [
      "Overview",
      "Project"
    ]
  },
  {
    "objectID": "project/index.html#team-repository-structure",
    "href": "project/index.html#team-repository-structure",
    "title": "Project",
    "section": "Team repository structure",
    "text": "Team repository structure\nAll of your work will be done within your team’s GitHub repository. Milestone submissions will happen by tagging specific commits.\nYou will work within an organized repository and apply coding and development best practices. The repository has the following structure:\n├── README.md\n├── code\n├── data\n├── docs\n└── website-source\nAdditional structure specifications for other components of the project can be found in the Website guidance.\n\nDescription\n\nThe code/ directory is where you will write all of your notebooks and scripts. You will have a combination of Pyspark and Python notebooks, and one sub-directory per major task area. You may add additional sub-directories as needed to modularize your development.\nThe data/ directory should contain your (small) data files and should have multiple sub-directories (i.e. raw, processed, analytical, etc.) as needed. No large datasets are permitted\nThe docs/ folder should contain your website, with the source code for the website residing in website-source/. This can then be deployed using Github Pages if you so choose.\n\n\n\nCode\n\nYour code files must be well-organized\nDo not work in a messy repository and then try to clean it up\nIn notebooks, use Markdown cells to explain what you are doing and the decisions you are making\nDo not write monolithic Notebooks or scripts\nModularize your code (a script should do a single task)\nUse code comments so others can understand and leverage your code in the future\nUse functions to promote code reuse",
    "crumbs": [
      "Overview",
      "Project"
    ]
  },
  {
    "objectID": "project/index.html#team-contribution",
    "href": "project/index.html#team-contribution",
    "title": "Project",
    "section": "Team contribution",
    "text": "Team contribution\nAll team members must contribute to the project equally and fairly. Individual team member contributions will be assessed through a) the count and content of commits, and b) a peer evaluation form. If peer evaluations indicate that students within a team are not contributing equally, those students will receive a grade penalty and a lower grade than the rest of their team.",
    "crumbs": [
      "Overview",
      "Project"
    ]
  },
  {
    "objectID": "project/index.html#peer-feedback",
    "href": "project/index.html#peer-feedback",
    "title": "Project",
    "section": "Peer Feedback",
    "text": "Peer Feedback\n\nGiving feedback: Each group will review one other group’s EDA deliverable. Criteria will be provided as the basis of the evaluation. The deliverable is a single word document that is provided to the team receiving feedback.\nReceiving feedback: Each group will receive feedback from another group. The group receiving feedback will email the group providing feedback with their EDA deliverable.",
    "crumbs": [
      "Overview",
      "Project"
    ]
  },
  {
    "objectID": "project/index.html#grading-rubric",
    "href": "project/index.html#grading-rubric",
    "title": "Project",
    "section": "Grading rubric",
    "text": "Grading rubric\nThe project will be evaluated using the following high-level criteria:\n\nLevel of analytical rigor at the graduate student level\nLevel of technical approach\nAppropriate use of tools\nQuality and clarity of your writing and overall presentation\n\n\n\n\n\n\n\nImportant\n\n\n\nThe project milestones are cumulative. Therefore, we will grade the project after the final submission with a holistic project rubric. We will qualitatively grade the milestones, and we will provide feedback and a trending grade with each milestone. It is up to you to incorporate the feedback provided. If your milestone trending grade is lower than you expected, and you do not incorporate the feedback we provide for improvement, do not expect your final project grade to improve.\n\n\n\nIf a deliverable exceeds the requirements and expectations, that is considered A level work.\nIf a deliverable just meets the requirements and expectations, that is considered A-/B+ level work.\nIf a deliverable does not meet the requirements, that is considered B or lesser level work.\n\nDeductions will be made for any of the following reasons:\n\nThere is a lack of analytical rigor:\n\nAnalytical decisions are not justified\nAnalysis is too simplistic\n\nVisualizations or tables are not professionally formatted\nBig data files included in the repository\nInstructions are not followed\nThere are missing sections of the deliverable\nThe overall presentation and/or writing is sloppy\nThere are no comments in your code\nThere are absolute filename links in your code\nThe repository structure is sloppy\nFiles are named incorrectly (wrong extensions, wrong case, etc.)",
    "crumbs": [
      "Overview",
      "Project"
    ]
  },
  {
    "objectID": "resources/index.html",
    "href": "resources/index.html",
    "title": "Resources",
    "section": "",
    "text": "Here are a bunch of resources that will help you in this class. This is a tough and challenging class, but if you put in the hard yards you will learn a lot. I promise you, that you will find me willing to work with you every step of the way.\n\nDue dates are closer than they appear.\n\n\n\n\n\n\n\nPay Attention\n\n\n\nDue dates are closer than they appear. Working with Big Data is tricky, start homeworks early to account for the unknown unknowns that will inevitably show up.\n\n\n\n\nFollow instructions for in the homeworks and lab assignments. The homeworks and labs require the output generated by your code to be in a specific format (JSON usually), in a specifically named file with specifically named fields. Your homeworks are graded by an autograder program (so I wrote code to grade the output of your code) and the autograder expects to see the output in a certain prescribed way and if it does not find that then it deducts points. I do manually review all grades once the autograder has done its job. This is important not only for this class but probably more so going forward when the output your code produces will be consumed by both humans and machines.",
    "crumbs": [
      "Resources",
      "Overview",
      "Resources"
    ]
  },
  {
    "objectID": "slides/01-slides.html#agenda-for-todays-session",
    "href": "slides/01-slides.html#agenda-for-todays-session",
    "title": "Lecture 1",
    "section": "Agenda for today’s session",
    "text": "Agenda for today’s session\n\nCourse and syllabus overview\nBig Data Concepts\n\nDefinition\nChallenges\nApproaches\n\nData Engineering\nIntroduction to bash\n\nLab: Linux command line"
  },
  {
    "objectID": "slides/01-slides.html#bookmark-these-links",
    "href": "slides/01-slides.html#bookmark-these-links",
    "title": "Lecture 1",
    "section": "Bookmark these links!",
    "text": "Bookmark these links!\n\nCourse website: https://gu-dsan.github.io/6000-fall-2025/\nGitHub Organization for your deliverables: https://github.com/gu-dsan/\nGitHub Classroom: https://classroom.github.com/classrooms/34950344-georgetown-university-dsan6000-big-data-and-cloud-computing\nSlack Workspace: DSAN6000 Fall 2025 - https://dsan6000fall2025.slack.com\n\nJoin link: https://join.slack.com/t/dsan6000fall2025/shared_invite/zt-3b22qhque-GagQykwYYNiEzli9UXJn4w\n\nInstructors email: dsan-Fall-2025@georgetown.edu\nCanvas: https://georgetown.instructure.com/courses/TBA-2025\n\n\n\n\n\n\n\nThese are also pinned on the Slack main channel"
  },
  {
    "objectID": "slides/01-slides.html#instructional-team---professors",
    "href": "slides/01-slides.html#instructional-team---professors",
    "title": "Lecture 1",
    "section": "Instructional Team - Professors",
    "text": "Instructional Team - Professors"
  },
  {
    "objectID": "slides/01-slides.html#amit-arora",
    "href": "slides/01-slides.html#amit-arora",
    "title": "Lecture 1",
    "section": "Amit Arora",
    "text": "Amit Arora\n\n\n\nPrincipal Solutions Architect - AI/ML at AWS\nAdjunct Professor at Georgetown University\nMultiple patents in telecommunications and applications of ML in telecommunications\n\nFun Facts\n\nI am a self-published author https://blueberriesinmysalad.com/\nMy book “Blueberries in my salad: my forever journey towards fitness & strength” is written as code in R and Markdown\nI love to read books about health and human performance, productivity, philosophy and Mathematics for ML. My reading list is online!"
  },
  {
    "objectID": "slides/01-slides.html#jeff-jacobs",
    "href": "slides/01-slides.html#jeff-jacobs",
    "title": "Lecture 1",
    "section": "Jeff Jacobs",
    "text": "Jeff Jacobs\n\n\n\nFull-time Professor at Georgetown University\nData Science and Engineering Expert\n\nFun Facts\n\nTeaching at Georgetown University\nExpertise in Apache Airflow and big data orchestration\nCloud computing and distributed systems specialist"
  },
  {
    "objectID": "slides/01-slides.html#teaching-assistant-1",
    "href": "slides/01-slides.html#teaching-assistant-1",
    "title": "Lecture 1",
    "section": "Teaching Assistant 1",
    "text": "Teaching Assistant 1\n\n\n\nTA placeholder information\nAdditional details to be added"
  },
  {
    "objectID": "slides/01-slides.html#teaching-assistant-2",
    "href": "slides/01-slides.html#teaching-assistant-2",
    "title": "Lecture 1",
    "section": "Teaching Assistant 2",
    "text": "Teaching Assistant 2\n\n\n\nTA placeholder information\nAdditional details to be added"
  },
  {
    "objectID": "slides/01-slides.html#course-description",
    "href": "slides/01-slides.html#course-description",
    "title": "Lecture 1",
    "section": "Course Description",
    "text": "Course Description\nData is everywhere! Many times, it’s just too big to work with traditional tools. This is a hands-on, practical workshop style course about using cloud computing resources to do analysis and manipulation of datasets that are too large to fit on a single machine and/or analyzed with traditional tools. The course will focus on Spark, MapReduce, the Hadoop Ecosystem and other tools.\nYou will understand how to acquire and/or ingest the data, and then massage, clean, transform, analyze, and model it within the context of big data analytics. You will be able to think more programmatically and logically about your big data needs, tools and issues.\n\nAlways refer to the syllabus and calendar in the course website for class policies."
  },
  {
    "objectID": "slides/01-slides.html#learning-objectives",
    "href": "slides/01-slides.html#learning-objectives",
    "title": "Lecture 1",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nSetup, operate and manage big data tools and cloud infrastructure, including Spark, MapReduce, DataBricks, Hadoop on Microsoft Azure and Amazon Web Services\nUse ancillary tools that support big data processing, including git and the Linux command line\nExecute a big data analytics exercise from start to finish: ingest, wrangle, clean, analyze, store, and present\nDevelop strategies to break down large problems and datasets into manageable pieces\nIdentify broad spectrum resources and documentation to remain current with big data tools and developments\nCommunicate and interpret the big data analytics results through written and verbal methods"
  },
  {
    "objectID": "slides/01-slides.html#evaluation",
    "href": "slides/01-slides.html#evaluation",
    "title": "Lecture 1",
    "section": "Evaluation",
    "text": "Evaluation\n\nGroup project : 40%\nAssignments : 30%\nLab completions : 20%\nQuizzes : 10%"
  },
  {
    "objectID": "slides/01-slides.html#course-materials",
    "href": "slides/01-slides.html#course-materials",
    "title": "Lecture 1",
    "section": "Course Materials",
    "text": "Course Materials\n\nSlides/labs/assignment on Website/GitHub\nQuizzes and readings in Canvas"
  },
  {
    "objectID": "slides/01-slides.html#communication",
    "href": "slides/01-slides.html#communication",
    "title": "Lecture 1",
    "section": "Communication",
    "text": "Communication\n\nSlack is the primary form of communication\nInstructional team email: dsan-Fall-2025@georgetown.edu"
  },
  {
    "objectID": "slides/01-slides.html#slack-rules",
    "href": "slides/01-slides.html#slack-rules",
    "title": "Lecture 1",
    "section": "Slack rules:",
    "text": "Slack rules:\n\nPost any question/comment about the course, assignments or any technical issue.\nDMs are to be used sparingly\nYou may not DM multiple people in the instructional team at the same time for the same issue\nKeep an eye on the questions posted in Slack. Use the search function. It’s very possible that we have already answered a questions\nYou may DM us back only if we DM you first on a given issue\nLab/assignment/project questions will only be answered up to 6 hours before something is due (i.e. 6pm on Mondays)"
  },
  {
    "objectID": "slides/01-slides.html#project",
    "href": "slides/01-slides.html#project",
    "title": "Lecture 1",
    "section": "Project",
    "text": "Project\n\nGroups of 3-4 students\nUse an archive of Reddit data, augmented with external data\nExploratory analysis\nNLP\nMachine Learning\nWriteup\n\nData sourcing and ingesting\nExploratory analysis\nModeling\nChallenges and Learnings\nConclusions\nFuture work"
  },
  {
    "objectID": "slides/01-slides.html#where-does-it-come-from-how-is-it-being-created",
    "href": "slides/01-slides.html#where-does-it-come-from-how-is-it-being-created",
    "title": "Lecture 1",
    "section": "Where does it come from?How is it being created?",
    "text": "Where does it come from?How is it being created?"
  },
  {
    "objectID": "slides/01-slides.html#in-one-minute-of-time-2018",
    "href": "slides/01-slides.html#in-one-minute-of-time-2018",
    "title": "Lecture 1",
    "section": "In one minute of time (2018)",
    "text": "In one minute of time (2018)"
  },
  {
    "objectID": "slides/01-slides.html#in-one-minute-of-time-2019",
    "href": "slides/01-slides.html#in-one-minute-of-time-2019",
    "title": "Lecture 1",
    "section": "In one minute of time (2019)",
    "text": "In one minute of time (2019)"
  },
  {
    "objectID": "slides/01-slides.html#in-one-minute-of-time-2020",
    "href": "slides/01-slides.html#in-one-minute-of-time-2020",
    "title": "Lecture 1",
    "section": "In one minute of time (2020)",
    "text": "In one minute of time (2020)"
  },
  {
    "objectID": "slides/01-slides.html#in-one-minute-of-time-2021",
    "href": "slides/01-slides.html#in-one-minute-of-time-2021",
    "title": "Lecture 1",
    "section": "In one minute of time (2021)",
    "text": "In one minute of time (2021)"
  },
  {
    "objectID": "slides/01-slides.html#a-lot-of-it-is-hapenning-online.",
    "href": "slides/01-slides.html#a-lot-of-it-is-hapenning-online.",
    "title": "Lecture 1",
    "section": "A lot of it is hapenning online.",
    "text": "A lot of it is hapenning online.\nWe can record every: * click * ad impression * billing event * video interaction * server request * transaction * network message * fault * …"
  },
  {
    "objectID": "slides/01-slides.html#it-can-also-be-user-generated-content-e.g.",
    "href": "slides/01-slides.html#it-can-also-be-user-generated-content-e.g.",
    "title": "Lecture 1",
    "section": "It can also be user-generated content, e.g.:",
    "text": "It can also be user-generated content, e.g.:\n\nInstagram posts\nTweets\nVideos\nYelp reviews\nFacebook posts\nStack Overflow posts\n…"
  },
  {
    "objectID": "slides/01-slides.html#but-health-and-scientific-computing-create-a-lot-too",
    "href": "slides/01-slides.html#but-health-and-scientific-computing-create-a-lot-too",
    "title": "Lecture 1",
    "section": "But health and scientific computing create a lot too!",
    "text": "But health and scientific computing create a lot too!\n\n???"
  },
  {
    "objectID": "slides/01-slides.html#theres-lots-of-graph-data-too",
    "href": "slides/01-slides.html#theres-lots-of-graph-data-too",
    "title": "Lecture 1",
    "section": "There’s lots of graph data too",
    "text": "There’s lots of graph data too\nMany interesting datasets have a graph structure:\n\nSocial networks\nGoogle’s knowledge graph\nTelecom networks\nComputer networks\nRoad networks\nCollaboration/relationships\n\nSome of these are HUGE"
  },
  {
    "objectID": "slides/01-slides.html#apache-web-server-log-files",
    "href": "slides/01-slides.html#apache-web-server-log-files",
    "title": "Lecture 1",
    "section": "Apache (web server) log files",
    "text": "Apache (web server) log files"
  },
  {
    "objectID": "slides/01-slides.html#system-log-files",
    "href": "slides/01-slides.html#system-log-files",
    "title": "Lecture 1",
    "section": "System log files",
    "text": "System log files"
  },
  {
    "objectID": "slides/01-slides.html#internet-of-things-iot",
    "href": "slides/01-slides.html#internet-of-things-iot",
    "title": "Lecture 1",
    "section": "Internet of Things (IoT)",
    "text": "Internet of Things (IoT)\nSensors everywhere…"
  },
  {
    "objectID": "slides/01-slides.html#smartphones-collecting-our-information",
    "href": "slides/01-slides.html#smartphones-collecting-our-information",
    "title": "Lecture 1",
    "section": "Smartphones collecting our information",
    "text": "Smartphones collecting our information"
  },
  {
    "objectID": "slides/01-slides.html#where-else",
    "href": "slides/01-slides.html#where-else",
    "title": "Lecture 1",
    "section": "Where else?",
    "text": "Where else?\n\nThe Internet\nTransactions\nDatabases\nExcel\nPDF Files\nAnything digital (music, movies, apps)\nSome old floppy disk lying around the house"
  },
  {
    "objectID": "slides/01-slides.html#typical-real-world-scenario",
    "href": "slides/01-slides.html#typical-real-world-scenario",
    "title": "Lecture 1",
    "section": "Typical real world scenario",
    "text": "Typical real world scenario\nYou have a laptop with 16GB of RAM and a 256GB Solid State drive. You are given a 1TB dataset in text files, where every file is slightly different. Oh no, what do you do?\nAD: This was the situation I experienced during my AstraZeneca interview. We had to do a data analysis for our interview, and the data given was the FDA drug adverse event reporting database."
  },
  {
    "objectID": "slides/01-slides.html#lets-discuss",
    "href": "slides/01-slides.html#lets-discuss",
    "title": "Lecture 1",
    "section": "Let’s discuss!",
    "text": "Let’s discuss!\n\nExponential data growth"
  },
  {
    "objectID": "slides/01-slides.html#big-data-definitions",
    "href": "slides/01-slides.html#big-data-definitions",
    "title": "Lecture 1",
    "section": "Big Data Definitions",
    "text": "Big Data Definitions\nWikipedia\n“In essence, is a term for a collection of datasets so large and complex that it becomes difficult to process using traditional tools and applications. Big Data technologies describe a new generation of technologies and architectures designed to economically extract value from very large volumes of a wide variety of data, by enabling high-velocity capture, discover and/or analysis”\nO’Reilly\n“Big data is when the size of the data itself becomes part of the problem”\nEMC/IDC\n“Big data technologies describe a new generation of technologies and architectures, designed to economically extract value from very large volumes of a wide variety of data, by enabling high-velocity capture, discovery, and/or analysis.”"
  },
  {
    "objectID": "slides/01-slides.html#frameworks-for-thinking-about-big-data",
    "href": "slides/01-slides.html#frameworks-for-thinking-about-big-data",
    "title": "Lecture 1",
    "section": "Frameworks for thinking about Big Data",
    "text": "Frameworks for thinking about Big Data\nIBM: (The famous 3-V’s definition)\n\nVolume (Gigabytes -&gt; Exabytes)\nVelocity (Batch -&gt; Streaming Data)\nVariety (Structured, Semi-structured, & Unstructured)\n\nAdditional V’s\n\nVariability\nVeracity\nVisualization\nValue"
  },
  {
    "objectID": "slides/01-slides.html#think-of-data-size-as-a-function-of-processing-and-storage",
    "href": "slides/01-slides.html#think-of-data-size-as-a-function-of-processing-and-storage",
    "title": "Lecture 1",
    "section": "Think of data size as a function of processing and storage",
    "text": "Think of data size as a function of processing and storage\n\nCan you analyze/process your data on a single machine?\nCan you store (or is it stored) on a single machine?\n\nIf any of of the answers is no then you have a big-ish data problem!"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes",
    "href": "slides/01-slides.html#relative-data-sizes",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes-1",
    "href": "slides/01-slides.html#relative-data-sizes-1",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes-2",
    "href": "slides/01-slides.html#relative-data-sizes-2",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes-3",
    "href": "slides/01-slides.html#relative-data-sizes-3",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes-4",
    "href": "slides/01-slides.html#relative-data-sizes-4",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes-5",
    "href": "slides/01-slides.html#relative-data-sizes-5",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#data-types",
    "href": "slides/01-slides.html#data-types",
    "title": "Lecture 1",
    "section": "Data Types",
    "text": "Data Types\n\nStructured\nUnstructured\nNatural language\nMachine-generated\nGraph-based\nAudio, video, and images\nStreaming"
  },
  {
    "objectID": "slides/01-slides.html#big-data-vs.-small-data",
    "href": "slides/01-slides.html#big-data-vs.-small-data",
    "title": "Lecture 1",
    "section": "Big Data vs. Small Data",
    "text": "Big Data vs. Small Data\n\n\n\n\n\n\n\n\n\nSmall Data is usually…\nOn the other hand, Big Data…\n\n\n\n\nGoals\ngathered for a specific goal\nmay have a goal in mind when it’s first started, but things can evolve or take unexpected directions\n\n\nLocation\nin one place, and often in a single computer file\ncan be in multiple files in multiple servers on computers in different geographic locations\n\n\nStructure/Contents\nhighly structured like an Excel spreadsheet, and it’s got rows and columns of data\ncan be unstructured, it can have many formats in files involved across disciplines, and may link to other resources\n\n\nPreparation\nprepared by the end user for their own purposes\nis often prepared by one group of people, analyzed by a second group of people, and then used by a third group of people, and they may have different purposes, and they may have different disciplines\n\n\nLongevity\nkept for a specific amount of time after the project is over because there’s a clear ending point. In the academic world it’s maybe five or seven years and then you can throw it away\ncontains data that must be stored in perpetuity. Many big data projects extend into the past and future\n\n\nMeasurements\nmeasured with a single protocol using set units and it’s usually done at the same time\nis collected and measured using many sources, protocols, units, etc\n\n\nReproducibility\nbe reproduced in their entirety if something goes wrong in the process\nreplication is seldom feasible\n\n\nStakes\nif things go wrong the costs are limited, it’s not an enormous problem\ncan have high costs of failure in terms of money, time and labor\n\n\nAccess\nidentified by a location specified in a row/column\nunless it is exceptionally well designed, the organization can be inscrutable\n\n\nAnalysis\nanalyzed together, all at once\nis ordinarily analyzed in incremental steps"
  },
  {
    "objectID": "slides/01-slides.html#traditional-data-analysis-tools-like-r-and-python-are-single-threaded",
    "href": "slides/01-slides.html#traditional-data-analysis-tools-like-r-and-python-are-single-threaded",
    "title": "Lecture 1",
    "section": "Traditional data analysis tools like R and Python are single threaded",
    "text": "Traditional data analysis tools like R and Python are single threaded"
  },
  {
    "objectID": "slides/01-slides.html#tools-at-a-glance",
    "href": "slides/01-slides.html#tools-at-a-glance",
    "title": "Lecture 1",
    "section": "Tools at-a-glance",
    "text": "Tools at-a-glance\n\n\nLanguages, libraries, and projects\n\nPython\n\npandas\npolars\nPySpark\nduckdb\ndask\nray\n\nApache Arrow\nApache Spark\nSQL\n\nWe’ll talk briefly about Apache Hadoop today but we will not cover it in this course.\n\nCloud Services\n\nAmazon Web Services (AWS)\n\nAWS Sagemaker\nAmazon S3\n\nAzure\n\nAzure Blob\nAzure Machine Learning\n\n\nOther:\n\nAWS Elastic MapReduce (EMR)"
  },
  {
    "objectID": "slides/01-slides.html#additional-links-of-interest",
    "href": "slides/01-slides.html#additional-links-of-interest",
    "title": "Lecture 1",
    "section": "Additional links of interest",
    "text": "Additional links of interest\n\nMatt Turck’s Machine Learning, Artificial Intelligence & Data Landscape (MAD)\n\nArticle\nInteractive Landscape\n\nIs there life after Hadoop?\n10 Best Big Data Tools for 2023"
  },
  {
    "objectID": "slides/01-slides.html#difference-between-data-scientist-and-data-engineer",
    "href": "slides/01-slides.html#difference-between-data-scientist-and-data-engineer",
    "title": "Lecture 1",
    "section": "Difference between Data Scientist and Data Engineer",
    "text": "Difference between Data Scientist and Data Engineer\nIn this course, you’ll be doing a little data engineering!"
  },
  {
    "objectID": "slides/01-slides.html#responsibilities",
    "href": "slides/01-slides.html#responsibilities",
    "title": "Lecture 1",
    "section": "Responsibilities",
    "text": "Responsibilities"
  },
  {
    "objectID": "slides/01-slides.html#data-engineering-falls-into-levels-2-and-3-primarily",
    "href": "slides/01-slides.html#data-engineering-falls-into-levels-2-and-3-primarily",
    "title": "Lecture 1",
    "section": "Data Engineering falls into levels 2 and 3 primarily",
    "text": "Data Engineering falls into levels 2 and 3 primarily"
  },
  {
    "objectID": "slides/01-slides.html#as-an-analystdata-scientist-you-really-need-both",
    "href": "slides/01-slides.html#as-an-analystdata-scientist-you-really-need-both",
    "title": "Lecture 1",
    "section": "As an analyst/data scientist, you really need both",
    "text": "As an analyst/data scientist, you really need both"
  },
  {
    "objectID": "slides/01-slides.html#architecture",
    "href": "slides/01-slides.html#architecture",
    "title": "Lecture 1",
    "section": "Architecture",
    "text": "Architecture"
  },
  {
    "objectID": "slides/01-slides.html#storage",
    "href": "slides/01-slides.html#storage",
    "title": "Lecture 1",
    "section": "Storage",
    "text": "Storage"
  },
  {
    "objectID": "slides/01-slides.html#source-control",
    "href": "slides/01-slides.html#source-control",
    "title": "Lecture 1",
    "section": "Source control",
    "text": "Source control"
  },
  {
    "objectID": "slides/01-slides.html#orchestration",
    "href": "slides/01-slides.html#orchestration",
    "title": "Lecture 1",
    "section": "Orchestration",
    "text": "Orchestration"
  },
  {
    "objectID": "slides/01-slides.html#processing",
    "href": "slides/01-slides.html#processing",
    "title": "Lecture 1",
    "section": "Processing",
    "text": "Processing"
  },
  {
    "objectID": "slides/01-slides.html#analytics",
    "href": "slides/01-slides.html#analytics",
    "title": "Lecture 1",
    "section": "Analytics",
    "text": "Analytics"
  },
  {
    "objectID": "slides/01-slides.html#machine-learning",
    "href": "slides/01-slides.html#machine-learning",
    "title": "Lecture 1",
    "section": "Machine Learning",
    "text": "Machine Learning"
  },
  {
    "objectID": "slides/01-slides.html#governance",
    "href": "slides/01-slides.html#governance",
    "title": "Lecture 1",
    "section": "Governance",
    "text": "Governance"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-1",
    "href": "slides/01-slides.html#linux-command-line-1",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nTerminal\n\n\nTerminal access was THE ONLY way to do programming\nNo GUIs! No Spyder, Jupyter, RStudio, etc.\nCoding is still more powerful than graphical interfaces for complex jobs\nCoding makes work repeatable"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-2",
    "href": "slides/01-slides.html#linux-command-line-2",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nBASH\n\n\nCreated in 1989 by Brian Fox\nBrian Fox also built the first online interactive banking software\nBASH is a command processor\nConnection between you and the machine language and hardware"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-3",
    "href": "slides/01-slides.html#linux-command-line-3",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nThe Prompt\nusername@hostname:current_directory $\nWhat do we learn from the prompt?\n\nWho you are - username\nThe machine where your code is running - hostname\nThe directory where your code is running - current_directory\nThe shell type - $ - this symbol means BASH"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-4",
    "href": "slides/01-slides.html#linux-command-line-4",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nSyntax\nCOMMAND -F --FLAG * COMMAND is the program * Everything after that are arguments * F is a single letter flag * FLAG is a single word or words connected by dashes flag. A space breaks things into a new argument. + Sometimes single letter and long form flags (e.g. F and FLAG) can refer to the same argument\nCOMMAND -F --FILE file1\nHere we pass an text argument “file1” into the FILE flag\nThe -h flag is usually to get help. You can also run the man command and pass the name of the program as the argument to get the help page.\nLet’s try basic commands:\n\ndate to get the current date\nwhoami to get your user name\necho \"Hello World\" to print to the console"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-5",
    "href": "slides/01-slides.html#linux-command-line-5",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nExamining Files\nFind out your Present Working Directory pwd\nExamine the contents of files and folders using the ls command\nMake new files from scratch using the touch command\nGlobbing - how to select files in a general way\n\n\\* for wild card any number of characters\n\\? for wild card for a single character\n[] for one of many character options\n! for exclusion\nspecial options [:alpha:], [:alnum:], [:digit:], [:lower:], [:upper:]\n\nReference material Reference material: Shell Lesson 1,2,4,5"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-6",
    "href": "slides/01-slides.html#linux-command-line-6",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nNavigating Directories\nKnowing where your terminal is executing code ensures you are working with the right inputs and making the right outputs.\nUse the command pwd to determine the Present Working Directory.\nLet’s say you need to change to a folder called “git-repo”. To change directories you can use a command like cd git-repo.\n\n. refers to the current directory, such as ./git-repo\n.. can be used to move up one folder, use cd .., and can be combined to move up multiple levels ../../my_folder\n/ is the root of the Linux OS, where there are core folders, such as system, users, etc.\n~ is the home directory. Move to folders referenced relative to this path by including it at the start of your path, for example ~/projects.\n\nTo view the structure of directories from your present working directory, use the tree command\nReference link"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-7",
    "href": "slides/01-slides.html#linux-command-line-7",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nInteracting with Files\nNow that we know how to navigate through directories, we need to learn the commands for interacting with files\n\nmv to move files from one location to another\n\nCan use file globbing here - ?, *, [], …\n\ncp to copy files instead of moving\n\nCan use file globbing here - ?, *, [], …\n\nmkdir to make a directory\nrm to remove files\nrmdir to remove directories\nrm -rf to blast everything! WARNING!!! DO NOT USE UNLESS YOU KNOW WHAT YOU ARE DOING"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-8",
    "href": "slides/01-slides.html#linux-command-line-8",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nUsing BASH for Data Exploration\nCommands:\n\nhead FILENAME / tail FILENAME - glimpsing the first / last few rows of data\nmore FILENAME / less FILENAME - viewing the data with basic up / (up & down) controls\ncat FILENAME - print entire file contents into terminal\nvim FILENAME - open (or edit!) the file in vim editor\ngrep FILENAME - search for lines within a file that match a regex expression\nwc FILENAME - count the number of lines (-l flag) or number of words (-w flag)\n\nReference link Reference material: Text Lesson 8,9,15,16"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-9",
    "href": "slides/01-slides.html#linux-command-line-9",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nPipes and Arrows\n\n| sends the stdout to another command (is the most powerful symbol in BASH!)\n&gt; sends stdout to a file and overwrites anything that was there before\n&gt;&gt; appends the stdout to the end of a file (or starts a new file from scratch if one does not exist yet)\n&lt; sends stdin into the command on the left\n\nTo-dos:\n\necho Hello World\nCounting rows of data with certain attributes\n\nReference material: Text Lesson 1,2,3,4,5"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-10",
    "href": "slides/01-slides.html#linux-command-line-10",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nAlias and User Files\n.bashrc is where your shell settings are located\nIf we wanted a shortcut to find out the number of our running processes, we would write a commmand like whoami | xargs ps -u | wc -l.\nWe don’t want to write out this full command every time! Let’s make an alias.\nalias alias_name=\"command_to_run\"\nalias nproc=\"whoami | xargs ps -u | wc -l\"\nNow we need to put this alias into the .bashrc\nalias nproc=\"whoami | xargs ps -u | wc -l\" &gt;&gt; ~/.bashrc\nWhat happened??\necho alias nproc=\"whoami | xargs ps -u | wc -l\" &gt;&gt; ~/.bashrc\nYour commands get saved in ~/.bash_history"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-11",
    "href": "slides/01-slides.html#linux-command-line-11",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nProcess Managment\nUse the command ps to see your running processes.\nUse the command top or even better htop to see all the running processes on the machine.\nInstall the program htop using the command sudo yum install htop -y\nFind the process ID (PID) so you can kill a broken process.\nUse the command kill [PID NUM] to signal the process to terminate. If things get really bad, then use the command kill -9 [PID NUM]\nTo kill a command in the terminal window it is running in, try using Ctrl + C or Ctrl + /\nRun the cat command on its own to let it stay open. Now open a new terminal to examine the processes and find the cat process.\nReference material: Text Lesson 1,2,3,7,9,10\nTry playing a Linux game!\nhttps://gitlab.com/slackermedia/bashcrawl is a game to help you practice your navigation and file access skills. Click on the binder link in this repo to launch a jupyter lab session and explore!"
  },
  {
    "objectID": "slides/03-slides.html#agenda-and-goals-for-today",
    "href": "slides/03-slides.html#agenda-and-goals-for-today",
    "title": "Lecture 3",
    "section": "Agenda and Goals for Today",
    "text": "Agenda and Goals for Today\n\nScaling up and scaling out\nParallelization\nMap and Reduce functions\nLab Preview: Parallelization with Python\n\nUse the multiprocessing module\nImplement synchronous and asynchronous processing\n\nHomework Preview: Parallelization with Python\n\nParallel data processing"
  },
  {
    "objectID": "slides/03-slides.html#looking-back",
    "href": "slides/03-slides.html#looking-back",
    "title": "Lecture 3",
    "section": "Looking back",
    "text": "Looking back\n\nContinued great use of Slack \n\nNice interactions\n\nDue date reminders:\n\nAssignment 2: September 17, 2025\nLab 3: September 17, 2025\nAssignment 3: September 24, 2025"
  },
  {
    "objectID": "slides/03-slides.html#glossary",
    "href": "slides/03-slides.html#glossary",
    "title": "Lecture 3",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nLocal\nYour current workstation (laptop, desktop, etc.), wherever you start the terminal/console application.\n\n\nRemote\nAny machine you connect to via ssh or other means.\n\n\nEC2\nSingle virtual machine in the cloud where you can run computation (ephemeral)\n\n\nSageMaker\nIntegrated Developer Environment where you can conduct data science on single machines\n\n\nEphemeral\nLasting for a short time - any machine that will get turned off or place you will lose data\n\n\nPersistent\nLasting for a long time - any environment where your work is NOT lost when the timer goes off"
  },
  {
    "objectID": "slides/03-slides.html#typical-real-world-scenarios",
    "href": "slides/03-slides.html#typical-real-world-scenarios",
    "title": "Lecture 3",
    "section": "Typical real world scenarios",
    "text": "Typical real world scenarios\n\n\nYou are a Data Scientist and you want to cross-validate your models. This involves running the model 1000 times but each run takes over an hour.\nYou are a Data Scientist and you want to run multiple models on your data, where each run can take up to 1 hour.\nYou are a genomics researcher and have been using small datasets of sequence data but soon you will receive a new type of sequencing data that is 10 times as large. This means 10x more transcripts to process, but the processing for each transcript is similar.\nYou are an engineer using a fluid dynamics package that has an option to run in parallel. So far, you haven’t used this option on your workstation. When moving from 2D to 3D simulations, the simulation time has more than tripled so it may make sense to take advantage of the parallel feature\nYou are a Data Scientist at the Federal Reserve and you have millions of text to process. So far, you have only executed NLP on thousands of articles and have not implemented machine learning models on them."
  },
  {
    "objectID": "slides/03-slides.html#linear-vs.-parallel",
    "href": "slides/03-slides.html#linear-vs.-parallel",
    "title": "Lecture 3",
    "section": "Linear vs. Parallel",
    "text": "Linear vs. Parallel\n\n\nLinear/Sequential\n\nA program starts to run\nThe program issues an instruction\nThe instruction is executed\nSteps 2 and 3 are repeated\nThe program finishes running\n\n\nParallel\n\nA program starts to run\nThe program divides up the work into chunks of instructions and data\nEach chunk of work is executed independently\nThe chunks of work are reassembled\nThe program finishes running"
  },
  {
    "objectID": "slides/03-slides.html#linear-vs.-parallel-1",
    "href": "slides/03-slides.html#linear-vs.-parallel-1",
    "title": "Lecture 3",
    "section": "Linear vs. Parallel",
    "text": "Linear vs. Parallel"
  },
  {
    "objectID": "slides/03-slides.html#linear-vs.-parallel-2",
    "href": "slides/03-slides.html#linear-vs.-parallel-2",
    "title": "Lecture 3",
    "section": "Linear vs. Parallel",
    "text": "Linear vs. Parallel\nFrom a data science perspective\n\n\nLinear\n\nThe data remains monolithic\nProcedures act on the data sequentially\n\nEach procedure has to complete before the next procedure can start\n\nYou can think of this as a single pipeline\n\n\nParallel\n\nThe data can be split up into chunks\nThe same procedures can be run on each chunk at the same time\nOr, independent procedures can run on different chunks at the same time\nNeed to bring things back together at the end\n\n\n\nWhat are some examples of linear and parallel data science workflows?"
  },
  {
    "objectID": "slides/03-slides.html#embarrasingly-parallel",
    "href": "slides/03-slides.html#embarrasingly-parallel",
    "title": "Lecture 3",
    "section": "Embarrasingly Parallel",
    "text": "Embarrasingly Parallel\nIt’s easy to speed things up when:\n\nYou need to calculate the same thing many times\nCalculations are independent of each other\nEach calculation takes a decent amount of time\n\nJust run multiple calculations at the same time"
  },
  {
    "objectID": "slides/03-slides.html#embarrasingly-parallel-1",
    "href": "slides/03-slides.html#embarrasingly-parallel-1",
    "title": "Lecture 3",
    "section": "Embarrasingly Parallel",
    "text": "Embarrasingly Parallel\nThe concept is based on the old middle/high school math problem:\n\nIf 5 people can shovel a parking lot in 6 hours, how long will it take 100 people to shovel the same parking lot?\n\nBasic idea is that many hands (cores/instances) make lighter (faster/more efficient) work of the same problem, as long as the effort can be split up appropriately into nearly equal parcels\n\n\nThe classical answer to the problem is 18 minutes"
  },
  {
    "objectID": "slides/03-slides.html#embarassingly-parallel",
    "href": "slides/03-slides.html#embarassingly-parallel",
    "title": "Lecture 3",
    "section": "Embarassingly parallel",
    "text": "Embarassingly parallel\n\n\nIf you can truly split up your problem into multiple independent parts, then you can often get linear speedups with the number of parallel components (to a limit)\n\nThe more cores you use and the more you parallelize, the more you incur communication overhead and decrease available RAM, so the speedup is almost certainly sub-linear, i.e. for a 4-core machine you’ll probably get a 3-4x speedup, but rarely a full 4x speedup1\n\nThe question often is, which part of your problem is embarassingly parallel?\nAmdahl’s law (which we’ll see in a few slides) shows how parallelization can benefit overall if a large proportion of the problem is parallelizable\nIt’s not all milk and honey. Setting up, programming, evaluating, debugging parallel computations requires better infrastructure and more expertise.\n\n\nGorelick & Ozsvald, 2020. High Performance Python, O’Reilly"
  },
  {
    "objectID": "slides/03-slides.html#some-limitations",
    "href": "slides/03-slides.html#some-limitations",
    "title": "Lecture 3",
    "section": "Some limitations",
    "text": "Some limitations\nYou can get speedups by parallelizing computations, but\n\nHaving to transport data between parallel processes (memory bottlenecks) and communication between processes (I/O bottlenecks) can make things more expensive and can exceed the benefits of parallelization\nIf you’re moving a lot of data but not doing a lot of parallel computing, it’s often not worth the effort\n\n\nSetting up and debugging parallel programs can be difficult\nBut this has become easier with better software, like the multiprocessing module in Python\n\n\nMaking sure that we can get back all the pieces needs monitoring\n\nFailure tolerance and protections (Hadoop, e.g.)\nProper collection and aggregation of the processed data"
  },
  {
    "objectID": "slides/03-slides.html#amdahls-law",
    "href": "slides/03-slides.html#amdahls-law",
    "title": "Lecture 3",
    "section": "Amdahl’s Law",
    "text": "Amdahl’s Law\n\n\n\n\n\n\\[\n\\lim_{s\\rightarrow\\infty} S_{latency} = \\frac{1}{1-p}\n\\]\nwhere \\(s\\) is the speedup of that part of the task (which is \\(p\\) proportion of the overall task) benefitting from improved resources.\nIf 50% of the task is embarassingly parallel, you can get a maximum speedup of 2-fold, while if 90% is embarassingly parallel, you can get a maximum speedup of \\(1/(1-0.9) = 10\\) fold.\n\n\nBy Daniels220 at English Wikipedia, CC BY-SA 3.0, Link"
  },
  {
    "objectID": "slides/03-slides.html#pros-and-cons-of-parallelization",
    "href": "slides/03-slides.html#pros-and-cons-of-parallelization",
    "title": "Lecture 3",
    "section": "Pros and cons of parallelization",
    "text": "Pros and cons of parallelization\n\nYes\n\nGroup by analysis\nSimulations\nResampling / Bootstrapping\nOptimization\nCross-validation\nTraining bagged models (like Random Forests)\nMultiple chains in a Bayesian MCMC\nScoring (predicting) using trained models\n\n\n\nNo\n\nSQL Operations\nInverting a matrix\nTraining linear regression\nTraining logistic regression\nTraining trees\nTraining neural nets\nTraining boosted models (like gradient boosted trees)\nEach chain in a Bayesian MCMC\nMost things time series\n\n\n\n\n\n\n\n\n\nFor processes in the “No” column, each step depends on a previous step, and so they cannot be parallelized. However, there are approximate numerical methods applicable to big data which are parallelizable and get you to the right answer, based on parallely taking random subsets of the data. We’ll see some of these when we look at Spark ML"
  },
  {
    "objectID": "slides/03-slides.html#pros-and-cons-of-parallelization-1",
    "href": "slides/03-slides.html#pros-and-cons-of-parallelization-1",
    "title": "Lecture 3",
    "section": "Pros and cons of parallelization",
    "text": "Pros and cons of parallelization\n\n\nPros\n\nHigher efficiency\nUsing modern infrastructure\nScalable to larger data, more complex procedures\n\nproviso procedures are embarassingly parallel\n\n\n\n\nCons\n\nHigher programming complexity\nNeed proper software infrastructure (MPI, Hadoop, etc)\nNeed to ensure right packages/modules are distributed across processors\nNeed to account for a proportion of jobs failing, and recovering from them\nHence, Hadoop/Spark and other technologies\nHigher setup cost in terms of time/expertise/money\n\n\n\n\nThere are good solutions today for most of the cons, so the pros have it and so this paradigm is widely accepted and implemented"
  },
  {
    "objectID": "slides/03-slides.html#distributed-memory-message-passing-model",
    "href": "slides/03-slides.html#distributed-memory-message-passing-model",
    "title": "Lecture 3",
    "section": "Distributed memory / Message Passing Model",
    "text": "Distributed memory / Message Passing Model"
  },
  {
    "objectID": "slides/03-slides.html#data-parallel-model",
    "href": "slides/03-slides.html#data-parallel-model",
    "title": "Lecture 3",
    "section": "Data parallel model",
    "text": "Data parallel model"
  },
  {
    "objectID": "slides/03-slides.html#hybrid-model",
    "href": "slides/03-slides.html#hybrid-model",
    "title": "Lecture 3",
    "section": "Hybrid model",
    "text": "Hybrid model"
  },
  {
    "objectID": "slides/03-slides.html#partitioning-data",
    "href": "slides/03-slides.html#partitioning-data",
    "title": "Lecture 3",
    "section": "Partitioning data",
    "text": "Partitioning data"
  },
  {
    "objectID": "slides/03-slides.html#designing-parallel-programs",
    "href": "slides/03-slides.html#designing-parallel-programs",
    "title": "Lecture 3",
    "section": "Designing parallel programs",
    "text": "Designing parallel programs\n\nData partitioning\nCommunication\nSynchronization / Orchestration\nData dependencies\nLoad balancing\nInput and Output (I/O)\nDebugging\n\n\nA lot of these components are data engineering and DevOps issues\n\n\nInfrastructures have standardized many of these and have helped data scientists implement parallel programming much more easily\n\n\nWe’ll see in the lab how the multiprocessing module in Python makes parallel processing on a machine quite easy to implement"
  },
  {
    "objectID": "slides/03-slides.html#map-and-reduce",
    "href": "slides/03-slides.html#map-and-reduce",
    "title": "Lecture 3",
    "section": "Map and Reduce",
    "text": "Map and Reduce"
  },
  {
    "objectID": "slides/03-slides.html#components-of-a-parallel-programming-workflow",
    "href": "slides/03-slides.html#components-of-a-parallel-programming-workflow",
    "title": "Lecture 3",
    "section": "Components of a parallel programming workflow",
    "text": "Components of a parallel programming workflow\n\nDivide the work into chunks\nWork on each chunk separately\nReassemble the work\n\nThis paradigm is often referred to as a map-reduce framework, or, more descriptively, the split-apply-combine paradigm"
  },
  {
    "objectID": "slides/03-slides.html#map-1",
    "href": "slides/03-slides.html#map-1",
    "title": "Lecture 3",
    "section": "Map",
    "text": "Map\nThe map operation is a 1-1 operation that takes each split and processes it\nThe map operation keeps the same number of objects in its output that were present in its input"
  },
  {
    "objectID": "slides/03-slides.html#map-2",
    "href": "slides/03-slides.html#map-2",
    "title": "Lecture 3",
    "section": "Map",
    "text": "Map\n\n\n\nThe operations included in a particular map can be quite complex, involving multiple steps. In fact, you can implement a pipeline of procedures within the map step to process each data object.\nThe main point is that the same operations will be run on each data object in the map implementation"
  },
  {
    "objectID": "slides/03-slides.html#map-3",
    "href": "slides/03-slides.html#map-3",
    "title": "Lecture 3",
    "section": "Map",
    "text": "Map\nSome examples of a map operations are\n\nExtracting a standard table from online reports from multiple years\nExtracting particular records from multiple JSON objects\nTransforming data (as opposed to summarizing it)\nRun a normalization script on each transcript in a GWAS dataset\nStandardizing demographic data for each of the last 20 years against the 2000 US population"
  },
  {
    "objectID": "slides/03-slides.html#reduce-1",
    "href": "slides/03-slides.html#reduce-1",
    "title": "Lecture 3",
    "section": "Reduce",
    "text": "Reduce\nThe reduce operation takes multiple objects and reduces them to a (perhaps) smaller number of objects using transformations that aren’t amenable to the map paradigm.\nThese transformations are often serial/linear in nature\nThe reduce transformation is usually the last, not-so-elegant transformation needed after most of the other transformations have been efficiently handled in a parallel fashion by map"
  },
  {
    "objectID": "slides/03-slides.html#reduce-2",
    "href": "slides/03-slides.html#reduce-2",
    "title": "Lecture 3",
    "section": "Reduce",
    "text": "Reduce\nThe reduce operation requires\n\n\nAn accumulator function, that will update serially as new data is fed into it\n\nA sequence of objects to run through the accumulator function\n\nA starting value from which the accumulator function starts\n\nProgrammatically, this can be written as"
  },
  {
    "objectID": "slides/03-slides.html#reduce-3",
    "href": "slides/03-slides.html#reduce-3",
    "title": "Lecture 3",
    "section": "Reduce",
    "text": "Reduce\nThe reduce operation works serially from “left” to “right”, passing each object successively through the accumulator function.\nFor example, if we were to add successive numbers with a function called add…"
  },
  {
    "objectID": "slides/03-slides.html#reduce-4",
    "href": "slides/03-slides.html#reduce-4",
    "title": "Lecture 3",
    "section": "Reduce",
    "text": "Reduce\nSome examples:\n\nFinding the common elements (intersection) of a large number of sets\nComputing a table of group-wise summaries\nFiltering\nTabulating"
  },
  {
    "objectID": "slides/03-slides.html#map-reduce-1",
    "href": "slides/03-slides.html#map-reduce-1",
    "title": "Lecture 3",
    "section": "map-reduce",
    "text": "map-reduce\nCombining the map and reduce operations creates a powerful pipeline that can handle a diverse range of problems in the Big Data context"
  },
  {
    "objectID": "slides/03-slides.html#parallelization-and-map-reduce-are-bed-mates",
    "href": "slides/03-slides.html#parallelization-and-map-reduce-are-bed-mates",
    "title": "Lecture 3",
    "section": "Parallelization and map-reduce are bed-mates",
    "text": "Parallelization and map-reduce are bed-mates\n\n\n\n\nOne of the issues here is, how to split the data in a “good” manner so that the map-reduce framework works well"
  },
  {
    "objectID": "slides/03-slides.html#the-multiprocessing-module",
    "href": "slides/03-slides.html#the-multiprocessing-module",
    "title": "Lecture 3",
    "section": "The multiprocessing module",
    "text": "The multiprocessing module\n\nFocused on single-machine multicore parallelism\nFacilitates:\n\nprocess- and thread-based parallel processing\nsharing work over queues\nsharing data among processes"
  },
  {
    "objectID": "slides/03-slides.html#processes-and-threads",
    "href": "slides/03-slides.html#processes-and-threads",
    "title": "Lecture 3",
    "section": "Processes and threads",
    "text": "Processes and threads\n\n\nA process is an executing program, that is self-contained and has dedicated runtime and memory\nA thread is the basic unit to which the operating system allocates processor time. It is an entity within a process. A thread can execute any part of the process code, including parts currently being executed by another thread.\nA thread will often be faster to spin up and terminate than a full process\nThreads can share memory and data with each other\n\n\n\n\n\n\n\n\n\nPython has the Global Interpretor Lock (GIL) which only allows only one thread to interact with Python objects at a time. So the way to parallel process in Python is to do multi-processor parallelization, where we run multiple Python interpretors across multiple processes, each with its own private memory space and GIL."
  },
  {
    "objectID": "slides/03-slides.html#some-concepts-in-multiprocessing",
    "href": "slides/03-slides.html#some-concepts-in-multiprocessing",
    "title": "Lecture 3",
    "section": "Some concepts in multiprocessing1",
    "text": "Some concepts in multiprocessing1\nProcess\nA forked copy of the current process; this creates a new process identifier, and the task runs as an independent child process in the operating system\nPool\nWraps the Process into a convenient pool of workers that share a chunk of work and return an aggregated result\nGorelick & Ozsvald, 2020. High Performance Python, O’Reilly"
  },
  {
    "objectID": "slides/03-slides.html#other-methods-of-parallel-processing-in-python",
    "href": "slides/03-slides.html#other-methods-of-parallel-processing-in-python",
    "title": "Lecture 3",
    "section": "Other methods of parallel processing in Python",
    "text": "Other methods of parallel processing in Python\n\nThe joblib module\nMost scikit-learn functions have implicit parallelization baked in through the n_jobs parameter\n\nFor example\nfrom sklearn.ensembles import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators = 100, random_state = 124, n_jobs=-1)\nuses the joblib module to use all available processors (n_jobs=-1) to do the bootstrapping\n\n\n\n\n\nSee here for a description of parallel processing in the scikit-learn module"
  },
  {
    "objectID": "slides/03-slides.html#the-need-for-async-io",
    "href": "slides/03-slides.html#the-need-for-async-io",
    "title": "Lecture 3",
    "section": "The need for Async I/O",
    "text": "The need for Async I/O\n\n\n\n\nWhen talking to external systems (databases, APIs) the bottleneck is not local CPU/memory but rather the time it takes to receive a response from the external system.\nThe Async I/O model addresses this by allowing to send multiple request in parallel without having to wait for a response.\nReferences: asyncio — Asynchronous I/O, Async IO in Python: A Complete Walkthrough\n\n\n\n\n\n\nAsync I/O"
  },
  {
    "objectID": "slides/03-slides.html#concurrency-and-parallelism-in-python3",
    "href": "slides/03-slides.html#concurrency-and-parallelism-in-python3",
    "title": "Lecture 3",
    "section": "Concurrency and parallelism in Python3",
    "text": "Concurrency and parallelism in Python3\n\n\n\n\nParallelism: multiple tasks are running in parallel, each on a different processors. This is done through the multiprocessing module.\nConcurrency: multiple tasks are taking turns to run on the same processor. Another task can be scheduled while the current one is blocked on I/O.\nThreading: multiple threads take turns executing tasks. One process can contain multiple threads. Similar to concurrency but within the context of a single process.\n\n\n\n\n\n\nConcurrency in Python"
  },
  {
    "objectID": "slides/03-slides.html#how-to-implement-concurrency-with-asyncio",
    "href": "slides/03-slides.html#how-to-implement-concurrency-with-asyncio",
    "title": "Lecture 3",
    "section": "How to implement concurrency with asyncio",
    "text": "How to implement concurrency with asyncio\n\n\n\n\nasyncio is a library to write concurrent code using the async/await syntax.\nUse of async and await keywords. You can call an async function multiple times while you await the result of a previous invocation.\nawait the result of multiple async tasks using gather.\nThe main function in this example is called a coroutine. Multiple coroutines can be run concurrnetly as awaitable tasks.\n\n\n\nimport asyncio\n\nasync def main():\n    print('Hello ...')\n    await asyncio.sleep(1)\n    print('... World!')\n\nasyncio.run(main())"
  },
  {
    "objectID": "slides/03-slides.html#anatomy-of-an-asyncio-python-program",
    "href": "slides/03-slides.html#anatomy-of-an-asyncio-python-program",
    "title": "Lecture 3",
    "section": "Anatomy of an asyncio Python program",
    "text": "Anatomy of an asyncio Python program\n\n\n\n\nWrite a regular Python function that makes a call to a database, an API or any other blocking functionality as you normally would.\nCreate a coroutine i.e. an async wrapper function to the blocking function using async and await, with the call to blocking function made using the asyncio.to_thread function. This enables the coroutine execution in a separate thread.\nCreate another coroutine that makes multiple calls (in a loop, list comprehension) to the async wrapper created in the previous step and awaits completion of all of the invocations using the asyncio.gather function.\nCall the coroutine created in the previous step from another function using the asyncio.run function.\n\n\n\nimport time\nimport asyncio\n\ndef my_blocking_func(i):\n    # some blocking code such as an api call\n    print(f\"{i}, entry\")\n    time.sleep(1)\n    print(f\"{i}, exiting\")\n    return None\n  \nasync def async_my_blocking_func(i: int):\n    return await asyncio.to_thread(my_blocking_func, i)\n\nasync def async_my_blocking_func_for_multiple(n: int):\n    return await asyncio.gather(*[async_my_blocking_func(i) for i in range(n)])\n\nif __name__ == \"__main__\":\n    # async version\n    s = time.perf_counter()\n    n = 20\n    brewery_counts = asyncio.run(async_my_blocking_func_for_multiple(n))\n    elapsed_async = time.perf_counter() - s\n    print(f\"{__file__}, async_my_blocking_func_for_multiple finished in {elapsed_async:0.2f} seconds\")"
  },
  {
    "objectID": "slides/05-slides.html#what-is-data-engineering",
    "href": "slides/05-slides.html#what-is-data-engineering",
    "title": "Lecture 5",
    "section": "What is Data Engineering?",
    "text": "What is Data Engineering?\nAs the scale of the data grew, the existing ETL processes alone were not sufficient, a separate discipline was needed for:\n\nCollecting data\nManaging storage\nCataloging\nMaking it available for applications such as analytics & machine learning)\nSecurity\nLifecycle management\nAnd more…\n\n\nFrom Wikipedia: Data engineering refers to the building of systems to enable the collection and usage of data. This data is usually used to enable subsequent analysis and data science; which often involves machine learning. Making the data usable usually involves substantial compute and storage, as well as data processing and cleaning."
  },
  {
    "objectID": "slides/05-slides.html#what-do-data-engineers-do",
    "href": "slides/05-slides.html#what-do-data-engineers-do",
    "title": "Lecture 5",
    "section": "What do Data Engineers do?",
    "text": "What do Data Engineers do?\nData engineers build systems that collect data from different sources and make this data available for analytics and ML application. This usually involves the following:\n\nAcquisition: Finding all the different datasets around the business.\n\nThese could be availble in databases, shared drives, ingested directly from IoT devices, external datasets, and more.\n\nCleansing: The raw data usually cannot be used as is, it needs to be cleaned.\nConversion: Since the data is coming from different sources, it would probably in different formats (database tables, CSV, JSON, custom). Needs to be converted into a common format (such as parquet) before it becomes usable.\n\nMultiple datasets need to be joined together to answer a business question."
  },
  {
    "objectID": "slides/05-slides.html#what-do-data-engineers-do-contd.",
    "href": "slides/05-slides.html#what-do-data-engineers-do-contd.",
    "title": "Lecture 5",
    "section": "What do Data Engineers do (contd.)?",
    "text": "What do Data Engineers do (contd.)?\n\nDisambiguation: How to interpret what the data means? Use a data catalog and then with the help of subject matter experts (often called Data Stewards) add meaningful description to the datasets.\nDeduplication: Having a single source of truth!\nData Governance: for how long to store the data, how to enforce access controls (Principle of least privilege) etc.\n\nOnce this is done, data may be stored in a central repository such as a data lake or data lakehouse. Data engineers may also copy and move subsets of data into a data warehouse."
  },
  {
    "objectID": "slides/05-slides.html#data-engineering-tools-technologies",
    "href": "slides/05-slides.html#data-engineering-tools-technologies",
    "title": "Lecture 5",
    "section": "Data engineering tools & technologies",
    "text": "Data engineering tools & technologies\nData engineers work with a variety of tools and technologies, including:\n\nETL Tools: ETL (extract, transform, load) tools move data between systems. They access data, then apply rules to “transform” the data through steps that make it more suitable for analysis.\nSQL: Structured Query Language (SQL) is the standard language for querying relational databases.\nPython: Python is a general programming language. Data engineers may choose to use Python for ETL tasks. Spark (pyspark) for Big Data, Apache Flink for streaming data.\nCloud Data Storage: Including Amazon S3, Azure Data Lake Storage (ADLS), Google Cloud Storage, etc.\nCloud Data Warehouses: Data ready for use by data scientists and analysts is stored in data warehouses, such as Amazon Redshift, Google BigQuery, Azure Data Warehouse, Snowflake etc."
  },
  {
    "objectID": "slides/05-slides.html#popular-data-engineering-tools",
    "href": "slides/05-slides.html#popular-data-engineering-tools",
    "title": "Lecture 5",
    "section": "Popular Data engineering tools",
    "text": "Popular Data engineering tools\n\nSource: https://www.secoda.co/blog/the-top-20-most-commonly-used-data-engineering-tools"
  },
  {
    "objectID": "slides/05-slides.html#data-lakes",
    "href": "slides/05-slides.html#data-lakes",
    "title": "Lecture 5",
    "section": "Data Lakes",
    "text": "Data Lakes\n\n\nFrom Wikipedia: A data lake is a system or repository of data stored in its natural/raw format, usually object blobs or files. A data lake is usually a single store of data including raw copies of source system data, sensor data, social data etc., and transformed data used for tasks such as reporting, visualization, advanced analytics and machine learning. A data lake can include structured data from relational databases (rows and columns), semi-structured data (CSV, logs, XML, JSON), unstructured data (emails, documents, PDFs) and binary data (images, audio, video)."
  },
  {
    "objectID": "slides/05-slides.html#data-lake-cloud-provider-definitions",
    "href": "slides/05-slides.html#data-lake-cloud-provider-definitions",
    "title": "Lecture 5",
    "section": "Data Lake (Cloud Provider Definitions)",
    "text": "Data Lake (Cloud Provider Definitions)\nDefinitions in the wild (emphasis mine).\n\n\nAWS\nA data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. You can store your data as-is, without having to first structure the data, and run different types of analytics—from dashboards and visualizations to big data processing, real-time analytics, and machine learning to guide better decisions.\n\nGCP\nA data lake provides a scalable and secure platform that allows enterprises to: ingest any data from any system at any speed—even if the data comes from on-premises, cloud, or edge-computing systems; store any type or volume of data in full fidelity; process data in real time or batch mode; and analyze data using SQL, Python, R, or any other language, third-party data, or analytics application.\n\nAzure\nAzure Data Lake includes all the capabilities required to make it easy for developers, data scientists, and analysts to store data of any size, shape, and speed, and do all types of processing and analytics across platforms and languages.\n\nDataBricks\nA data lake is a central location that holds a large amount of data in its native, raw format. Compared to a hierarchical data warehouse, which stores data in files or folders, a data lake uses a flat architecture and object storage to store the data.‍"
  },
  {
    "objectID": "slides/05-slides.html#working-with-a-cloud-data-lake",
    "href": "slides/05-slides.html#working-with-a-cloud-data-lake",
    "title": "Lecture 5",
    "section": "Working with a Cloud Data Lake",
    "text": "Working with a Cloud Data Lake\nA cloud data lake is setup using the cloud provider’s object store (S3, GCS, Azure Blob Storage).\n\nThe object stores are extremely scalable, for context, the maximum size of an object in S3 is 5 TB and there is no limit to number of objects in an S3 bucket.\nThey are extremely duarable, 99.999999999%. Provide strong consistency (read-after-write, listing buckets and objects, granting permissions etc.).\nCost effective, with multiple storage classes.\nIntegration with data processing tools such as Spark, machine learning tools such as SageMaker, data warehouses such as RedShift and data cataloging tools.\nThey provide Fine Grained Access Control to the data."
  },
  {
    "objectID": "slides/05-slides.html#data-warehouses",
    "href": "slides/05-slides.html#data-warehouses",
    "title": "Lecture 5",
    "section": "Data Warehouses",
    "text": "Data Warehouses\n\n\nFrom Wikipedia: In computing, a data warehouse (DW or DWH), also known as an enterprise data warehouse (EDW), is a system used for reporting and data analysis and is considered a core component of business intelligence. DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data in one single place that are used for creating analytical reports for workers throughout the enterprise."
  },
  {
    "objectID": "slides/05-slides.html#data-warehouse-cloud-provider-definitions",
    "href": "slides/05-slides.html#data-warehouse-cloud-provider-definitions",
    "title": "Lecture 5",
    "section": "Data Warehouse (Cloud Provider Definitions)",
    "text": "Data Warehouse (Cloud Provider Definitions)\nDefinitions in the wild (emphasis mine).\n\n\nAWS\nA data warehouse is a central repository of information that can be analyzed to make more informed decisions. Data flows into a data warehouse from transactional systems, relational databases, and other sources, typically on a regular cadence. Business analysts, data engineers, data scientists, and decision makers access the data through business intelligence (BI) tools, SQL clients, and other analytics applications.\nAzure\nA data warehouse is a centralized repository of integrated data from one or more disparate sources. Data warehouses store current and historical data and are used for reporting and analysis of the data.\n\nGCP\nA data warehouse is an enterprise system used for the analysis and reporting of structured and semi-structured data from multiple sources, such as point-of-sale transactions, marketing automation, customer relationship management, and more.\nSnowflake\nA data warehouse (DW) is a relational database that is designed for analytical rather than transactional work. It collects and aggregates data from one or many sources so it can be analyzed to produce business insights."
  },
  {
    "objectID": "slides/05-slides.html#working-with-a-cloud-data-warehouse",
    "href": "slides/05-slides.html#working-with-a-cloud-data-warehouse",
    "title": "Lecture 5",
    "section": "Working with a Cloud Data Warehouse",
    "text": "Working with a Cloud Data Warehouse\nAll cloud providers provide a data warehouse solution that works in conjunction with their data lake solution.\n\nAWS has Redshift, Azure has Synapse Analytics. GCP has BigQuery and then there is Snowflake.\nIn a data warehouse, Online analytical processing (OLAP) allows for fast querying and analysis of data from different perspectives. It also helps in pre-aggregating and pre-calculating the information available in the archive.\nData warehouses are Peta Byte scale (Amazon RedShift, Google BigQuery, Azure Synapse Analytics).\nData warehouses can have dedicated compute provisioned or be serverless (BigQuery is serverless, Redshift allows both options now).\nData warehouses now offer integrated ML capabilities, you can build models with SQL and use them in queries (Amazon RedShift ML, Google BigQuery ML, Azure Synapse Analytics ML).\nIntegration with reporting and dashboarding tools such as Tableau, Grafana, Looker etc. and data analytics tools such as Spark and data cataloging tools.\nThey provide Fine Grained Access Control to the data."
  },
  {
    "objectID": "slides/05-slides.html#combining-data-lakes-and-data-warehouses",
    "href": "slides/05-slides.html#combining-data-lakes-and-data-warehouses",
    "title": "Lecture 5",
    "section": "Combining Data Lakes and Data Warehouses",
    "text": "Combining Data Lakes and Data Warehouses\nCombine the flexibility, cost-efficiency, and scale of data lakes with the data management and ACID transactions of data warehouses to provide a single architecture that can enable business intelligence and machine learning on all data.\n\nBuild a lakehouse architecture on AWS\nThe Databricks lakehouse platform\nOpen data lakehouse on Google Cloud\nThe data lakehouse, the data warehouse and a modern data platform on Azure"
  },
  {
    "objectID": "slides/05-slides.html#nosql-databases",
    "href": "slides/05-slides.html#nosql-databases",
    "title": "Lecture 5",
    "section": "NoSQL Databases",
    "text": "NoSQL Databases\nAt some point we needed to think beyond relation databases, because: - Data became more and more complex (not all data is tabular, thinkin JSON data emitted by an IoT device). - Cost of storage decreased (everything did not need to be stored in the 3rd normal form) - More data on the cloud meant data needed to be placed across different servers (scale-out) - Data needed to be placed intelligently in geo locations of interest - And more…\n\nFrom Wikipedia: A NoSQL (originally referring to “non-SQL” or “non-relational”) database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases. Such databases have existed since the late 1960s, but the name “NoSQL” was only coined in the early 21st century, triggered by the needs of Web 2.0 companies. NoSQL databases are increasingly used in big data and real-time web applications. NoSQL systems are also sometimes called Not only SQL to emphasize that they may support SQL-like query languages or sit alongside SQL databases in polyglot-persistent architectures."
  },
  {
    "objectID": "slides/05-slides.html#types-of-nosql-databases",
    "href": "slides/05-slides.html#types-of-nosql-databases",
    "title": "Lecture 5",
    "section": "Types of NoSQL Databases",
    "text": "Types of NoSQL Databases\nOver time, four major types of NoSQL databases emerged: document databases, key-value databases, wide-column stores, and graph databases.\n\nDocument databases store data in documents similar to JSON (JavaScript Object Notation) objects. Each document contains pairs of fields and values. The values can typically be a variety of types including things like strings, numbers, booleans, arrays, or objects.\nKey-value databases are a simpler type of database where each item contains keys and values.\nWide-column stores store data in tables, rows, and dynamic columns.\nGraph databases store data in nodes and edges. Nodes typically store information about people, places, and things, while edges store information about the relationships between the nodes."
  },
  {
    "objectID": "slides/05-slides.html#examples-of-nosql-databases",
    "href": "slides/05-slides.html#examples-of-nosql-databases",
    "title": "Lecture 5",
    "section": "Examples of NoSQL Databases",
    "text": "Examples of NoSQL Databases\n\n\n\n\n\n\n\nNOSQL Database Type\nExamples\n\n\n\n\nDocument Database\nAmazon DocumentDB, MongoDB, Cosmos DB, ArangoDB, Couchbase Server, CouchDB\n\n\nKey-value Database\nAmazon DynamoDB, Couchbase, Memcached, Redis\n\n\nWide-column datastores\nAmazon DynamoDB, Apache Cassandra, Google Bigtable, Azure Tables\n\n\nGraph databases\nAmazon Neptune, Neo4j\n\n\n\nAs a data scientist you would work with a NoSQL database through an SDK/API. Several programming languages are supported including Python, Java, Go, C++ etc."
  },
  {
    "objectID": "slides/05-slides.html#example-of-documents-in-a-document-database",
    "href": "slides/05-slides.html#example-of-documents-in-a-document-database",
    "title": "Lecture 5",
    "section": "Example of documents in a document database",
    "text": "Example of documents in a document database\nHere is an example of a document inserted in a key-value/document database such as MongoDB.\n```{bash}\n{\n   \"name\" : \"IS 350\",\n   \"year\" : \"2015\",\n   \"make\" : \"Lexus\",\n   \"colors\" : [\"black\",\"white\",\"grey\"],\n   \"spec\" : {\n      \"engine\" : \"V6\",\n      \"wheelbase\" : \"110.2 in\",\n      \"length\" : \"183.7 in\"\n   }\n}\n```\nThe same example can be inserted in an Amazon DynamoDB table called (say) Cars.\n```{python}\ndatabase = boto3.resource('dynamodb')\ntable = database.Table('cars')\nitem = {\n   \"name\" : \"IS 350\",\n   \"year\" : \"2015\",\n   \"make\" : \"Lexus\",\n   \"colors\" : [\"black\",\"white\",\"grey\"],\n   \"spec\" : {\n      \"engine\" : \"V6\",\n      \"wheelbase\" : \"110.2 in\",\n      \"length\" : \"183.7 in\"\n   }\n}\ntable.put_item(Item = item)\n```"
  },
  {
    "objectID": "slides/05-slides.html#other-data-stores-to-know-about",
    "href": "slides/05-slides.html#other-data-stores-to-know-about",
    "title": "Lecture 5",
    "section": "Other Data stores to know about",
    "text": "Other Data stores to know about\nBesides the general concepts about data lkakes, warehouses, different types of databases, there are some purpose built databases that are good to know about.\n\nSplunk\nElasticsearch\nDuckDB\nMany many more…"
  },
  {
    "objectID": "slides/05-slides.html#splunk",
    "href": "slides/05-slides.html#splunk",
    "title": "Lecture 5",
    "section": "Splunk",
    "text": "Splunk\nSplunk is a software platform to search, analyze and visualize the machine-generated data gathered from the websites, applications, sensors, devices etc. which make up your IT infrastructure and business. See https://www.splunk.com/.\n\nThe logfile we analyzed in assignment 4 s3://bigdatateaching/forensicswiki/2012_logs.txt, is the typical kind of data that gets ingested into Splunk.\n\n\n\n\nImage courtsey: https://docs.splunk.com/Documentation/Splunk/9.0.2/SearchTutorial/Aboutthesearchapp"
  },
  {
    "objectID": "slides/05-slides.html#elasticsearch",
    "href": "slides/05-slides.html#elasticsearch",
    "title": "Lecture 5",
    "section": "Elasticsearch",
    "text": "Elasticsearch\nFrom https://www.elastic.co/what-is/elasticsearch, emphasis mine\nElasticsearch is a distributed, free and open search and analytics engine for all types of data, including textual, numerical, geospatial, structured, and unstructured. Elasticsearch is built on Apache Lucene and was first released in 2010 by Elasticsearch N.V. (now known as Elastic).\nCommonly referred to as the ELK Stack (after Elasticsearch, Logstash, and Kibana), the Elastic Stack now includes a rich collection of lightweight shipping agents known as Beats for sending data to Elasticsearch.\nData is inserted in Elasticsearch indexes as JSON documents using a REST API/SDK.\n\n\n\nImage courtsey: https://www.elastic.co/kibana"
  },
  {
    "objectID": "slides/05-slides.html#duckdb",
    "href": "slides/05-slides.html#duckdb",
    "title": "Lecture 5",
    "section": "DuckDB",
    "text": "DuckDB\nDuckDB is an in-process SQL OLAP database management system.\n\nIt is like sqllite, but for analytics. What does this mean? It means that your database runs inside your process, there are no servers to manage, no remote system to connect to. Very snappy, easy to experiment with SQL like syntax.\n\nDuckDB does vectorized processing i.e. loads chunks of data into memory (tries to keep everything in the CPU’s L1 and L2 cache) and is thus able to handle datasets bigger than the amount of RAM available.\nDuckDB supports Python, R and a host of other languages.\nWhen to use DuckDB: when you cannot use Pandas for rapid experimentation with Big Data, especially when combining DuckDB with Arrow.\nWhen not to use DuckDB: DuckDB has a valuable but very niche use-case.\n\n\nPaper on DuckDB by Hannes Mühleisen & Mark Raasveldt\nDuckDB: an Embeddable Analytical Database https://hannes.muehleisen.org/publications/SIGMOD2019-demo-duckdb.pdf"
  },
  {
    "objectID": "slides/05-slides.html#further-reading",
    "href": "slides/05-slides.html#further-reading",
    "title": "Lecture 5",
    "section": "Further Reading",
    "text": "Further Reading\nPlease lookup these topics on Google for further reading. Not providing specific links here because they all point to vendor specific products.\n\nData Catalog\nData Governance\nData Mesh\nData Fabric"
  },
  {
    "objectID": "slides/05-slides.html#lab-analyzing-nyc-taxi-dataset-with-athena",
    "href": "slides/05-slides.html#lab-analyzing-nyc-taxi-dataset-with-athena",
    "title": "Lecture 5",
    "section": "Lab: Analyzing NYC-Taxi dataset with Athena",
    "text": "Lab: Analyzing NYC-Taxi dataset with Athena"
  },
  {
    "objectID": "slides/05-slides.html#a-simple-example-of-using-duckdb-and-apache-arrow-using-nyc-taxi-dataset",
    "href": "slides/05-slides.html#a-simple-example-of-using-duckdb-and-apache-arrow-using-nyc-taxi-dataset",
    "title": "Lecture 5",
    "section": "A simple example of using DuckDB and Apache Arrow using NYC Taxi dataset",
    "text": "A simple example of using DuckDB and Apache Arrow using NYC Taxi dataset\nThis notebook reads the NYC taxi dataset files for the first 6 months of the year 2023 (about ~20 million rows) and runs some analytics operation on this dataset. This dataset is too big to fit into memory.\nGitHub Classroom Link for lab\nGitHub Classroom Link for lab"
  },
  {
    "objectID": "slides/07-slides.html#agenda-and-goals-for-today",
    "href": "slides/07-slides.html#agenda-and-goals-for-today",
    "title": "Lecture 7",
    "section": "Agenda and Goals for Today",
    "text": "Agenda and Goals for Today\n\n\nLecture\n\nSpark Diagnostics\nSpark UDFs\nProject\n\n\nLab\n\nSpark on Sagemaker\nSpark DataFrames\nSpark UDFs"
  },
  {
    "objectID": "slides/07-slides.html#logistics-and-review",
    "href": "slides/07-slides.html#logistics-and-review",
    "title": "Lecture 7",
    "section": "Logistics and Review",
    "text": "Logistics and Review\n\n\nDeadlines\n\n~~Assignment 5: DuckDB & Polars ~~\n~~Lab 6: Intro to Spark ~~\nLab 7: Spark DataFrames\nLab 8: SparkNLP\nAssignment 6: Spark (Multi-part)\nLab 9: SparkML\nLab 10: Spark Streaming\nLab 11: Dask\nLab 12: Ray\n\n\nLook back and ahead\n\nSearching Slack for existing Q&A - like StackOverflow!\nSpark RDDs, Spark DataFrames\nNow: More Spark and Project\nNext week: SparkNLP and more Project"
  },
  {
    "objectID": "slides/07-slides.html#connected-and-extensible",
    "href": "slides/07-slides.html#connected-and-extensible",
    "title": "Lecture 7",
    "section": "Connected and extensible",
    "text": "Connected and extensible"
  },
  {
    "objectID": "slides/07-slides.html#caching-and-persistence",
    "href": "slides/07-slides.html#caching-and-persistence",
    "title": "Lecture 7",
    "section": "Caching and Persistence",
    "text": "Caching and Persistence\nBy default, RDDs are recomputed every time you run an action on them. This can be expensive (in time) if you need to use a dataset more than once.\nSpark allows you to control what is cached in memory.\nTo tell spark to cache an object in memory, use persist() or cache():\n\ncache(): is a shortcut for using default storage level, which is memory only\npersist(): can be customized to other ways to persist data (including both memory and/or disk)\n\n```{python}\n# caches error RDD in memory, but only after an action is run\nerrors = logs.filter(lambda x: \"error\" in x and \"2019-12\" in x).cache()\n```"
  },
  {
    "objectID": "slides/07-slides.html#review-of-pysparksql-cheatsheet",
    "href": "slides/07-slides.html#review-of-pysparksql-cheatsheet",
    "title": "Lecture 7",
    "section": "Review of PySparkSQL Cheatsheet",
    "text": "Review of PySparkSQL Cheatsheet\nhttps://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf"
  },
  {
    "objectID": "slides/07-slides.html#collect-caution",
    "href": "slides/07-slides.html#collect-caution",
    "title": "Lecture 7",
    "section": "collect CAUTION",
    "text": "collect CAUTION"
  },
  {
    "objectID": "slides/07-slides.html#review-of-htop",
    "href": "slides/07-slides.html#review-of-htop",
    "title": "Lecture 7",
    "section": "Review of htop",
    "text": "Review of htop"
  },
  {
    "objectID": "slides/07-slides.html#htop-top-section-explanation",
    "href": "slides/07-slides.html#htop-top-section-explanation",
    "title": "Lecture 7",
    "section": "htop top section explanation",
    "text": "htop top section explanation"
  },
  {
    "objectID": "slides/07-slides.html#htop-bottom-section-explanation",
    "href": "slides/07-slides.html#htop-bottom-section-explanation",
    "title": "Lecture 7",
    "section": "htop bottom section explanation",
    "text": "htop bottom section explanation\n\n\n\nhttps://codeahoy.com/2017/01/20/hhtop-explained-visually/"
  },
  {
    "objectID": "slides/07-slides.html#understanding-how-the-cluster-is-running-your-job",
    "href": "slides/07-slides.html#understanding-how-the-cluster-is-running-your-job",
    "title": "Lecture 7",
    "section": "Understanding how the cluster is running your job",
    "text": "Understanding how the cluster is running your job\nSpark Application UI shows important facts about you Spark job:\n\nEvent timeline for each stage of your work\nDirected acyclical graph (DAG) of your job\nSpark job history\nStatus of Spark executors\nPhysical / logical plans for any SQL queries\n\nTool to confirm you are getting the horizontal scaling that you need!\nAdapted from AWS Glue Spark UI docs and Spark UI docs"
  },
  {
    "objectID": "slides/07-slides.html#spark-ui---event-timeline",
    "href": "slides/07-slides.html#spark-ui---event-timeline",
    "title": "Lecture 7",
    "section": "Spark UI - Event timeline",
    "text": "Spark UI - Event timeline"
  },
  {
    "objectID": "slides/07-slides.html#spark-ui---dag",
    "href": "slides/07-slides.html#spark-ui---dag",
    "title": "Lecture 7",
    "section": "Spark UI - DAG",
    "text": "Spark UI - DAG"
  },
  {
    "objectID": "slides/07-slides.html#spark-ui---job-history",
    "href": "slides/07-slides.html#spark-ui---job-history",
    "title": "Lecture 7",
    "section": "Spark UI - Job History",
    "text": "Spark UI - Job History"
  },
  {
    "objectID": "slides/07-slides.html#spark-ui---executors",
    "href": "slides/07-slides.html#spark-ui---executors",
    "title": "Lecture 7",
    "section": "Spark UI - Executors",
    "text": "Spark UI - Executors"
  },
  {
    "objectID": "slides/07-slides.html#spark-ui---sql",
    "href": "slides/07-slides.html#spark-ui---sql",
    "title": "Lecture 7",
    "section": "Spark UI - SQL",
    "text": "Spark UI - SQL"
  },
  {
    "objectID": "slides/07-slides.html#udf-workflow",
    "href": "slides/07-slides.html#udf-workflow",
    "title": "Lecture 7",
    "section": "UDF Workflow",
    "text": "UDF Workflow"
  },
  {
    "objectID": "slides/07-slides.html#udf-code-structure",
    "href": "slides/07-slides.html#udf-code-structure",
    "title": "Lecture 7",
    "section": "UDF Code Structure",
    "text": "UDF Code Structure\nClear input - a single row of data with one or more columns used\nFunction - some work written in python that process the input using python syntax. No PySpark needed!\nClear output - output with a scoped data type"
  },
  {
    "objectID": "slides/07-slides.html#udf-example",
    "href": "slides/07-slides.html#udf-example",
    "title": "Lecture 7",
    "section": "UDF Example",
    "text": "UDF Example\nProblem: make a new column with ages for adults-only\n+-------+--------------+\n|room_id|   guests_ages|\n+-------+--------------+\n|      1|  [18, 19, 17]|\n|      2|   [25, 27, 5]|\n|      3|[34, 38, 8, 7]|\n+-------+--------------+\nAdapted from UDFs in Spark"
  },
  {
    "objectID": "slides/07-slides.html#udf-code-solution",
    "href": "slides/07-slides.html#udf-code-solution",
    "title": "Lecture 7",
    "section": "UDF Code Solution",
    "text": "UDF Code Solution\n```{python}\nfrom pyspark.sql.functions import udf, col\n\n@udf(\"array&lt;integer&gt;\")\n   def filter_adults(elements):\n   return list(filter(lambda x: x &gt;= 18, elements))\n\n# alternatively\nfrom pyspark.sql.types IntegerType, ArrayType\n@udf(returnType=ArrayType(IntegerType()))\ndef filter_adults(elements):\n   return list(filter(lambda x: x &gt;= 18, elements))\n```\n+-------+----------------+------------+\n|room_id| guests_ages    | adults_ages|\n+-------+----------------+------------+\n| 1     | [18, 19, 17]   |    [18, 19]|\n| 2     | [25, 27, 5]    |    [25, 27]|\n| 3     | [34, 38, 8, 7] |    [34, 38]|\n| 4     |[56, 49, 18, 17]|[56, 49, 18]|\n+-------+----------------+------------+"
  },
  {
    "objectID": "slides/07-slides.html#alternative-to-spark-udf",
    "href": "slides/07-slides.html#alternative-to-spark-udf",
    "title": "Lecture 7",
    "section": "Alternative to Spark UDF",
    "text": "Alternative to Spark UDF\n```{python}\n# Spark 3.1\nfrom pyspark.sql.functions import col, filter, lit\n\ndf.withColumn('adults_ages',\n              filter(col('guests_ages'), lambda x: x &gt;= lit(18))).show()\n```"
  },
  {
    "objectID": "slides/07-slides.html#another-udf-example",
    "href": "slides/07-slides.html#another-udf-example",
    "title": "Lecture 7",
    "section": "Another UDF Example",
    "text": "Another UDF Example\n\nSeparate function definition form\n\n```{python}\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import LongType\n\n# define the function that can be tested locally\ndef squared(s):\n  return s * s\n\n# wrap the function in udf for spark and define the output type\nsquared_udf = udf(squared, LongType())\n\n# execute the udf\ndf = spark.table(\"test\")\ndisplay(df.select(\"id\", squared_udf(\"id\").alias(\"id_squared\")))\n```\n\nSingle function definition form\n\n```{python}\nfrom pyspark.sql.functions import udf\n@udf(\"long\")\ndef squared_udf(s):\n  return s * s\ndf = spark.table(\"test\")\ndisplay(df.select(\"id\", squared_udf(\"id\").alias(\"id_squared\")))\n```"
  },
  {
    "objectID": "slides/07-slides.html#can-also-refer-to-a-udf-in-sql",
    "href": "slides/07-slides.html#can-also-refer-to-a-udf-in-sql",
    "title": "Lecture 7",
    "section": "Can also refer to a UDF in SQL",
    "text": "Can also refer to a UDF in SQL\n```{sql}\nspark.udf.register(\"squaredWithPython\", squared)\nselect id, squaredWithPython(id) as id_squared from test\n```\n\nConsider all the corner cases\nWhere could the data be null or an unexpected value\nLeverage python control structure to handle corner cases\n\nsource"
  },
  {
    "objectID": "slides/07-slides.html#udf-speed-comparison",
    "href": "slides/07-slides.html#udf-speed-comparison",
    "title": "Lecture 7",
    "section": "UDF Speed Comparison",
    "text": "UDF Speed Comparison\n\n\n\nCosts:\n\nSerialization/deserialization (think pickle files)\nData movement between JVM and Python\nLess Spark optimization possible\n\nOther ways to make your Spark jobs faster source:\n\nCache/persist your data into memory\nUsing Spark DataFrames over Spark RDDs\nUsing Spark SQL functions before jumping into UDFs\nSave to serialized data formats like Parquet"
  },
  {
    "objectID": "slides/07-slides.html#pandas-udf",
    "href": "slides/07-slides.html#pandas-udf",
    "title": "Lecture 7",
    "section": "Pandas UDF",
    "text": "Pandas UDF\nFrom PySpark docs - Pandas UDFs are user defined functions that are executed by Spark using Arrow to transfer data and Pandas to work with the data, which allows vectorized operations. A Pandas UDF is defined using the pandas_udf as a decorator or to wrap the function, and no additional configuration is required. A Pandas UDF behaves as a regular PySpark function API in general.\n```{python}\n@pandas_udf(\"string\")\ndef to_upper(s: pd.Series) -&gt; pd.Series:\n    return s.str.upper()\n\ndf = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\ndf.select(to_upper(\"name\")).show()\n+--------------+\n|to_upper(name)|\n+--------------+\n|      JOHN DOE|\n+--------------+\n```"
  },
  {
    "objectID": "slides/07-slides.html#another-example",
    "href": "slides/07-slides.html#another-example",
    "title": "Lecture 7",
    "section": "Another example",
    "text": "Another example\n```{python}\n@pandas_udf(\"first string, last string\")\ndef split_expand(s: pd.Series) -&gt; pd.DataFrame:\n    return s.str.split(expand=True)\n\n\ndf = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\ndf.select(split_expand(\"name\")).show()\n+------------------+\n|split_expand(name)|\n+------------------+\n|       [John, Doe]|\n+------------------+\n```\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pandas_udf.html"
  },
  {
    "objectID": "slides/07-slides.html#scalar-pandas-udfs",
    "href": "slides/07-slides.html#scalar-pandas-udfs",
    "title": "Lecture 7",
    "section": "Scalar Pandas UDFs",
    "text": "Scalar Pandas UDFs\n\nVectorizing scalar operations - one plus one\nPandas UDF needs to have same size input and output series\n\nUDF Form\n```{python}\nfrom pyspark.sql.functions import udf\n\n# Use udf to define a row-at-a-time udf\n@udf('double')\n# Input/output are both a single double value\ndef plus_one(v):\n      return v + 1\n\ndf.withColumn('v2', plus_one(df.v))\n```\nPandas UDF Form - faster vectorized form\n```{python}\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\n\n# Use pandas_udf to define a Pandas UDF\n@pandas_udf('double', PandasUDFType.SCALAR)\n# Input/output are both a pandas.Series of doubles\n\ndef pandas_plus_one(v):\n    return v + 1\n\ndf.withColumn('v2', pandas_plus_one(df.v))\n```\nsource"
  },
  {
    "objectID": "slides/07-slides.html#grouped-map-pandas-udfs",
    "href": "slides/07-slides.html#grouped-map-pandas-udfs",
    "title": "Lecture 7",
    "section": "Grouped Map Pandas UDFs",
    "text": "Grouped Map Pandas UDFs\n\nSplit, apply, combine using Pandas syntax\n\n```{python}\n@pandas_udf(df.schema, PandasUDFType.GROUPED_MAP)\n# Input/output are both a pandas.DataFrame\ndef subtract_mean(pdf):\n    return pdf.assign(v=pdf.v - pdf.v.mean())\n\ndf.groupby('id').apply(subtract_mean)\n```"
  },
  {
    "objectID": "slides/07-slides.html#comparison-of-scalar-and-grouped-map-pandas-udfs",
    "href": "slides/07-slides.html#comparison-of-scalar-and-grouped-map-pandas-udfs",
    "title": "Lecture 7",
    "section": "Comparison of Scalar and Grouped Map Pandas UDFs",
    "text": "Comparison of Scalar and Grouped Map Pandas UDFs\n\n\nInput of the user-defined function:\n\nScalar: pandas.Series\nGrouped map: pandas.DataFrame\n\nOutput of the user-defined function:\n\nScalar: pandas.Series\nGrouped map: pandas.DataFrame\n\nGrouping semantics:\n\nScalar: no grouping semantics\nGrouped map: defined by “groupby” clause\n\nOutput size:\n\nScalar: same as input size\nGrouped map: any size"
  },
  {
    "objectID": "slides/07-slides.html#reddit-data",
    "href": "slides/07-slides.html#reddit-data",
    "title": "Lecture 7",
    "section": "Reddit Data!",
    "text": "Reddit Data!"
  },
  {
    "objectID": "slides/07-slides.html#reddit-big-data",
    "href": "slides/07-slides.html#reddit-big-data",
    "title": "Lecture 7",
    "section": "Reddit BIG data",
    "text": "Reddit BIG data\nData from 2021-01 to 2023-03\nComments 71,786 files (10MB blobs), 749GB, 6,114,480,451 comments (original 7.7TB)\nSubmissions 26186 files (10MB blobs), 268GB, 892,832,344 submissions (original 3.2TB)"
  },
  {
    "objectID": "slides/07-slides.html#reddit-background",
    "href": "slides/07-slides.html#reddit-background",
    "title": "Lecture 7",
    "section": "Reddit Background",
    "text": "Reddit Background"
  },
  {
    "objectID": "slides/07-slides.html#comparison-of-social-data-infrastructure",
    "href": "slides/07-slides.html#comparison-of-social-data-infrastructure",
    "title": "Lecture 7",
    "section": "Comparison of social data infrastructure",
    "text": "Comparison of social data infrastructure"
  },
  {
    "objectID": "slides/07-slides.html#data-dictionary",
    "href": "slides/07-slides.html#data-dictionary",
    "title": "Lecture 7",
    "section": "Data Dictionary",
    "text": "Data Dictionary"
  },
  {
    "objectID": "slides/07-slides.html#timeline",
    "href": "slides/07-slides.html#timeline",
    "title": "Lecture 7",
    "section": "Timeline",
    "text": "Timeline\nDetailed requirements will be published in the next week\n\n\n\nDeliverable\nDeadline\n\n\n\n\nProject EDA Milestone\n2023-11-06 11:59pm\n\n\nProject NLP Milestone\n2023-11-20 11:59pm\n\n\nProject Peer Feedback\n2023-11-20 11:59pm\n\n\nProject ML Milestone\n2023-11-30 11:59pm\n\n\nFinal Project Milestone\n2023-12-08 11:59pm"
  },
  {
    "objectID": "slides/07-slides.html#deliverables",
    "href": "slides/07-slides.html#deliverables",
    "title": "Lecture 7",
    "section": "Deliverables",
    "text": "Deliverables\n\nEDA - project plan, initial data exploration, summary graphs and tables, some data transformation\nNLP - external dataset merging, more data transformation, leverage an NLP model\nPeer feedback - review another group’s projet and provide constructive feedback on their EDA milestone.\nML - build several ML models, compare performance, answer some interesting questions\nFinal delivery - complete output that takes into account feedback given by instructors and peers for your project, improved analysis and work from intermediate deliverables"
  },
  {
    "objectID": "slides/07-slides.html#next-steps",
    "href": "slides/07-slides.html#next-steps",
    "title": "Lecture 7",
    "section": "Next Steps",
    "text": "Next Steps\n\nForm into groups of 3-4 students. Register on Canvas by 10/10\nDiscuss potential interesting questions with your group\nConsider what type of split to the data you will want, do you want to look at a specific subreddit, time period, etc.\nThink about potential external data to merge with your Reddit data\nMore to come next week!"
  },
  {
    "objectID": "slides/09-slides.html#aws-academy",
    "href": "slides/09-slides.html#aws-academy",
    "title": "Lecture 9",
    "section": "AWS Academy",
    "text": "AWS Academy\n\nCredit limit - $100\nCourse numbers:\n\nCourse #1 - 24178\nCourse #2 - 27354\nCourse #3 - 22802\nCourse #4 - 26418\n\n\nSTAY WITH COURSE 24178 UNLESS YOU HAVE RUN OUT OF CREDITS OR &gt;$90 USED!\nNote that you will have to repeat several setup steps:\n\nsecurity group\nEC2 keypair uploading (the AWS part only)\nany S3 uploading or copying as well as bucket creation as necessary\nEMR configuration"
  },
  {
    "objectID": "slides/09-slides.html#review-of-file-systems",
    "href": "slides/09-slides.html#review-of-file-systems",
    "title": "Lecture 9",
    "section": "Review of File Systems",
    "text": "Review of File Systems\nWhat are the possible file system options for each item: S3, HDFS, Local file system\n\nhadoop jar /usr/lib/hadoop/hadoop-streaming.jar #1\n\n\n\n-files basic-mapper.py,basic-reducer.py #2\n\n\n\n-input /user/hadoop/in_data #3\n-output /user/hadoop/in_data #3\n\n\n\n-mapper basic-mapper.py #4\n-reducer basic-reducer.py #4\n\n\n\nLocal file system\n\n\n\n\nLocal file system/S3/ OR HDFS – UPDATE\n\n\n\n\nHDFS or S3\n\n\n\n\nUPDATE – It’s complicated - “The -files option creates a symlink in the current working directory of the tasks that points to the local copy of the file.” Location of #4 can be:\n\n\nSymlinked file (one you passed with -files) in the pwd in distributed containers\nExecutable path (local to a worker) that is consistent across all workers on the cluster"
  },
  {
    "objectID": "slides/09-slides.html#connected-and-extensible",
    "href": "slides/09-slides.html#connected-and-extensible",
    "title": "Lecture 9",
    "section": "Connected and extensible",
    "text": "Connected and extensible"
  },
  {
    "objectID": "slides/09-slides.html#caching-and-persistence",
    "href": "slides/09-slides.html#caching-and-persistence",
    "title": "Lecture 9",
    "section": "Caching and Persistence",
    "text": "Caching and Persistence\nBy default, RDDs are recomputed every time you run an action on them. This can be expensive (in time) if you need to use a dataset more than once.\nSpark allows you to control what is cached in memory.\nTo tell spark to cache an object in memory, use persist() or cache():\n\ncache(): is a shortcut for using default storage level, which is memory only\npersist(): can be customized to other ways to persist data (including both memory and/or disk)\n\n# caches error RDD in memory, but only after an action is run\nerrors = logs.filter(lambda x: \"error\" in x and \"2019-12\" in x).cache()"
  },
  {
    "objectID": "slides/09-slides.html#collect-caution",
    "href": "slides/09-slides.html#collect-caution",
    "title": "Lecture 9",
    "section": "collect CAUTION",
    "text": "collect CAUTION"
  },
  {
    "objectID": "slides/09-slides.html#spark-ui---executors",
    "href": "slides/09-slides.html#spark-ui---executors",
    "title": "Lecture 9",
    "section": "Spark UI - Executors",
    "text": "Spark UI - Executors"
  },
  {
    "objectID": "slides/09-slides.html#udf-speed-comparison",
    "href": "slides/09-slides.html#udf-speed-comparison",
    "title": "Lecture 9",
    "section": "UDF Speed Comparison",
    "text": "UDF Speed Comparison\n\n\n\nCosts:\n\nSerialization/deserialization (think pickle files)\nData movement between JVM and Python\nLess Spark optimization possible\n\nOther ways to make your Spark jobs faster source:\n\nCache/persist your data into memory\nUsing Spark DataFrames over Spark RDDs\nUsing Spark SQL functions before jumping into UDFs\nSave to serialized data formats like Parquet"
  },
  {
    "objectID": "slides/09-slides.html#transformers",
    "href": "slides/09-slides.html#transformers",
    "title": "Lecture 9",
    "section": "Transformers",
    "text": "Transformers\n\nTransformers take DataFrames as input, and return a new DataFrame as output. Transformers do not learn any parameters from the data, they simply apply rule-based transformations to either prepare data for model training or generate predictions using a trained model.\nTransformers are run using the .transform() method"
  },
  {
    "objectID": "slides/09-slides.html#estimators",
    "href": "slides/09-slides.html#estimators",
    "title": "Lecture 9",
    "section": "Estimators",
    "text": "Estimators\n\nEstimators learn (or “fit”) parameters from your DataFrame via the .fit() method, and return a model which is a Transformer"
  },
  {
    "objectID": "slides/09-slides.html#pipelines",
    "href": "slides/09-slides.html#pipelines",
    "title": "Lecture 9",
    "section": "Pipelines",
    "text": "Pipelines"
  },
  {
    "objectID": "slides/09-slides.html#pipelines-1",
    "href": "slides/09-slides.html#pipelines-1",
    "title": "Lecture 9",
    "section": "Pipelines",
    "text": "Pipelines\n\nPipelines combine multiple steps into a single workflow that can be easily run. * Data cleaning and feature processing via transformers, using stages * Model definition * Run the pipeline to do all transformations and fit the model\nThe Pipeline constructor takes an array of pipeline stages"
  },
  {
    "objectID": "slides/09-slides.html#why-pipelines",
    "href": "slides/09-slides.html#why-pipelines",
    "title": "Lecture 9",
    "section": "Why Pipelines?",
    "text": "Why Pipelines?\n\nCleaner Code: Accounting for data at each step of preprocessing can get messy. With a Pipeline, you won’t need to manually keep track of your training and validation data at each step.\nFewer Bugs: There are fewer opportunities to misapply a step or forget a preprocessing step.\nEasier to Productionize: It can be surprisingly hard to transition a model from a prototype to something deployable at scale, but Pipelines can help.\nMore Options for Model Validation: We can easily apply cross-validation and other techniques to our Pipelines."
  },
  {
    "objectID": "slides/09-slides.html#reddit-data",
    "href": "slides/09-slides.html#reddit-data",
    "title": "Lecture 9",
    "section": "Reddit Data!",
    "text": "Reddit Data!"
  },
  {
    "objectID": "slides/09-slides.html#comparison-of-social-data-infrastructure",
    "href": "slides/09-slides.html#comparison-of-social-data-infrastructure",
    "title": "Lecture 9",
    "section": "Comparison of social data infrastructure",
    "text": "Comparison of social data infrastructure"
  },
  {
    "objectID": "slides/09-slides.html#data-dictionary",
    "href": "slides/09-slides.html#data-dictionary",
    "title": "Lecture 9",
    "section": "Data Dictionary",
    "text": "Data Dictionary"
  },
  {
    "objectID": "slides/09-slides.html#next-steps",
    "href": "slides/09-slides.html#next-steps",
    "title": "Lecture 9",
    "section": "Next Steps",
    "text": "Next Steps\n\nStart working with your group to conduct EDA on the dataset\nInitial data questions - understand the structure of the data!\nDefine your business questions - how do you plan to work with this data\nLook into external data to merge with your Reddit data\n\nBasic requirements will be published on Thursday\nDeadline for Project Milestone 1: EDA Monday November 7"
  },
  {
    "objectID": "slides/09-slides.html#spark-methods-for-text-analytics",
    "href": "slides/09-slides.html#spark-methods-for-text-analytics",
    "title": "Lecture 9",
    "section": "Spark methods for text analytics",
    "text": "Spark methods for text analytics\n\nString manipulation SQL functions: F.length(col), F.substring(str, pos, len), F.trim(col), F.upper(col), …\nML transformers: Tokenizer(), StopWordsRemover(), Word2Vec(), CountVectorizer()"
  },
  {
    "objectID": "slides/09-slides.html#tokenizer",
    "href": "slides/09-slides.html#tokenizer",
    "title": "Lecture 9",
    "section": "Tokenizer",
    "text": "Tokenizer\n&gt;&gt;&gt; df = spark.createDataFrame([(\"a b c\",)], [\"text\"])\n\n&gt;&gt;&gt; tokenizer = Tokenizer(outputCol=\"words\")\n\n&gt;&gt;&gt; tokenizer.setInputCol(\"text\")\n\n&gt;&gt;&gt; tokenizer.transform(df).head()\n\nRow(text='a b c', words=['a', 'b', 'c'])\n\n&gt;&gt;&gt; # Change a parameter.\n&gt;&gt;&gt; tokenizer.setParams(outputCol=\"tokens\").transform(df).head()\nRow(text='a b c', tokens=['a', 'b', 'c'])\n\n&gt;&gt;&gt; # Temporarily modify a parameter.\n&gt;&gt;&gt; tokenizer.transform(df, {tokenizer.outputCol: \"words\"}).head()\nRow(text='a b c', words=['a', 'b', 'c'])\n\n&gt;&gt;&gt; tokenizer.transform(df).head()\nRow(text='a b c', tokens=['a', 'b', 'c'])\ndoc"
  },
  {
    "objectID": "slides/09-slides.html#stopwordsremover",
    "href": "slides/09-slides.html#stopwordsremover",
    "title": "Lecture 9",
    "section": "StopWordsRemover",
    "text": "StopWordsRemover\n&gt;&gt;&gt; df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], [\"text\"])\n&gt;&gt;&gt; remover = StopWordsRemover(stopWords=[\"b\"])\n&gt;&gt;&gt; remover.setInputCol(\"text\")\n&gt;&gt;&gt; remover.setOutputCol(\"words\")\n&gt;&gt;&gt; remover.transform(df).head().words == ['a', 'c']\nTrue\n&gt;&gt;&gt; stopWordsRemoverPath = temp_path + \"/stopwords-remover\"\n&gt;&gt;&gt; remover.save(stopWordsRemoverPath)\n&gt;&gt;&gt; loadedRemover = StopWordsRemover.load(stopWordsRemoverPath)\n&gt;&gt;&gt; loadedRemover.getStopWords() == remover.getStopWords()\nTrue\n&gt;&gt;&gt; loadedRemover.getCaseSensitive() == remover.getCaseSensitive()\nTrue\n&gt;&gt;&gt; loadedRemover.transform(df).take(1) == remover.transform(df).take(1)\nTrue\n&gt;&gt;&gt; df2 = spark.createDataFrame([([\"a\", \"b\", \"c\"], [\"a\", \"b\"])], [\"text1\", \"text2\"])\n&gt;&gt;&gt; remover2 = StopWordsRemover(stopWords=[\"b\"])\n&gt;&gt;&gt; remover2.setInputCols([\"text1\", \"text2\"]).setOutputCols([\"words1\", \"words2\"])\n&gt;&gt;&gt; remover2.transform(df2).show()\n+---------+------+------+------+\n|    text1| text2|words1|words2|\n+---------+------+------+------+\n|[a, b, c]|[a, b]|[a, c]|   [a]|\n+---------+------+------+------+\ndoc"
  },
  {
    "objectID": "slides/09-slides.html#application-of-sentiment-analysis-in-pyspark",
    "href": "slides/09-slides.html#application-of-sentiment-analysis-in-pyspark",
    "title": "Lecture 9",
    "section": "Application of Sentiment Analysis in PySpark",
    "text": "Application of Sentiment Analysis in PySpark\nData: S&P company earnings calls - 10s of millions of text statements\nMethod: proximity-based sentiment analysis\nTech: PySpark, Python UDFs, lots of list comprehensions!\nOutcome: Time series trends of company concerns about supply chain related issues"
  },
  {
    "objectID": "slides/09-slides.html#application-continued",
    "href": "slides/09-slides.html#application-continued",
    "title": "Lecture 9",
    "section": "Application continued",
    "text": "Application continued\nExample: find the A’s within a certain distance of a Y\n# within2 -&gt; 0\nX X X X Y X X X A\n# within2 -&gt; 1\nX X A X Y X X X A\n# within2 -&gt; 2\nX A X A Y A X X A\n# within4 -&gt; 3\nA X A X Y X X X A\n\nCount the number of Y’s in the text that have an A near enough to them\nAggregate at scale!\nProject uses Tokenizer() and StopWordsRemover()\n\nProf. Anderson’s Paper\nCOVID paper that championed the method"
  },
  {
    "objectID": "slides/09-slides.html#jonsnowlabs-spark-nlp-package",
    "href": "slides/09-slides.html#jonsnowlabs-spark-nlp-package",
    "title": "Lecture 9",
    "section": "JonSnowLabs Spark NLP Package",
    "text": "JonSnowLabs Spark NLP Package\nWhy use UDFs, run proximity-based sentiment? Let’s use more advanced natural language processing packages!\nWhich libraries have the most features?"
  },
  {
    "objectID": "slides/09-slides.html#comparing-nlp-packages",
    "href": "slides/09-slides.html#comparing-nlp-packages",
    "title": "Lecture 9",
    "section": "Comparing NLP Packages",
    "text": "Comparing NLP Packages\nJust because it is scalable does not mean it lacks features!"
  },
  {
    "objectID": "slides/09-slides.html#most-popular-aiml-packages",
    "href": "slides/09-slides.html#most-popular-aiml-packages",
    "title": "Lecture 9",
    "section": "Most Popular AI/ML Packages",
    "text": "Most Popular AI/ML Packages"
  },
  {
    "objectID": "slides/09-slides.html#spark-nlp-is-faster-than-spacy",
    "href": "slides/09-slides.html#spark-nlp-is-faster-than-spacy",
    "title": "Lecture 9",
    "section": "Spark NLP is faster than SpaCy",
    "text": "Spark NLP is faster than SpaCy\nBenchmark for training a pipeline with sentence bounder, tokenizer, and POS tagger on a 4-core laptop\nWhy??\n\nWhole stage code generation, vectorized in-memory columnar data\nNo copying of text in memory\nExtensive profiling, config & code optimization of Spark and TensorFlow\nOptimized for training and inference\n\nAaaaannndddd…. it scales!"
  },
  {
    "objectID": "slides/09-slides.html#spark-nlp",
    "href": "slides/09-slides.html#spark-nlp",
    "title": "Lecture 9",
    "section": "Spark NLP",
    "text": "Spark NLP\n\nReusing the Spark ML Pipeline\n\nUnified NLP & ML pipelines\nEnd-to-end execution planning\nSerializable\nDistributable\n\nReusing NLP Functionality\n\nTF-IDF calculation\nString distance calculation\nStop word removal\nTopic modeling\nDistributed ML algorithms"
  },
  {
    "objectID": "slides/09-slides.html#spark-nlp-terminology",
    "href": "slides/09-slides.html#spark-nlp-terminology",
    "title": "Lecture 9",
    "section": "Spark NLP Terminology",
    "text": "Spark NLP Terminology\nAnnotators\n\nLike the ML tools we used in Spark\nAlways need input and output columns\nTwo flavors:\n\nApproach - like ML estimators that need a fit() method to make an Annotator Model or Transformer\nModel - like ML transformers and uses transform() method only\n\n\nAnnotator Models\n\nPretrained public versions of models available through .pretained() method\n\nQ: Do transformer ML methods ever replace or remove columns in a Spark DataFrame?\n\nNo!! They only add columns."
  },
  {
    "objectID": "slides/09-slides.html#spark-nlp-sentiment-example",
    "href": "slides/09-slides.html#spark-nlp-sentiment-example",
    "title": "Lecture 9",
    "section": "Spark NLP Sentiment Example",
    "text": "Spark NLP Sentiment Example"
  },
  {
    "objectID": "slides/09-slides.html#spark-nlp-pipeline-example",
    "href": "slides/09-slides.html#spark-nlp-pipeline-example",
    "title": "Lecture 9",
    "section": "Spark NLP Pipeline Example",
    "text": "Spark NLP Pipeline Example"
  },
  {
    "objectID": "slides/09-slides.html#spark-nlp-pipeline-types",
    "href": "slides/09-slides.html#spark-nlp-pipeline-types",
    "title": "Lecture 9",
    "section": "Spark NLP Pipeline Types",
    "text": "Spark NLP Pipeline Types\nSpark Pipeline\n\nEfficiently run on a whole Spark Dataframe\nDistributable on a cluster\nUses Spark tasks, optimizations & execution planning\nUsed by PretrainedPipeline.transform()\n\nLight Pipeline\n\nEfficiently run on a single sentence\nFaster than a Spark pipeline for up to 50,000 local documents\nEasiest way to publish a pipeline as an API\nUsed by PretrainedPipeline.annotate()\n\nRecursive Pipeline\n\nGive annotators access to other annotators in the same pipeline\nRequired when training your own models\n\nEssential Spark NLP reading"
  },
  {
    "objectID": "slides/09-slides.html#overall-code-example",
    "href": "slides/09-slides.html#overall-code-example",
    "title": "Lecture 9",
    "section": "Overall Code Example",
    "text": "Overall Code Example\n\nfrom pyspark.ml import Pipeline\n\ndocument_assembler = DocumentAssembler()\\\n .setInputCol(“text”)\\\n .setOutputCol(“document”)\n \nsentenceDetector = SentenceDetector()\\\n .setInputCols([“document”])\\\n .setOutputCol(“sentences”)\n \ntokenizer = Tokenizer() \\\n .setInputCols([“sentences”]) \\\n .setOutputCol(“token”)\n \nnormalizer = Normalizer()\\\n .setInputCols([“token”])\\\n .setOutputCol(“normal”)\n \nword_embeddings=WordEmbeddingsModel.pretrained()\\\n .setInputCols([“document”,”normal”])\\\n .setOutputCol(“embeddings”)\n\nnlpPipeline = Pipeline(stages=[\n document_assembler, \n sentenceDetector,\n tokenizer,\n normalizer,\n word_embeddings,\n ])\n\npipelineModel = nlpPipeline.fit(df)"
  },
  {
    "objectID": "slides/09-slides.html#code-example-for-documentassembler",
    "href": "slides/09-slides.html#code-example-for-documentassembler",
    "title": "Lecture 9",
    "section": "Code Example for DocumentAssembler",
    "text": "Code Example for DocumentAssembler\nimport sparknlp; from sparknlp.base import *\nfrom sparknlp.annotator import *; from pyspark.ml import Pipeline\n\ndata = spark.createDataFrame([[\"Spark NLP is an open-source text processing library.\"]]).toDF(\"text\")\ndocumentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n\nresult = documentAssembler.transform(data)\n\nresult.select(\"document\").show(truncate=False)\n+----------------------------------------------------------------------------------------------+\n|document                                                                                      |\n+----------------------------------------------------------------------------------------------+\n|[[document, 0, 51, Spark NLP is an open-source text processing library., [sentence -&gt; 0], []]]|\n+----------------------------------------------------------------------------------------------+"
  },
  {
    "objectID": "slides/09-slides.html#continued",
    "href": "slides/09-slides.html#continued",
    "title": "Lecture 9",
    "section": "Continued",
    "text": "Continued\nresult.select(\"document\").printSchema\nroot\n |-- document: array (nullable = True)\n |    |-- element: struct (containsNull = True)\n |    |    |-- annotatorType: string (nullable = True)\n |    |    |-- begin: integer (nullable = False)\n |    |    |-- end: integer (nullable = False)\n |    |    |-- result: string (nullable = True)\n |    |    |-- metadata: map (nullable = True)\n |    |    |    |-- key: string\n |    |    |    |-- value: string (valueContainsNull = True)\n |    |    |-- embeddings: array (nullable = True)\n |    |    |    |-- element: float (containsNull = False)"
  },
  {
    "objectID": "slides/09-slides.html#using-huggingface-transformers-models-with-sparknlp",
    "href": "slides/09-slides.html#using-huggingface-transformers-models-with-sparknlp",
    "title": "Lecture 9",
    "section": "Using HuggingFace Transformers Models with SparkNLP",
    "text": "Using HuggingFace Transformers Models with SparkNLP\nhttps://github.com/JohnSnowLabs/spark-nlp/discussions/5669\nfrom transformers import TFDistilBertForSequenceClassification, DistilBertTokenizer \nfrom sparknlp.annotator import *\nfrom sparknlp.base import *\n\nMODEL_NAME = 'distilbert-base-uncased-finetuned-sst-2-english'\n\ntokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.save_pretrained('./{}_tokenizer/'.format(MODEL_NAME))\n\nmodel = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\nmodel.save_pretrained(\"./{}\".format(MODEL_NAME), saved_model=True)\n\nasset_path = '{}/saved_model/1/assets'.format(MODEL_NAME)\n!cp {MODEL_NAME}_tokenizer/vocab.txt {asset_path}\n\nsequenceClassifier = DistilBertForSequenceClassification.loadSavedModel(\n     '{}/saved_model/1'.format(MODEL_NAME),spark)\\\n     .setInputCols([\"document\",'token'])\\\n  .setOutputCol(\"class\").setCaseSensitive(True).setMaxSentenceLength(128)\n  \n#### WHERE IS THIS SAVING TO???\nsequenceClassifier.write().overwrite().save(\"./{}_spark_nlp\".format(MODEL_NAME))"
  },
  {
    "objectID": "slides/09-slides.html#readings",
    "href": "slides/09-slides.html#readings",
    "title": "Lecture 9",
    "section": "Readings",
    "text": "Readings\nRequired:\n\nSpark NLP Code Concepts\n\nEncouraged:\n\nSpark NLP Annotators\nIntro to Spark NLP"
  },
  {
    "objectID": "slides/11-slides.html#aws-academy",
    "href": "slides/11-slides.html#aws-academy",
    "title": "Lecture 11",
    "section": "AWS Academy",
    "text": "AWS Academy\n\nCredit limit - $100\nCourse numbers:\n\nCourse #1 - 24178\nCourse #2 - 27354\nCourse #3 - 22802\nCourse #4 - 26418\n\n\nSTAY WITH COURSE 24178 UNLESS YOU HAVE RUN OUT OF CREDITS OR &gt;$90 USED!\nNote that you will have to repeat several setup steps:\n\nsecurity group\nEC2 keypair uploading (the AWS part only)\nsagemaker setup\nany S3 uploading or copying as well as bucket creation as necessary\nEMR configuration"
  },
  {
    "objectID": "slides/11-slides.html#ray",
    "href": "slides/11-slides.html#ray",
    "title": "Lecture 11",
    "section": "Ray",
    "text": "Ray\n\n\nRay is an open-source unified compute framework that makes it easy to scale AI and Python workloads — from reinforcement learning to deep learning to tuning, and model serving. Learn more about Ray’s rich set of libraries and integrations."
  },
  {
    "objectID": "slides/11-slides.html#why-do-we-need-ray",
    "href": "slides/11-slides.html#why-do-we-need-ray",
    "title": "Lecture 11",
    "section": "Why do we need Ray?",
    "text": "Why do we need Ray?\n\nTo scale any Python workload from laptop to cloud\nRemember the Spark ecosystem with all its integrations…\nHadoop (fault tolerance, resiliency using commodity hardware) -&gt; Spark (in memory data processing) -&gt; Ray (asynch processing, everything you need for scaling AI workloads)\n\n\n\nRay Summit 2022 - Ray and Large Language Models"
  },
  {
    "objectID": "slides/11-slides.html#what-can-we-do-with-ray",
    "href": "slides/11-slides.html#what-can-we-do-with-ray",
    "title": "Lecture 11",
    "section": "What can we do with Ray?",
    "text": "What can we do with Ray?"
  },
  {
    "objectID": "slides/11-slides.html#what-comes-in-the-box",
    "href": "slides/11-slides.html#what-comes-in-the-box",
    "title": "Lecture 11",
    "section": "What comes in the box?",
    "text": "What comes in the box?"
  },
  {
    "objectID": "slides/11-slides.html#our-focus-today",
    "href": "slides/11-slides.html#our-focus-today",
    "title": "Lecture 11",
    "section": "Our focus today …",
    "text": "Our focus today …\n\n\nRay Core Ray Core provides a small number of core primitives (i.e., tasks, actors, objects) for building and scaling distributed applications.\nhttps://docs.ray.io/en/latest/ray-core/walkthrough.html\n\nRay Datasets\nRay Datasets are the standard way to load and exchange data in Ray libraries and applications. They provide basic distributed data transformations such as maps (map_batches), global and grouped aggregations (GroupedDataset), and shuffling operations (random_shuffle, sort, repartition), and are compatible with a variety of file formats, data sources, and distributed frameworks.\nhttps://docs.ray.io/en/latest/data/dataset.html"
  },
  {
    "objectID": "slides/11-slides.html#ray-core---tasks",
    "href": "slides/11-slides.html#ray-core---tasks",
    "title": "Lecture 11",
    "section": "Ray Core - Tasks",
    "text": "Ray Core - Tasks\n\nRay enables arbitrary functions to be executed asynchronously on separate Python workers.\nSuch functions are called Ray remote functions and their asynchronous invocations are called Ray tasks.\n\n```{python}\n# By adding the `@ray.remote` decorator, a regular Python function\n# becomes a Ray remote function.\n@ray.remote\ndef my_function():\n    # do something\n    time.sleep(10)\n    return 1\n\n# To invoke this remote function, use the `remote` method.\n# This will immediately return an object ref (a future) and then create\n# a task that will be executed on a worker process.\nobj_ref = my_function.remote()\n\n# The result can be retrieved with ``ray.get``.\nassert ray.get(obj_ref) == 1\n\n# Specify required resources.\n@ray.remote(num_cpus=4, num_gpus=2)\ndef my_other_function():\n    return 1\n\n# Ray tasks are executed in parallel.\n# All computation is performed in the background, driven by Ray's internal event loop.\nfor _ in range(4):\n    # This doesn't block.\n    my_function.remote()\n```"
  },
  {
    "objectID": "slides/11-slides.html#ray-core---actors",
    "href": "slides/11-slides.html#ray-core---actors",
    "title": "Lecture 11",
    "section": "Ray Core - Actors",
    "text": "Ray Core - Actors\n\nActors extend the Ray API from functions (tasks) to classes.\nAn actor is a stateful worker (or a service).\nWhen a new actor is instantiated, a new worker is created, and methods of the actor are scheduled on that specific worker and can access and mutate the state of that worker.\n\n```{python}\n@ray.remote(num_cpus=2, num_gpus=0.5)\nclass Counter(object):\n    def __init__(self):\n        self.value = 0\n\n    def increment(self):\n        self.value += 1\n        return self.value\n\n    def get_counter(self):\n        return self.value\n\n# Create an actor from this class.\ncounter = Counter.remote()\n# Call the actor.\nobj_ref = counter.increment.remote()\nassert ray.get(obj_ref) == 1\n```"
  },
  {
    "objectID": "slides/11-slides.html#ray-core---objects",
    "href": "slides/11-slides.html#ray-core---objects",
    "title": "Lecture 11",
    "section": "Ray Core - Objects",
    "text": "Ray Core - Objects\n\nTasks and actors create and compute on objects.\nObjects are referred as remote objects because they can be stored anywhere in a Ray cluster\n\nWe use object refs to refer to them.\n\nRemote objects are cached in Ray’s distributed shared-memory object store\nThere is one object store per node in the cluster.\n\nIn the cluster setting, a remote object can live on one or many nodes, independent of who holds the object ref(s).\n\n\n\n\n\n\n\n\nNote\n\n\nRemote objects are immutable. That is, their values cannot be changed after creation. This allows remote objects to be replicated in multiple object stores without needing to synchronize the copies."
  },
  {
    "objectID": "slides/11-slides.html#ray-core---objects-contd.",
    "href": "slides/11-slides.html#ray-core---objects-contd.",
    "title": "Lecture 11",
    "section": "Ray Core - Objects (contd.)",
    "text": "Ray Core - Objects (contd.)\n```{python}\n# Put an object in Ray's object store.\ny = 1\nobject_ref = ray.put(y)\n\n# Get the value of one object ref.\nobj_ref = ray.put(1)\nassert ray.get(obj_ref) == 1\n\n# Get the values of multiple object refs in parallel.\nassert ray.get([ray.put(i) for i in range(3)]) == [0, 1, 2]\n```"
  },
  {
    "objectID": "slides/11-slides.html#ray-datasets",
    "href": "slides/11-slides.html#ray-datasets",
    "title": "Lecture 11",
    "section": "Ray Datasets",
    "text": "Ray Datasets\n\nRay Datasets are the standard way to load and exchange data in Ray libraries and applications.\nThey provide basic distributed data transformations such as maps, global and grouped aggregations, and shuffling operations.\nCompatible with a variety of file formats, data sources, and distributed frameworks.\nRay Datasets are designed to load and preprocess data for distributed ML training pipelines.\nDatasets simplify general purpose parallel GPU and CPU compute in Ray.\n\nProvide a higher-level API for Ray tasks and actors for such embarrassingly parallel compute, internally handling operations like batching, pipelining, and memory management."
  },
  {
    "objectID": "slides/11-slides.html#ray-datasets-1",
    "href": "slides/11-slides.html#ray-datasets-1",
    "title": "Lecture 11",
    "section": "Ray Datasets",
    "text": "Ray Datasets\n\nCreate\n\n```{python}\nimport ray\n\ndataset = ray.data.read_csv(\"s3://anonymous@air-example-data/iris.csv\")\n\ndataset.show(limit=1)\n```\n\nTransform\n\n```{python}\nimport pandas as pd\n\n# Find rows with spepal length &lt; 5.5 and petal length &gt; 3.5.\ndef transform_batch(df: pd.DataFrame) -&gt; pd.DataFrame:\n    return df[(df[\"sepal length (cm)\"] &lt; 5.5) & (df[\"petal length (cm)\"] &gt; 3.5)]\n\ntransformed_dataset = dataset.map_batches(transform_batch)\nprint(transformed_dataset)\n```\n\nConsume\n\n```{python}\nbatches = transformed_dataset.iter_batches(batch_size=8)\nprint(next(iter(batches)))\n```\n\nSave\n\n```{python}\nimport os\n\ntransformed_dataset.write_parquet(\"iris\")\n\nprint(os.listdir(\"iris\"))\n```"
  },
  {
    "objectID": "slides/11-slides.html#references",
    "href": "slides/11-slides.html#references",
    "title": "Lecture 11",
    "section": "References",
    "text": "References\n\nRay website\nLarge Scale Data Loading and Data Preprocessing with Ray\nRay NYC March 2023 with NYC ML/AI Meetup\nLarge-scale deep learning training and tuning with Ray at Uber\nAccelerating AI/ML scaling and AI development with Anyscale and AWS\nScaling AI with Project Ray, the Successor to Spark"
  },
  {
    "objectID": "slides/13-slides.html#before-we-begin..",
    "href": "slides/13-slides.html#before-we-begin..",
    "title": "Lecture 13",
    "section": "Before we begin..",
    "text": "Before we begin..\n\nPandas: pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\n\n\n\nPandas is slow, well yes but also not so much if you use it the right way.\n\n\nApache Arrow and the “10 Things I Hate About pandas” (A 2017 post from the creator of Pandas..)\n50 times faster data loading for Pandas: no problem (but this is an old 2019 article..)\nIs Pandas really that slow?\n\n\nPandas 2.0 and the arrow revolution"
  },
  {
    "objectID": "slides/13-slides.html#polars",
    "href": "slides/13-slides.html#polars",
    "title": "Lecture 13",
    "section": "Polars",
    "text": "Polars\n\nPolars: Lightning-fast DataFrame library for Rust and Python\n\nWhy is it faster than Pandas?\n\n\nWritten in Rust (compiled not interpreted).\nUses all available cores of your machine.\nUse PyArrow.\n[My opinion] Makes it easier to write code the right away (has a strict schema and others)!"
  },
  {
    "objectID": "slides/13-slides.html#coming-from-pandas-to-polars",
    "href": "slides/13-slides.html#coming-from-pandas-to-polars",
    "title": "Lecture 13",
    "section": "Coming from Pandas to Polars",
    "text": "Coming from Pandas to Polars\n\n\nimport pandas as pd\ndf = pd.DataFrame({\n    \"type\": [\"m\", \"n\", \"o\", \"m\", \"m\", \"n\", \"n\"],\n    \"c\": [1, 1, 1, 2, 2, 2, 2],\n})\n\ndf[\"size\"] = df.groupby(\"c\")[\"type\"].transform(len)\ndf\n\nimport polars as pl\ndf = pl.DataFrame({\n    \"type\": [\"m\", \"n\", \"o\", \"m\", \"m\", \"n\", \"n\"],\n    \"c\": [1, 1, 1, 2, 2, 2, 2],\n})\ndf.select([\n    pl.all(),\n    pl.col(\"type\").count().over(\"c\").alias(\"size\")\n])\n\n\nhttps://pola-rs.github.io/polars-book/user-guide/coming_from_pandas.html"
  },
  {
    "objectID": "slides/13-slides.html#coming-from-spark-to-polars",
    "href": "slides/13-slides.html#coming-from-spark-to-polars",
    "title": "Lecture 13",
    "section": "Coming From Spark to Polars",
    "text": "Coming From Spark to Polars\n\nhttps://pola-rs.github.io/polars-book/user-guide/coming_from_spark.html"
  },
  {
    "objectID": "slides/13-slides.html#polars-useful-links",
    "href": "slides/13-slides.html#polars-useful-links",
    "title": "Lecture 13",
    "section": "Polars: useful links",
    "text": "Polars: useful links\n\nPolars\nUser guide\nPolars GitHub repo\nPandas Vs Polars: a syntax and speed comparison\nTips & tricks for working with strings in Polars"
  },
  {
    "objectID": "slides/13-slides.html#duckdb",
    "href": "slides/13-slides.html#duckdb",
    "title": "Lecture 13",
    "section": "DuckDB",
    "text": "DuckDB\nDuckDB is an in-process SQL OLAP database management system\npip install duckdb==0.7.1\n\nDuck DBAlso checkout MotherDuck"
  },
  {
    "objectID": "slides/13-slides.html#duckdb-contd.",
    "href": "slides/13-slides.html#duckdb-contd.",
    "title": "Lecture 13",
    "section": "DuckDB (contd.)",
    "text": "DuckDB (contd.)\n\nDuck DB"
  },
  {
    "objectID": "slides/13-slides.html#rapids",
    "href": "slides/13-slides.html#rapids",
    "title": "Lecture 13",
    "section": "RAPIDS",
    "text": "RAPIDS\nRAPIDS is a suite of open-source software libraries and APIs for executing data science pipelines entirely on GPUs—and can reduce training times from days to minutes. Built on NVIDIA® CUDA-X AI™, RAPIDS unites years of development in graphics, machine learning, deep learning, high-performance computing (HPC), and more.\nhttps://www.nvidia.com/en-us/deep-learning-ai/software/rapids"
  },
  {
    "objectID": "slides/week-tbd--slides.html#agenda-and-goals-for-today",
    "href": "slides/week-tbd--slides.html#agenda-and-goals-for-today",
    "title": "Lecture 4",
    "section": "Agenda and Goals for Today",
    "text": "Agenda and Goals for Today\n\n\nLecture\n\nDocker containers\n\nBackground and concepts\nSyntax and commands\nExamples\n\nLambda/microservices\n\nBackground and concepts\nSyntax and commands\nExamples\n\n\n\nLab\n\nLambda service exploration\nCloud9 Setup\nDocker Container exploration in Cloud9"
  },
  {
    "objectID": "slides/week-tbd--slides.html#logistics-and-review",
    "href": "slides/week-tbd--slides.html#logistics-and-review",
    "title": "Lecture 4",
    "section": "Logistics and Review",
    "text": "Logistics and Review\n\n\nDeadlines\n\nAssignment 1: Python Skills Due Sept 5 11:59pm\nLab 2: Cloud Tooling Due Sept 5 6pm\nAssignment 2: Shell & Linux Due Sept 11 11:59pm\nLab 3: Parallel Computing Due Sept 12 6pm\nAssignment 3: Parallelization Due Sept 18 11:59pm\nLab 4: Docker and Lambda Due Sept 19 6pm\nAssignment 4: Containers Due Sept 25 11:59pm\nLab 5: DuckDB & Polars Due Sept 26 6pm\n\n\nLook back and ahead\n\nContinue to use Slack for questions!\nParallelization in Python\nConcepts of map and reduce\nChoosing when to re-factor/optimize code for scalability\nQuestions on Assignment 3: Parallelization?"
  },
  {
    "objectID": "slides/week-tbd--slides.html#process-isolation-and-deployment",
    "href": "slides/week-tbd--slides.html#process-isolation-and-deployment",
    "title": "Lecture 4",
    "section": "Process Isolation and Deployment",
    "text": "Process Isolation and Deployment\n\nEncapsulate all the dependencies of your code\nDeploy your code into production in the same environment as your development space\nRun your code without other processes affecting your work\nYou need virtualization!"
  },
  {
    "objectID": "slides/week-tbd--slides.html#motivation",
    "href": "slides/week-tbd--slides.html#motivation",
    "title": "Lecture 4",
    "section": "Motivation",
    "text": "Motivation\nWhy should you care?"
  },
  {
    "objectID": "slides/week-tbd--slides.html#history-of-virtualization",
    "href": "slides/week-tbd--slides.html#history-of-virtualization",
    "title": "Lecture 4",
    "section": "History of Virtualization",
    "text": "History of Virtualization\n\nTwo types of virtualization - hardware and software\nHardware time sharing started in the late 1950s\nVirtualization first introduced in IBM in the 1960s\nVMWare started in\n\n\nlink"
  },
  {
    "objectID": "slides/week-tbd--slides.html#virtual-machines",
    "href": "slides/week-tbd--slides.html#virtual-machines",
    "title": "Lecture 4",
    "section": "Virtual Machines",
    "text": "Virtual Machines\n\n\n\nHardware server is the host\nMultiple virtual machines can sit on the hypervisor as “guests”\nPros: decades of development in VMs, multiple emulated full computers on single hardware, allows for infrastructure “lift and shift” to the cloud\nCons: VM images very large (10-100s GBs), “guest” OS takes up resources, large start up cost\n\nlink"
  },
  {
    "objectID": "slides/week-tbd--slides.html#containers-1",
    "href": "slides/week-tbd--slides.html#containers-1",
    "title": "Lecture 4",
    "section": "Containers",
    "text": "Containers\n\n\n\nPackage and run applications in isolated environment\nGreat for production work, not as much for developmental work\nDocker application manages “guest” applications with a single operating system\nPros: Lightweight (10-100s MBs), faster spin up time, one host can manage many more containers\nCons: containers must all work with the same OS, somewhat less secure due to shared OS resources, only 10 years old\n\nlink"
  },
  {
    "objectID": "slides/week-tbd--slides.html#docker-vs.-virtual-machine",
    "href": "slides/week-tbd--slides.html#docker-vs.-virtual-machine",
    "title": "Lecture 4",
    "section": "Docker vs. Virtual Machine",
    "text": "Docker vs. Virtual Machine"
  },
  {
    "objectID": "slides/week-tbd--slides.html#docker-container-architecture",
    "href": "slides/week-tbd--slides.html#docker-container-architecture",
    "title": "Lecture 4",
    "section": "Docker Container Architecture",
    "text": "Docker Container Architecture\n\nImages are a snapshot of instructions for setup of an application environment\nContainers are executing applications deployed based on an image\nContainer images become containers at runtime (Docker Engine!)"
  },
  {
    "objectID": "slides/week-tbd--slides.html#containers-are-not-enough",
    "href": "slides/week-tbd--slides.html#containers-are-not-enough",
    "title": "Lecture 4",
    "section": "Containers are not enough",
    "text": "Containers are not enough"
  },
  {
    "objectID": "slides/week-tbd--slides.html#container-lifecycle",
    "href": "slides/week-tbd--slides.html#container-lifecycle",
    "title": "Lecture 4",
    "section": "Container lifecycle",
    "text": "Container lifecycle\n\nContainers (like applications) need a lifecycle"
  },
  {
    "objectID": "slides/week-tbd--slides.html#container-orchestration",
    "href": "slides/week-tbd--slides.html#container-orchestration",
    "title": "Lecture 4",
    "section": "Container orchestration",
    "text": "Container orchestration"
  },
  {
    "objectID": "slides/week-tbd--slides.html#docker-layers",
    "href": "slides/week-tbd--slides.html#docker-layers",
    "title": "Lecture 4",
    "section": "Docker layers",
    "text": "Docker layers\nEach command to modify the environment is a layer in your build process"
  },
  {
    "objectID": "slides/week-tbd--slides.html#docker-base-images",
    "href": "slides/week-tbd--slides.html#docker-base-images",
    "title": "Lecture 4",
    "section": "Docker base images",
    "text": "Docker base images\n\nThe first command in a Docker build is to call FROM XXXXX. This is the base image that has the core OS-level software.\nsmaller size == faster execution"
  },
  {
    "objectID": "slides/week-tbd--slides.html#the-storage-problem-of-containers",
    "href": "slides/week-tbd--slides.html#the-storage-problem-of-containers",
    "title": "Lecture 4",
    "section": "The storage problem of containers",
    "text": "The storage problem of containers\n\nWhen containers are killed, all temporary is lost! What to do…\nContainers need persistent storage sometimes\nDocker volumes live outside the container and allow for permanent storage"
  },
  {
    "objectID": "slides/week-tbd--slides.html#dockerfile-syntax",
    "href": "slides/week-tbd--slides.html#dockerfile-syntax",
    "title": "Lecture 4",
    "section": "Dockerfile Syntax",
    "text": "Dockerfile Syntax\nStudents explain what each command is doing\n\nFormat is always INSTRUCTION arguments\nFROM [--platform=&lt;platform&gt;] &lt;image&gt;[:&lt;tag&gt;] [AS &lt;name&gt;] - Pulling the base image to start with\nRUN &lt;command&gt; or RUN [\"executable\", \"param1\", \"param2\"] - Run commands of any type\nVOLUME - Creates mountpoint for Docker volume\nENV &lt;key&gt;=&lt;value&gt; ... - Set environment variables for your container\nWORKDIR - Sets working directory for other commands\nEXPOSE &lt;port&gt; [&lt;port&gt;/&lt;protocol&gt;...] - Open a port for the container\nCOPY [--chown=&lt;user&gt;:&lt;group&gt;] [--chmod=&lt;perms&gt;] &lt;src&gt;... &lt;dest&gt; - Copies files from host file system to container file system. Note working directory matters in the container too!\nADD - Copy with a link to allow for reusing of already built layers. Helps with build speed!"
  },
  {
    "objectID": "slides/week-tbd--slides.html#dockerfile-cmd-and-entrypoint",
    "href": "slides/week-tbd--slides.html#dockerfile-cmd-and-entrypoint",
    "title": "Lecture 4",
    "section": "Dockerfile CMD and ENTRYPOINT",
    "text": "Dockerfile CMD and ENTRYPOINT\n\nShould have a purpose for your container and have it run something!\nCMD command param1 param2 - Final command to prepare the launching container. Typically occurs before an ENTRYPOINT instruction or as its own standalone command. CMD can be overwritten by new docker run arguments.\nENTRYPOINT command param1 param2 - Command that will run the Docker container as an executable, such as a web server, Jupter environment, or data pipeline."
  },
  {
    "objectID": "slides/week-tbd--slides.html#dockerfile-syntax-examples",
    "href": "slides/week-tbd--slides.html#dockerfile-syntax-examples",
    "title": "Lecture 4",
    "section": "Dockerfile Syntax Examples",
    "text": "Dockerfile Syntax Examples\n\nRUN [\"/bin/bash\", \"-c\", \"echo hello\"]\n\n```{bash}\nFROM busybox\nENV FOO=/bar\nWORKDIR ${FOO}   # WORKDIR /bar\nADD . $FOO       # ADD . /bar\nCOPY \\$FOO /quux # COPY $FOO /quu\n```\n```{bash}\n# syntax=docker/dockerfile:1\nFROM python:3.6\nADD mypackage.tgz wheels/\nRUN --network=none pip install --find-links wheels mypackage\n```\nDockerfile reference link"
  },
  {
    "objectID": "slides/week-tbd--slides.html#s3-example",
    "href": "slides/week-tbd--slides.html#s3-example",
    "title": "Lecture 4",
    "section": "S3 example",
    "text": "S3 example\n```{bash}\n# syntax=docker/dockerfile:1\nFROM python:3\nRUN pip install awscli\nRUN --mount=type=secret,id=aws,target=/root/.aws/credentials aws s3 cp s3://... ...\n```"
  },
  {
    "objectID": "slides/week-tbd--slides.html#web-server-example",
    "href": "slides/week-tbd--slides.html#web-server-example",
    "title": "Lecture 4",
    "section": "Web server example",
    "text": "Web server example\n```{bash}\nFROM debian:stable\nRUN apt-get update && apt-get install -y --force-yes apache2\nEXPOSE 80 443\nVOLUME [\"/var/www\", \"/var/log/apache2\", \"/etc/apache2\"]\nENTRYPOINT [\"/usr/sbin/apache2ctl\", \"-D\", \"FOREGROUND\"]\n```"
  },
  {
    "objectID": "slides/week-tbd--slides.html#docker-for-python-jobs",
    "href": "slides/week-tbd--slides.html#docker-for-python-jobs",
    "title": "Lecture 4",
    "section": "Docker for Python jobs",
    "text": "Docker for Python jobs\nBasic needs:\n\nDockerfile that sets up environment\nApplication code to execute\nList of packages to install\n\nWalkthrough"
  },
  {
    "objectID": "slides/week-tbd--slides.html#docker-python-example",
    "href": "slides/week-tbd--slides.html#docker-python-example",
    "title": "Lecture 4",
    "section": "Docker Python example",
    "text": "Docker Python example\n\nWrite your Dockerfile\n\nFROM python:3.8-slim-buster\nWORKDIR /app\nCOPY requirements.txt requirements.txt\nRUN pip3 install -r requirements.txt\nCOPY . .\nCMD [ \"python3\", \"-m\" , \"flask\", \"run\", \"--host=0.0.0.0\"]\n\nBuild the docker image using a tag and version number docker build ./ -t docker-tag-name:1.0. Read about docker building options here.\nCheck your docker images in your local file system using docker images\nTest your docker image using docker run -it docker-tag-name. Read about the flags here. The i is to keep STDIN open, the t is for pseudo-TTY mode (shell).\nDeploy your docker image somewhere!"
  },
  {
    "objectID": "slides/week-tbd--slides.html#docker-links",
    "href": "slides/week-tbd--slides.html#docker-links",
    "title": "Lecture 4",
    "section": "Docker links",
    "text": "Docker links\nAWS examples\nArticle discussing performance considerations\nWhat are containers?"
  },
  {
    "objectID": "slides/week-tbd--slides.html#overview",
    "href": "slides/week-tbd--slides.html#overview",
    "title": "Lecture 4",
    "section": "Overview",
    "text": "Overview\n\nLambda (or \\(\\lambda\\)) is the name given to anonymous functions in some languages like Python.\nLambda is a powerful tool to build serverless microfunctions in a variety of programming languages that can be triggered in a variety of ways.\nThe execution of a Lambda can scale to meet the burst needs of users going to a website, or the number of rows of incoming data.\nThere can be thousands of executions happening simultaneously.\nLambda is billed by the millisecond (ms).\n\n\n\nThe name Lambda is derived from Lambda calculus, a mathematical system to express computation introduced by Dr. Alonzo Church in the 1930s. See wikipedia entry on Lambda calculus for why the symbol \\(\\lambda\\) was choosen."
  },
  {
    "objectID": "slides/week-tbd--slides.html#lambda-details",
    "href": "slides/week-tbd--slides.html#lambda-details",
    "title": "Lecture 4",
    "section": "Lambda Details",
    "text": "Lambda Details\n\n\nMicro executions without dealing with any hardware, “serverless” computation.\nCan run many copies of code depending on number of requests or files.\nSupported languages: Java, Go, PowerShell, Node.js, C#, Python, and Ruby.\n\nProvides a Runtime API which allows you to use any additional programming languages to author your functions\n\nPricing based on RAM and time used - Lambda pricing\n\nMax execution time - 15 minutes\nMax RAM - 10 GB"
  },
  {
    "objectID": "slides/week-tbd--slides.html#lambda-triggers",
    "href": "slides/week-tbd--slides.html#lambda-triggers",
    "title": "Lecture 4",
    "section": "Lambda triggers",
    "text": "Lambda triggers\n\n\n\nLambdas only start executing in response to an event, such as an API being invoked, a schedule or more complex things.\nA Lambda function is used primarily in the following two ways:\n\nHosting an API, this is done in conjunction with an Amazon API Gateway.\nRunning custom code to respond to events such as a new file showing up in an S3 bucket, a row being deleted from a database etc.\n\n\n\n\n\n\nLambda triggers"
  },
  {
    "objectID": "slides/week-tbd--slides.html#different-ways-to-invoke-lambda",
    "href": "slides/week-tbd--slides.html#different-ways-to-invoke-lambda",
    "title": "Lecture 4",
    "section": "Different ways to invoke Lambda",
    "text": "Different ways to invoke Lambda\n Source: Understanding the different ways to invoke Lambda function"
  },
  {
    "objectID": "slides/week-tbd--slides.html#some-real-world-use-cases-for-a-lambda",
    "href": "slides/week-tbd--slides.html#some-real-world-use-cases-for-a-lambda",
    "title": "Lecture 4",
    "section": "Some real world use-cases for a Lambda",
    "text": "Some real world use-cases for a Lambda\n\nTransforming a picture taken by a delivery agent into a desired format and storing it in S3 in real-time.\nAs a front-end (along with an API Gateway) to a ML model to provide real-time prediction to a customer clicking a link on a website .\nA backend API that gets invoked when a QR code is scanned.\nA simple single page website with a form.\nSending a scheduled newsletter to email addresses on a mailing list.\n\nAWS Lambda customer case studies"
  },
  {
    "objectID": "slides/week-tbd--slides.html#lambda-can-be-set-on-a-schedule",
    "href": "slides/week-tbd--slides.html#lambda-can-be-set-on-a-schedule",
    "title": "Lecture 4",
    "section": "Lambda can be set on a schedule",
    "text": "Lambda can be set on a schedule\n\nSimilar to Linux Crontab or Windows Task Scheduler\n\nexample of schedule trigger\n\nSite for translating crontab symbols to next execution: https://crontab.guru/\nSite for generating crontab syntax from dropdown options: https://crontab-generator.org/"
  },
  {
    "objectID": "slides/week-tbd--slides.html#lambda-coding-choices",
    "href": "slides/week-tbd--slides.html#lambda-coding-choices",
    "title": "Lecture 4",
    "section": "Lambda coding choices",
    "text": "Lambda coding choices\nLevels of Lambda complexity:\n\nCode written directly on the AWS Lambda console\nZip archive\nDocker Containers"
  },
  {
    "objectID": "slides/week-tbd--slides.html#lambda-basic-code-demo",
    "href": "slides/week-tbd--slides.html#lambda-basic-code-demo",
    "title": "Lecture 4",
    "section": "Lambda basic code demo",
    "text": "Lambda basic code demo\n\nBeyond any simple prototyping, you would probably never use this method.\nThe “hello world” example and then numbers with an endpoint\n\nhttps://frankcorso.dev/aws-lambda-function-endpoint-api-gateway.html"
  },
  {
    "objectID": "slides/week-tbd--slides.html#using-a-zip-archive-for-lambda",
    "href": "slides/week-tbd--slides.html#using-a-zip-archive-for-lambda",
    "title": "Lecture 4",
    "section": "Using a Zip archive for Lambda",
    "text": "Using a Zip archive for Lambda\n\nThe Lambda code (say the index.py file) and all the dependencies i.e. Python packages used by the code are packaged together in a Zip file, typically called function.zip.\n\nBe careful to always install these packages in a special directory specifically created for the Lambda function so to not override anything currently installed in your existing Python installation.\n\nThe function.zip file is put in an S3 bucket and the Lambda is configured to pull it from there.\nExample script for building a Lambda function.zip and uploading it to S3.\n\nAWS example of zip archive"
  },
  {
    "objectID": "slides/week-tbd--slides.html#lambda-with-docker-containers",
    "href": "slides/week-tbd--slides.html#lambda-with-docker-containers",
    "title": "Lecture 4",
    "section": "Lambda with Docker containers",
    "text": "Lambda with Docker containers\nAWS example Walkthrough post"
  },
  {
    "objectID": "slides/week-tbd--slides.html#dockerfile",
    "href": "slides/week-tbd--slides.html#dockerfile",
    "title": "Lecture 4",
    "section": "Dockerfile",
    "text": "Dockerfile\n```{bash}\n# syntax=docker/dockerfile:1\n\n# adapted from https://www.philschmid.de/aws-lambda-with-custom-docker-image\n# https://docs.aws.amazon.com/lambda/latest/dg/python-image.html\nFROM public.ecr.aws/lambda/python:3.9\n\n# copy requirements file and install necessary packages\nADD requirements.txt ${LAMBDA_TASK_ROOT}\nRUN pip3 install -r ${LAMBDA_TASK_ROOT}/requirements.txt --target \"${LAMBDA_TASK_ROOT}\"\n\n# Copy function code to docker container\nCOPY app.py ${LAMBDA_TASK_ROOT}\n\n# app (name of py file)\n# handler (name of function to execute for lambda job)\nCMD [ \"app.lambda_handler\" ]\n```"
  },
  {
    "objectID": "slides/week-tbd--slides.html#interactivity-with-lambda",
    "href": "slides/week-tbd--slides.html#interactivity-with-lambda",
    "title": "Lecture 4",
    "section": "Interactivity with Lambda?",
    "text": "Interactivity with Lambda?\nIf you “hack” it!"
  },
  {
    "objectID": "slides/week-tbd--slides.html#connecting-multiple-lambdas-together",
    "href": "slides/week-tbd--slides.html#connecting-multiple-lambdas-together",
    "title": "Lecture 4",
    "section": "Connecting multiple Lambdas together",
    "text": "Connecting multiple Lambdas together\nConnecting multiple lambda using AWS Step Functions"
  },
  {
    "objectID": "slides/week-tbd--slides.html#performance-optimization",
    "href": "slides/week-tbd--slides.html#performance-optimization",
    "title": "Lecture 4",
    "section": "Performance optimization",
    "text": "Performance optimization\nwarm start vs cold start\nDiscussion post on AWS"
  },
  {
    "objectID": "slides/week-tbd--slides.html#extra-links",
    "href": "slides/week-tbd--slides.html#extra-links",
    "title": "Lecture 4",
    "section": "Extra links",
    "text": "Extra links\nhttps://www.tatvasoft.com/blog/aws-lambda-vs-azure-functions/\nserverless web scraper with python and lambda - up to date simple example https://towardsdatascience.com/serverless-covid-19-data-scraper-with-python-and-aws-lambda-d6789a551b78"
  },
  {
    "objectID": "tech-ref/azure.html",
    "href": "tech-ref/azure.html",
    "title": "Microsoft Azure",
    "section": "",
    "text": "A fully functional Microsoft Azure individual subscription will be created for you with credits. Most Azure services are available in this account.\nFollow these instructions to setup your Azure environment. The screenshots may look a bit different than what you are seeing, but the flow is the same.\nA note on managing your credits:\nUnlike the AWS managed Vocareum environment, in Azure you will have to manually stop down your resources to avoid consuming your credits."
  },
  {
    "objectID": "tech-ref/azure.html#accept-the-azure-subscription-ownership",
    "href": "tech-ref/azure.html#accept-the-azure-subscription-ownership",
    "title": "Microsoft Azure",
    "section": "Accept the Azure subscription ownership",
    "text": "Accept the Azure subscription ownership\nIn this step, you will login to Azure for the first time and take ownership of a subscription that was created for you by Prof. Marck.\n\n\n\n\n\n\nNote\n\n\n\nThis step is only done once.\n\n\n\nSteps to accept and access your Azure subscription:\nPlease perform this step in a private browser window. You may not need to use a private browser window in the future, but it is recommended for this step.\n\nIn your student email inbox, you will have an email from Microsoft Azure azure-noreply@microsoft.com with the subject You’ve been asked to accept Azure subscription ownership. \nOpen the email.\nRight click in the blue box where it says Accept ownership and open the link in a private/incognito browser page\nIn the sign in page enter NetID@georgetown.edu \nAuthenticate with your GU credentials and two-factor-authentication \nYou may be prompted for additional information, click Next \nOnce you enter all additional information, verify and click looks good \nYou will continue the sign in process and asked if you would like to stayed sign in. Since you are using a private browser it doesn’t matter. \nYou will end up in the Accept subscription ownership page. Click Review + accept \n\n\n\n\n\n\n\nCaution\n\n\n\nWhen you get to this step, please pause a moment and wait.\nThe best strategy for the next step if you are in the classroom with others is to stagger the clicking of Accept because if too many people do it at once there is a high possibility of errors. Coordinate amongst yourselves to do no more than 10 at a time.\n\n\n\nClick Accept \nYou will end up in Welcome to Azure page! and your Azure account is ready to be used! \nLog out of Azure and log back in to ensure that everything us updated."
  },
  {
    "objectID": "tech-ref/azure.html#azuresignin",
    "href": "tech-ref/azure.html#azuresignin",
    "title": "Microsoft Azure",
    "section": "Sign in to the Azure Portal",
    "text": "Sign in to the Azure Portal\n\n\n\n\n\n\nImportant\n\n\n\nYou must have already accepted ownership of the Azure subscription created for you before you sign in.\n\n\nThe URL to sign in to the Azure Portal is https://portal.azure.com\n\nGo to the Azure Portal\nIn the sign in page, use your username in the form of NETID@georgetown.edu \nIf you get shown a page that looks like the one below, select Work or School Account \nAuthenticate with your institution’s two-factor-authentication\nWhen you finish signing in, you’ll be at the Azure Portal home page, and you are ready to work in Azure!"
  },
  {
    "objectID": "tech-ref/azure.html#find-your-azure-subscription-information",
    "href": "tech-ref/azure.html#find-your-azure-subscription-information",
    "title": "Microsoft Azure",
    "section": "Find your Azure Subscription information",
    "text": "Find your Azure Subscription information\n\nSign in to the Azure portal if you are not already signed in\nClick on Home or on Microsoft Azure in the top left\nClick on Subscriptions (the key icon). You will see a list of all subcriptions associated with you Azure account (which is in turn, associated with your Georgetown email)\nClick on the Subscriptions oval and select Show all subscriptions\nUncheck the checkbox that says Show only subscriptions selected in the global subscriptions filter\nYou will see a subscription called DSAN6000 Fall 2024 Individual_XXXXX_YYYYY where XXXX is your first name, and YYYY is your last name.\nClick on your subscription name\nIn this page you can see all information regarding your subscription. You may want to keep the Subscription ID handy for identification purposes"
  },
  {
    "objectID": "tech-ref/azure.html#create-an-azure-resource-group",
    "href": "tech-ref/azure.html#create-an-azure-resource-group",
    "title": "Microsoft Azure",
    "section": "Create an Azure resource group",
    "text": "Create an Azure resource group\nMicrosoft Azure uses a logical grouping called resource groups. Any resource you create in Azure has to be within a Resource Group. Resource groups typically have a default region, but you can still have resources deployed in multiple regions within a single resource group.\nFor this course, you will create a single resource group for all your Azure services in the East US 2 region. All the Azure services and resources you will use will be within this resource group.\n\n\n\n\n\n\nNote\n\n\n\nYou will create a resource group ony once. Once it is created you do not need to create one again.\n\n\n\nSign in to Azure if not already signed in. Follow the instructions in the sign in page.\nClick on Microsoft Azure in the top left to go to the Azure Portal home screen\nClick on Resource groups\nClick on the Subscription Filter and make sure your subscription is selected\nClick Create\nIn the Create a resource group page:\n\nSelect your subscription from the dropdown\nEnter a name for the resource group. The name can be anything. Use the course number DSAN6000\nSelect (US) East US 2 from the dropdown\n\nClick Review + create at the bottom left\nAfter validation passes, click Create\nOnce the RG is created, you’ll see it in the RG list (if your right subscriptions are selected in the filter)"
  },
  {
    "objectID": "tech-ref/azure.html#upload-your-public-ssh-key-to-azure",
    "href": "tech-ref/azure.html#upload-your-public-ssh-key-to-azure",
    "title": "Microsoft Azure",
    "section": "Upload your public ssh key to Azure",
    "text": "Upload your public ssh key to Azure\n\n\n\n\n\n\nNote\n\n\n\nYou will do this only once.\n\n\n\nClick on Home in the top left\nType ssh keys in the search bar, and click on SSH Keys service \nClick Create\nSelect the right subscription from the dropdown\nSelect the resource group you created earlier from the dropdown.\nGive the key a name (it’s just a name, it doesn’t matter what it is as long as you know what it is)\nSelect Upload existing public key from the SSH public key source dropdown \nIn the Upload key box, paste the contents of your public key:\n\n~/.ssh/id_rsa.pub (Mac/Linux)\nC:\\Users\\&lt;your_name&gt;\\.ssh\\id_rsa.pub (Windows)\n\nClick Review + create\nAfter validation passes, click Create\nOnce the SSH Key is uploaded, you’ll see it in the SSH Key list (if your right subscriptions are selected in the filter). You may have to hit “Refresh” on your browser for the page to update with your ssh key.\n\n."
  },
  {
    "objectID": "tech-ref/azure.html#create-and-launch-iaas-virtual-machine-on-azure",
    "href": "tech-ref/azure.html#create-and-launch-iaas-virtual-machine-on-azure",
    "title": "Microsoft Azure",
    "section": "Create and launch IAAS virtual machine on Azure",
    "text": "Create and launch IAAS virtual machine on Azure\n\nClick on Home in the top left\nType virtual machines in the search bar, and click on the Virtual machines service \nClick Create and then Azure virtual machine \nIn the Create a virtual machine page\n\nSelect the right subscription from the dropdown\nSelect the right resource group from the dropdown (the one you created earlier)\nIn the Instance details section:\n\nGive your VM a name\nLeave all other values unchanged\nFor the size, click see all sizes, select size B2s and click Select at the bottom \n\nIn the Administrator account section:\n\nLeave SSH public key checked\nLeave azureuser as Username\nSelect Use existing key stored in Azure from the SSH public key source dropdown\nSelect the key you uploaded earlier by name from the dropdown \n\nIn the Inbound port rules section:\n\nLeave Allow selected ports selected\nLeave SSH (22) in the _Select inbound ports dropdown \n\nClick Review + create\nAfter validation passes, click Create \n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can ignore the warning “You have set SSH port(s) open to the internet”. This is what we need for our setup.\n\n\n\nOnce the VM is created you’ll see a notice and click Go to resource\n\n \n\nYou will end up in the VM overview panel \nIn the VM overview panel, you can see the machine’s public IP address. You will need this to be able to connect to it. \n\n\nExplore all the resources that got created with a single virtual machine\nWhen you created the virtual machine, there were actually several other resources that were created at the same time which are tied to the virtual machine. You can see all the resources in the resource group by navigating to the resource group’s summary panel:\n\nNetworking resources\n\nA vnet(virtual network): a virtual network within Azure with it’s own networking address space where you can have many resources that can communicate with one-another. You can have several vnets.\nA network security group: a set of inbound/outbound networking rules (firewall) associated with a virtual network.\nA network interface: the actual “network card” that is associated with a given virtual machine. A single machine can have multuple network interfaces. Every network integrace also gets an internal IP address.\nA public IP address: a public IP address (fixed or rotating) that is associated with a network interface.\n\nComputing resources\n\nThe actual virtual machine (virtual hardware) which has a specific machine type\n\nStorage resources\n\nThe system hard drive that has the operating system for a given virtual machine. A machine must have a system drive, and it can also have several other drives"
  },
  {
    "objectID": "tech-ref/azure.html#connect-to-a-virtual-machine-using-secure-shell-ssh",
    "href": "tech-ref/azure.html#connect-to-a-virtual-machine-using-secure-shell-ssh",
    "title": "Microsoft Azure",
    "section": "Connect to a virtual machine using Secure Shell ssh",
    "text": "Connect to a virtual machine using Secure Shell ssh\n\n\n\n\n\n\nImportant\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet.\n\n\n\nGo to the VM Overview page for the VM you just created. You can get there multiple ways:\n\n\nFrom the home screen\nFrom the resource group summary page\nFrom the virtual machines list\n\n\nIn the Essentials section on the overview panel, you can see the public IP address. Copy that.\n\n\n\n\n\n\n\nNote\n\n\n\nThe IP address may change every time you start the same virtual machine from a stopped (paused) state.\n\n\n\nOpen a terminal on your laptop. If you have one already open, type cd ~ to navigate back to your home directory\nType ssh azureuser@ and paste your IP address\n\n\nIf everything is configured correctly, the first time you connect to a host that you’ve never connected to, you will see something like this:\n\nThe authenticity of host 'xxx.xxx.xxx.xxx (xxx.xxx.xxx.xxx)' can't be established.\nECDSA key fingerprint is SHA256:TeYrgHLkYHvD/zcp23bO3wozsLMyPSiSn+edPPo88zE.\nAre you sure you want to continue connecting (yes/no)?\nNote: your values for IP address and fingerprint will be different\n\nEnter yes and press enter. You will only need to enter yes once.\nIf you are successful, you will see something like this:\n\nWarning: Permanently added '20.7.145.120' (ED25519) to the list of known hosts.\nWelcome to Ubuntu 20.04.4 LTS (GNU/Linux 5.15.0-1017-azure x86_64)\n\n * Documentation:  https://help.ubuntu.com\n * Management:     https://landscape.canonical.com\n * Support:        https://ubuntu.com/advantage\n\n  System information as of Tue Sep  6 16:32:56 UTC 2022\n\n  System load:  0.0               Processes:             112\n  Usage of /:   5.0% of 28.89GB   Users logged in:       0\n  Memory usage: 7%                IPv4 address for eth0: 10.0.0.4\n  Swap usage:   0%\n\n * Super-optimized for small spaces - read how we shrank the memory\n   footprint of MicroK8s to make it the smallest full K8s around.\n\n   https://ubuntu.com/blog/microk8s-memory-optimisation\n\n1 update can be applied immediately.\nTo see these additional updates run: apt list --upgradable\n\n\nThe list of available updates is more than a week old.\nTo check for new updates run: sudo apt update\n\n\nThe programs included with the Ubuntu system are free software;\nthe exact distribution terms for each program are described in the\nindividual files in /usr/share/doc/*/copyright.\n\nUbuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by\napplicable law.\n\nTo run a command as administrator (user \"root\"), use \"sudo &lt;command&gt;\".\nSee \"man sudo_root\" for details.\n\nazureuser@my-vm-name:~$\nCongratulations, you have successfully connected to your VM on Azure"
  },
  {
    "objectID": "tech-ref/azure.html#stop-pause-the-virtual-machine",
    "href": "tech-ref/azure.html#stop-pause-the-virtual-machine",
    "title": "Microsoft Azure",
    "section": "Stop (pause) the virtual machine",
    "text": "Stop (pause) the virtual machine\nThere are two concepts you need to understand:\n\nStopping a VM is like turning the machine off. The current state is saved and you can turn it on again later. No hourly charges are incurred while in this state (although some storage charges may occur.)\nDeleting a VM completely removes it. This cannot be undone.\n\n\nGo to the VM overview panel\nClick on Stop\nConfirm you want to stop it by clicking OK\nTo start again in the future, click on Restart, though the IP address may change"
  },
  {
    "objectID": "tech-ref/azure.html#delete-a-virtual-machine",
    "href": "tech-ref/azure.html#delete-a-virtual-machine",
    "title": "Microsoft Azure",
    "section": "Delete a virtual machine",
    "text": "Delete a virtual machine\n\nGo to the VM Overview page for the VM you just created. You can get there multiple ways.\nClick on Delete\nSelect all associated resources to the VM (OS Disk, Network interfaces, and Public addresses) to be deleted\nConfirm you want to delete it by clicking OK"
  },
  {
    "objectID": "tech-ref/azure.html#deploy-azure-machine-learning-azureml",
    "href": "tech-ref/azure.html#deploy-azure-machine-learning-azureml",
    "title": "Microsoft Azure",
    "section": "Deploy Azure Machine Learning (AzureML)",
    "text": "Deploy Azure Machine Learning (AzureML)\n\n\n\n\n\n\nNote\n\n\n\nYou will only deploy the service once.\n\n\n\nSign in to the Azure Portal as described in the sign in page\nIn the search bar at the top, type azure machine learning\nClick on Azure Machine Learning entry under Services\nFrom the Azure Machine Learning service page (which will be empty), Click on + Create\nSelect New workspace from the dropdown\nSelect your subscription from the Subscription drow-down\nSelect the resource group you crated from the dropdown\nEnter aml as the Name. You will notice that Storage account, Key vault, and Application insights values will auto-populate, and the Container registry value will be “None”. Do not change anything in those four fields.\nMake sure the region is East US 2\nClick Review + create\nClick Create once validation passes\n\nYour AzureML workspace will begin to be deployed. You’ll see a notification when it’s done."
  },
  {
    "objectID": "tech-ref/azure.html#access-azureml-studio",
    "href": "tech-ref/azure.html#access-azureml-studio",
    "title": "Microsoft Azure",
    "section": "Access AzureML Studio",
    "text": "Access AzureML Studio\nOnce the workspace is deployed you can access it by a direct URL (bookmark this): https://ml.azure.com/.\nIf you are not logged into Azure you will be asked to log in.\n\nCreate Compute Instance within AzureML\nThis is a managed compute instance, separate from the one you created as IAAS.\n\n\n\n\n\n\nImportant\n\n\n\nYou will start with creating (and using) a single compute instance. To create additional compute instances, the process is the same.\n\n\n\nLog into AzureML Studio\nSelect your workspace\nClick on Compute on the left-hand navigation panel (towards the bottom)\nClick + New\nEnter a Compute name (you can edit the default value.) This value has to be globally unique, so we recommend calling the compute instance dsan6000-&lt;NETID&gt;-ci01 (change  to your own.)\nSelect CPU\nSelect the Standard_E4ds_v4 instance size\nClick Next\nMake sure Auto shut down is enabled and change the value to 30\nClick Review + Create\nVerify all settings\nClick Create\n\nWhen the compute instance gets created it will be running. It should shut-down after 30 minutes of inactivity."
  }
]