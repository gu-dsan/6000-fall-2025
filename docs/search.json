[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "content/01-content.html",
    "href": "content/01-content.html",
    "title": "PPOL 5206",
    "section": "",
    "text": "Course overview, big data concepts, cloud computing and evolution of cloud technologies."
  },
  {
    "objectID": "content/01-content.html#readings",
    "href": "content/01-content.html#readings",
    "title": "PPOL 5206",
    "section": "Readings",
    "text": "Readings\nNo prescribed readings for this lecture, but you should have completed Lab 0 - Background Skills before this class."
  },
  {
    "objectID": "content/01-content.html#slides",
    "href": "content/01-content.html#slides",
    "title": "PPOL 5206",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys."
  },
  {
    "objectID": "content/01-content.html#lab",
    "href": "content/01-content.html#lab",
    "title": "PPOL 5206",
    "section": "Lab",
    "text": "Lab\nThe lab for today’s lesson is available online as an HTML file."
  },
  {
    "objectID": "content/01-content.html#assignment",
    "href": "content/01-content.html#assignment",
    "title": "PPOL 5206",
    "section": "Assignment",
    "text": "Assignment\nGitHub Classroom Link"
  },
  {
    "objectID": "content/02-content.html",
    "href": "content/02-content.html",
    "title": "PPOL 5206",
    "section": "",
    "text": "Introduction to cloud services to be used throughout the semester."
  },
  {
    "objectID": "content/02-content.html#readings",
    "href": "content/02-content.html#readings",
    "title": "PPOL 5206",
    "section": "Readings",
    "text": "Readings\nReadings for this lecture (to be completed before this class):\n- Laberis - What is Cloud\n- Rittinghouse - The Evolution of Cloud"
  },
  {
    "objectID": "content/02-content.html#slides",
    "href": "content/02-content.html#slides",
    "title": "PPOL 5206",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys."
  },
  {
    "objectID": "content/02-content.html#lab",
    "href": "content/02-content.html#lab",
    "title": "PPOL 5206",
    "section": "Lab",
    "text": "Lab\nAccessing cloud services, starting and connecting to a virtual machine, S3 commands. The lab for today’s lesson is available online as an HTML file."
  },
  {
    "objectID": "content/02-content.html#assignment",
    "href": "content/02-content.html#assignment",
    "title": "PPOL 5206",
    "section": "Assignment",
    "text": "Assignment\nGitHub Classroom Link"
  },
  {
    "objectID": "content/03-content.html",
    "href": "content/03-content.html",
    "title": "PPOL 5206",
    "section": "",
    "text": "Scaling up on a single machine with Python multiprocessing."
  },
  {
    "objectID": "content/03-content.html#readings",
    "href": "content/03-content.html#readings",
    "title": "PPOL 5206",
    "section": "Readings",
    "text": "Readings\nReadings for this lecture (to be completed before this class):\nWolohan Ch. 1,2,5"
  },
  {
    "objectID": "content/03-content.html#slides",
    "href": "content/03-content.html#slides",
    "title": "PPOL 5206",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys."
  },
  {
    "objectID": "content/03-content.html#lab",
    "href": "content/03-content.html#lab",
    "title": "PPOL 5206",
    "section": "Lab",
    "text": "Lab\nParallelization with Python and multiprocessing. The lab for today’s lesson is available online as an HTML file."
  },
  {
    "objectID": "content/03-content.html#assignment",
    "href": "content/03-content.html#assignment",
    "title": "PPOL 5206",
    "section": "Assignment",
    "text": "Assignment\nGitHub Classroom Link"
  },
  {
    "objectID": "content/04-content.html",
    "href": "content/04-content.html",
    "title": "PPOL 5206",
    "section": "",
    "text": "Scaling out: MapReduce, Hadoop, distributed filesystems, Hadoop Streaming."
  },
  {
    "objectID": "content/04-content.html#readings",
    "href": "content/04-content.html#readings",
    "title": "PPOL 5206",
    "section": "Readings",
    "text": "Readings\nReadings for this lecture (to be completed before this class):\n- Wolohan Ch.7\n- Ghemawat et.al - The Google File System\n- Dean, Ghemawat - MapReduce"
  },
  {
    "objectID": "content/04-content.html#slides",
    "href": "content/04-content.html#slides",
    "title": "PPOL 5206",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys."
  },
  {
    "objectID": "content/04-content.html#lab",
    "href": "content/04-content.html#lab",
    "title": "PPOL 5206",
    "section": "Lab",
    "text": "Lab\nStarting a cluster, running a Hadoop job with EMR on AWS. The lab for today’s lesson is available online as an HTML file."
  },
  {
    "objectID": "content/04-content.html#assignment",
    "href": "content/04-content.html#assignment",
    "title": "PPOL 5206",
    "section": "Assignment",
    "text": "Assignment\nGitHub Classroom Link"
  },
  {
    "objectID": "content/05-content.html",
    "href": "content/05-content.html",
    "title": "PPOL 5206",
    "section": "",
    "text": "Scaling up data analytics with Dask."
  },
  {
    "objectID": "content/05-content.html#readings",
    "href": "content/05-content.html#readings",
    "title": "PPOL 5206",
    "section": "Readings",
    "text": "Readings\nReadings for this lecture (to be completed before this class):\n- Rocklin"
  },
  {
    "objectID": "content/05-content.html#slides",
    "href": "content/05-content.html#slides",
    "title": "PPOL 5206",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys."
  },
  {
    "objectID": "content/05-content.html#lab",
    "href": "content/05-content.html#lab",
    "title": "PPOL 5206",
    "section": "Lab",
    "text": "Lab\nThe lab for today’s lesson is available online as an HTML file."
  },
  {
    "objectID": "content/05-content.html#assignment",
    "href": "content/05-content.html#assignment",
    "title": "PPOL 5206",
    "section": "Assignment",
    "text": "Assignment\nGitHub Classroom Link"
  },
  {
    "objectID": "content/06-content.html",
    "href": "content/06-content.html",
    "title": "PPOL 5206",
    "section": "",
    "text": "Spark 1: Introduction to Spark, Spark RDDs."
  },
  {
    "objectID": "content/06-content.html#readings",
    "href": "content/06-content.html#readings",
    "title": "PPOL 5206",
    "section": "Readings",
    "text": "Readings\nReadings for this lecture (to be completed before this class):\n- Damji et.al Ch.1,2,3"
  },
  {
    "objectID": "content/06-content.html#slides",
    "href": "content/06-content.html#slides",
    "title": "PPOL 5206",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys."
  },
  {
    "objectID": "content/06-content.html#lab",
    "href": "content/06-content.html#lab",
    "title": "PPOL 5206",
    "section": "Lab",
    "text": "Lab\nThe lab for today’s lesson is available online as an HTML file."
  },
  {
    "objectID": "content/06-content.html#assignment",
    "href": "content/06-content.html#assignment",
    "title": "PPOL 5206",
    "section": "Assignment",
    "text": "Assignment\nGitHub Classroom Link"
  },
  {
    "objectID": "content/07-content.html",
    "href": "content/07-content.html",
    "title": "PPOL 5206",
    "section": "",
    "text": "Spark 2: Spark DataFrames and SparkSQL."
  },
  {
    "objectID": "content/07-content.html#readings",
    "href": "content/07-content.html#readings",
    "title": "PPOL 5206",
    "section": "Readings",
    "text": "Readings\nNo prescribed readings for this lecture."
  },
  {
    "objectID": "content/07-content.html#slides",
    "href": "content/07-content.html#slides",
    "title": "PPOL 5206",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys."
  },
  {
    "objectID": "content/07-content.html#lab",
    "href": "content/07-content.html#lab",
    "title": "PPOL 5206",
    "section": "Lab",
    "text": "Lab\nThe lab for today’s lesson is available online as an HTML file."
  },
  {
    "objectID": "content/07-content.html#assignment",
    "href": "content/07-content.html#assignment",
    "title": "PPOL 5206",
    "section": "Assignment",
    "text": "Assignment\nGitHub Classroom Link"
  },
  {
    "objectID": "content/08-content.html",
    "href": "content/08-content.html",
    "title": "PPOL 5206",
    "section": "",
    "text": "Spark 3: Machine Learning with SparkML."
  },
  {
    "objectID": "content/08-content.html#readings",
    "href": "content/08-content.html#readings",
    "title": "PPOL 5206",
    "section": "Readings",
    "text": "Readings\nNo prescribed readings for this lecture."
  },
  {
    "objectID": "content/08-content.html#slides",
    "href": "content/08-content.html#slides",
    "title": "PPOL 5206",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys."
  },
  {
    "objectID": "content/08-content.html#lab",
    "href": "content/08-content.html#lab",
    "title": "PPOL 5206",
    "section": "Lab",
    "text": "Lab\nThe lab for today’s lesson is available online as an HTML file."
  },
  {
    "objectID": "content/08-content.html#assignment",
    "href": "content/08-content.html#assignment",
    "title": "PPOL 5206",
    "section": "Assignment",
    "text": "Assignment\nNo assignment for this class."
  },
  {
    "objectID": "content/09-content.html",
    "href": "content/09-content.html",
    "title": "PPOL 5206",
    "section": "",
    "text": "Spark 4: NLP with Spark."
  },
  {
    "objectID": "content/09-content.html#readings",
    "href": "content/09-content.html#readings",
    "title": "PPOL 5206",
    "section": "Readings",
    "text": "Readings\nNo prescribed readings for this lecture."
  },
  {
    "objectID": "content/09-content.html#slides",
    "href": "content/09-content.html#slides",
    "title": "PPOL 5206",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys."
  },
  {
    "objectID": "content/09-content.html#lab",
    "href": "content/09-content.html#lab",
    "title": "PPOL 5206",
    "section": "Lab",
    "text": "Lab\nThe lab for today’s lesson is available online as an HTML file."
  },
  {
    "objectID": "content/09-content.html#assignment",
    "href": "content/09-content.html#assignment",
    "title": "PPOL 5206",
    "section": "Assignment",
    "text": "Assignment\nNo assignment for this class."
  },
  {
    "objectID": "content/10-content.html",
    "href": "content/10-content.html",
    "title": "PPOL 5206",
    "section": "",
    "text": "Spark 5: Spark Streaming."
  },
  {
    "objectID": "content/10-content.html#readings",
    "href": "content/10-content.html#readings",
    "title": "PPOL 5206",
    "section": "Readings",
    "text": "Readings\nNo prescribed readings for this lecture."
  },
  {
    "objectID": "content/10-content.html#slides",
    "href": "content/10-content.html#slides",
    "title": "PPOL 5206",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys."
  },
  {
    "objectID": "content/10-content.html#lab",
    "href": "content/10-content.html#lab",
    "title": "PPOL 5206",
    "section": "Lab",
    "text": "Lab\nThe lab for today’s lesson is available online as an HTML file."
  },
  {
    "objectID": "content/10-content.html#assignment",
    "href": "content/10-content.html#assignment",
    "title": "PPOL 5206",
    "section": "Assignment",
    "text": "Assignment\nNo assignment for this class."
  },
  {
    "objectID": "content/11-content.html",
    "href": "content/11-content.html",
    "title": "PPOL 5206",
    "section": "",
    "text": "Lambda & Docker."
  },
  {
    "objectID": "content/11-content.html#readings",
    "href": "content/11-content.html#readings",
    "title": "PPOL 5206",
    "section": "Readings",
    "text": "Readings\nNo prescribed readings for this lecture."
  },
  {
    "objectID": "content/11-content.html#slides",
    "href": "content/11-content.html#slides",
    "title": "PPOL 5206",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys."
  },
  {
    "objectID": "content/11-content.html#lab",
    "href": "content/11-content.html#lab",
    "title": "PPOL 5206",
    "section": "Lab",
    "text": "Lab\nThe lab for today’s lesson is available online as an HTML file."
  },
  {
    "objectID": "content/11-content.html#assignment",
    "href": "content/11-content.html#assignment",
    "title": "PPOL 5206",
    "section": "Assignment",
    "text": "Assignment\nNo assignment for this class."
  },
  {
    "objectID": "content/12-content.html",
    "href": "content/12-content.html",
    "title": "PPOL 5206",
    "section": "",
    "text": "Data Engineering."
  },
  {
    "objectID": "content/12-content.html#readings",
    "href": "content/12-content.html#readings",
    "title": "PPOL 5206",
    "section": "Readings",
    "text": "Readings\nNo prescribed readings for this lecture."
  },
  {
    "objectID": "content/12-content.html#slides",
    "href": "content/12-content.html#slides",
    "title": "PPOL 5206",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys."
  },
  {
    "objectID": "content/12-content.html#lab",
    "href": "content/12-content.html#lab",
    "title": "PPOL 5206",
    "section": "Lab",
    "text": "Lab\nThe lab for today’s lesson is available online as an HTML file."
  },
  {
    "objectID": "content/12-content.html#assignment",
    "href": "content/12-content.html#assignment",
    "title": "PPOL 5206",
    "section": "Assignment",
    "text": "Assignment\nNo assignment for this class."
  },
  {
    "objectID": "content/13-content.html",
    "href": "content/13-content.html",
    "title": "PPOL 5206",
    "section": "",
    "text": "Ray, RAPIDS and DuckDB"
  },
  {
    "objectID": "content/13-content.html#readings",
    "href": "content/13-content.html#readings",
    "title": "PPOL 5206",
    "section": "Readings",
    "text": "Readings\nNo prescribed readings for this lecture, but you should have completed Lab 0 - Background Skills before this class."
  },
  {
    "objectID": "content/13-content.html#slides",
    "href": "content/13-content.html#slides",
    "title": "PPOL 5206",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys."
  },
  {
    "objectID": "content/13-content.html#lab",
    "href": "content/13-content.html#lab",
    "title": "PPOL 5206",
    "section": "Lab",
    "text": "Lab\nThe lab for today’s lesson is available online as an HTML file."
  },
  {
    "objectID": "content/13-content.html#assignment",
    "href": "content/13-content.html#assignment",
    "title": "PPOL 5206",
    "section": "Assignment",
    "text": "Assignment\nNo assignment for this class."
  },
  {
    "objectID": "content/14-content.html",
    "href": "content/14-content.html",
    "title": "PPOL 5206",
    "section": "",
    "text": "Project discussion and open session."
  },
  {
    "objectID": "content/14-content.html#readings",
    "href": "content/14-content.html#readings",
    "title": "PPOL 5206",
    "section": "Readings",
    "text": "Readings\nNo prescribed readings for this lecture, but you should have completed Lab 0 - Background Skills before this class."
  },
  {
    "objectID": "content/14-content.html#slides",
    "href": "content/14-content.html#slides",
    "title": "PPOL 5206",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys."
  },
  {
    "objectID": "content/14-content.html#lab",
    "href": "content/14-content.html#lab",
    "title": "PPOL 5206",
    "section": "Lab",
    "text": "Lab\nThe lab for today’s lesson is available online as an HTML file."
  },
  {
    "objectID": "content/14-content.html#assignment",
    "href": "content/14-content.html#assignment",
    "title": "PPOL 5206",
    "section": "Assignment",
    "text": "Assignment\nNo assignment for this class."
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Lectures and labs",
    "section": "",
    "text": "Each class is divided into two parts: a lecture followed by a lab. Some class optionally have a reading component also that needs to be completed before class.\n\nLecture\nDuring the lecture, I will be going over the slides for that lecture, this would usually include a quick review of the previous class, feedback on a recently graded assignment and then cover the topic of the day.\nThe lecture would usually last about 90 minutes, followed by 5 to 10 minutes break.\n\n\nLab\nThe lab for the class would involve a hands-on coding assignment provided through GitHub Classroom. You will start the lab in-class, myself and the TAs would be helping you with any questions with the lab and then you would need to turn in the lab by checking in your code and results in the GitHub repo (you will have until next class for this, but usually you would be able to do this much sooner)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PPOL 5206",
    "section": "",
    "text": "Big Data Analytics & Cloud Computing\n        \n        \n            “Learn how to analyze Big Data using cloud computing and technologies such as Concurrency, Spark and others in this hands-on, practical workshop-style course.”\n        \n        \n            PPOL 5206 • Spring 2024Amit Arora, McCourt School of Public PolicyGeorgetown University\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\n\nInstructor\n\n   Amit Arora\n   Online\n   aa1603@georgetown.edu\n   aarora79\n   Schedule an appointment\n\n\n\nCourse details\n\n   Thursdays\n   January 11 – May 9, 2024\n   6:30–9:00 PM\n   Car Barn 204\n   Slack\n\n\n\nContacting me\nE-mail and Slack are the best ways to get in contact with me. I will try to respond to all course-related e-mails and Slack messages within 24 hours (really), but also remember that life can be busy and chaotic for everyone (including me!), so if I don’t respond right away, don’t worry!"
  },
  {
    "objectID": "labs/01-labs.html",
    "href": "labs/01-labs.html",
    "title": "Lab 1",
    "section": "",
<<<<<<< HEAD
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet."
=======
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet. #"
>>>>>>> 5cfd7cc16e257c0a05ca9589ee545fcbfa2d7d0d
  },
  {
    "objectID": "labs/01-labs.html#goals",
    "href": "labs/01-labs.html#goals",
    "title": "Lab 1",
    "section": "Goals",
    "text": "Goals\n\nCreating your public/private ssh key pair and knowing where to find the files\nLearning to use GitHub Codespaces IDE\nLearning the Linux Shell\nBASH Exercise"
  },
  {
    "objectID": "labs/01-labs.html#windows-users",
    "href": "labs/01-labs.html#windows-users",
    "title": "Lab 1",
    "section": "Windows Users",
    "text": "Windows Users\nWindows users will be using the Windows Powershell: <https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/powershell>\nWindows Powershell is most likely installed if you have Windows 10. If you don’t have Powershell, take a look at this article: https://www.howtogeek.com/336775/how-to-enable-and-use-windows-10s-built-in-ssh-commands/ that explains how to install it.\nYou can find Powershell by typing “Powershell” into the search bar:\n\nOnce Powershell is running, this is your terminal:\n\n\nAdditional Powershell Configuration (you must do this!)\nYou need to perform this step **only once** to be able to use agent forwarding which is explained further in the lab.\n\nExit Powershell if running\nStart a new Powershell session using run as Administrator\nEnter the following command (you can cut/paste from here):\n\n\nGet-Service -Name ssh-agent | Set-Service -StartupType Manual\n\nExit Powershell. You should not need to run as administrator going forward."
  },
  {
    "objectID": "labs/01-labs.html#mac-and-linux-users",
    "href": "labs/01-labs.html#mac-and-linux-users",
    "title": "Lab 1",
    "section": "Mac and Linux Users",
    "text": "Mac and Linux Users\nFor Mac and Linux users, you will open up the Terminal.\n\nMacs and Linux have a built in Terminal.\nOr, you can use iTerm app: <https://www.iterm2.com/>\n\nIf you are on Linux (but not on Mac), you can open the terminal by using Ctrl-Alt-T."
  },
  {
    "objectID": "labs/01-labs.html#ssh-keypair-setup",
    "href": "labs/01-labs.html#ssh-keypair-setup",
    "title": "Lab 1",
    "section": "SSH Keypair Setup",
    "text": "SSH Keypair Setup\nWhen you want to connect to a remote machine, the method is called “Secure Shell”. This creates a connection between the local machine (where your terminal window lives) and the “remote” machine (where the commands you will send actually execute). In order for the local and remote machines to authenticate (trust) each other, we have to create a special password-like files called a keypair. It is called a keypair because there is a public version and a private version. Read more about SSH Keys here.\nNOTE: You only need to create your ssh public/private keypair one time only. If you already have a public/private keypair on your laptop let us know.\n\nOpen a terminal (on your laptop) if not already open. By default, every time you open a terminal it will open in your home directory.\nAt the command prompt run the following command: ssh-keygen -t rsa -b 2048 and press enter\nYou will see this prompt, just press enter\n\n\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/User/.ssh/id_rsa):\n\nYou will see this prompt, just press enter\n\n\nCreated directory '/home/User/.ssh'.\nEnter passphrase (empty for no passphrase):\n\nYou will see this prompt, just press enter\n\n\nEnter same passphrase again:\n\nYou will see these messages (your randomart will look different) and your keypair has been created.\n\n\nYour identification has been saved in /home/User_name/.ssh/id_rsa.\nYour public key has been saved in /home/User_name/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:xPJtMLmJSO73x/NQo3qMpqF6r6St4ONmshS8QZqfmHA User_name@WinDev1802Eval\n\nThe key's randomart image is:\n+---[RSA 2048]----+\n|                 |\n|       . .       |\n| .  . . *        |\n|+. o . = *       |\n|++E o . S o o    |\n|.=+o     . o .   |\n|+oo o o  +o      |\n|+= +.o oo.*.     |\n|*+=++ooooo o.    |\n+----[SHA256]-----+"
  },
  {
    "objectID": "labs/01-labs.html#see-your-key-files",
    "href": "labs/01-labs.html#see-your-key-files",
    "title": "Lab 1",
    "section": "See your key files",
    "text": "See your key files\n\nOpen a terminal if not already open\nChange to your .ssh directory\n\nThis is a hidden directory so if you list your files using ls you won’t see it. For seeing all files, use ls -la.\nTo change into the .ssh directory type cd .ssh\n\nType pwd to print your current working directory.\n\nWindows users in Powershell will see:\n\n\nPS C:\\\\Users\\\\your_name\\.ssh> pwd\n\nPath\n----\nC:\\\\Users\\\\your_name\\.ssh\n\n\nPS C:\\\\Users\\\\your_name\\.ssh>\n\nMac users will see:\n\n\npwd\n/Users/myusername/.ssh\n\nLinux users will see:\n\n\n\\$ pwd\n\n/home/myusername/.ssh\n\nNext, we need to open the new key file we just made.\n\nType ls to list the files in the directory.\n\nWhat is displayed may look different. You will not have a config file unless you have already created one.\n\nType ls -la to list all the files in the directory, even the hidden ones.\n\nWhat is displayed may look different. You will not have a config file unless you have already created one."
  },
  {
    "objectID": "labs/01-labs.html#get-your-public-key-info",
    "href": "labs/01-labs.html#get-your-public-key-info",
    "title": "Lab 1",
    "section": "Get your public key info",
    "text": "Get your public key info\n\nThe file id_rsa is your private key and this file will not leave your computer.\nThe file id_rsa.pub is the public key, whose contents we will upload to cloud services so you authenticate.\nThe known_hosts is a file that gets generated as you connect to different remote systems.\n\nThis is useful so you know you are connecting to the same server as previous times.\n\n\n\n\\$ ls -la\n\ntotal 32\n\ndrwxr-xr-x  6  your_name staff   192 May 29 20:39 .\ndrwxr-xr-x+ 75 your_name staff  2400 May 30 13:35 ..\n-rw-r--r--  1  your_name staff   181 May 29 15:50 config\n-r--------  1  your_name staff  3243 May 29 15:50 id_rsa\n-rw-r--r--  1  your_name staff   742 May 29 15:50 id_rsa.pub\n-rw-r--r--  1  your_name staff   363 May 29 20:42 known_hosts\n\nView the contents of your public_key file by running the command cat id_rsa.pub\n\nWhat is shown is a sample public key, yours will be different\n\n\n\n\\$ cat id_rsa.pub\n\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCnKuIRXwZu0JZH0/Q2XNrYYTaJT7bMtXGhGQaSSOZs6MhQ4SkSbHiygO7RauQf741buLnASzY27GKMMMml6InwfxJWrF60KhNK0r869POQkuZa9v9/cmYcEIzmAJe1xRPABEZ2yfbTG9Wq4sg9cU0mwt1Bx7wiN4QNf0Bak62EC8JWTbcKLduuzO1zabIb5xW9gfR9b4K3HwmqRLl18S8bNsfYQZfvtlwd0mCWQUeuEGbDOgqh//nLIj6DeXdyxbD5xrz79iOAuAK2nXAjNCEtKpxNGQr2Py7aWQjlH+U5laDEHVg4hzmBY7yoZ5eC3Ye45yPqpQA1y8JrbXVhPJRP User\\@WinDev1802Eval"
  },
  {
    "objectID": "labs/01-labs.html#pubkey",
    "href": "labs/01-labs.html#pubkey",
    "title": "Lab 1",
    "section": "Extracting your public key",
    "text": "Extracting your public key\n\nOpen a text editor (Notepad on Windows or Textpad on Mac, NOT MICROSOFT WORD) and select the output of your terminal with all the text from the ssh-rsa beginning all the way to the end, and paste it in your text editor as-is. We will use this in the next step.\n\nYou can also just copy/paste from your terminal screen.\nOn a Mac, you can also copy the contents of the id_rsa.pub file using\n\n\n\npbcopy < id_rsa.pub"
  },
  {
    "objectID": "labs/01-labs.html#adding-ssh-key-to-github",
    "href": "labs/01-labs.html#adding-ssh-key-to-github",
    "title": "Lab 1",
    "section": "Adding SSH Key to GitHub",
    "text": "Adding SSH Key to GitHub\n\na) Create a GitHub Account if you do not already have one\nGo to www.github.com to create a GitHub account if you do not already have one. Your username has to be globally unique, and the email address you use to register GitHub can be any email address you own.\n\n\nb) Upload your Public key to GitHub\n\nLog into to your GitHub account if you are not already logged in\nClick on your profile icon on the top-right of the screen and select Settings from the dropdown\nClick on SSH and GPG keys from the left hand menu\nClick on the New SSH key button on the top-right\nGive your key a name. This is just a name and is meaningful to you.\nPaste the contents of the public key in the Key box. Leave the “Key Type” dropdown as “Authentication Key”.\nClick the Add SSH Key button\n\n\n\nc) Test that your ssh key works with GitHub\n\nOpen a terminal if not already open on your laptop\nAt the command prompt, type ssh -T git@github.com and press enter to test. If it works, you will see something like this, with your GitHub username:\n\n\nThe authenticity of host 'github.com (192.30.253.112)' can't be established.\nRSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8.\nAre you sure you want to continue connecting (yes/no)? yes\nWarning: Permanently added 'github.com,192.30.253.112' (RSA) to the list of known hosts.\nHi wahalulu! You've successfully authenticated, but GitHub does not provide shell access.\nYou are now ready to use ssh authentication with GitHub."
  },
  {
    "objectID": "labs/01-labs.html#create-a-personal-access-token-on-github",
    "href": "labs/01-labs.html#create-a-personal-access-token-on-github",
    "title": "Lab 1",
    "section": "Create a Personal Access Token on GitHub",
    "text": "Create a Personal Access Token on GitHub\n\nLog into to your GitHub account if you are not already logged in\nClick on your profile icon on the top-right of the screen and select Settings from the dropdown\nClick Developer settings\nClick the Personal access tokens tab\nClick the Generate new token button\nEnter a token description (you can call it big-data-class)\nSelect the repo permission, and then click the Generate token button\n\n\n\nCopy the token and save it in a text file. You will need this token later on in the semester and if you lose it you will need to re-generate a token"
  },
  {
    "objectID": "labs/01-labs.html#github-codespaces-ide",
    "href": "labs/01-labs.html#github-codespaces-ide",
    "title": "Lab 1",
    "section": "GitHub Codespaces IDE",
    "text": "GitHub Codespaces IDE\nCodespaces is an integrated developer environment (IDE), which provides you with a VS Code environment with a cloud computing backend. The repository is automatically loaded into your environment and you do not need any additional authentication steps to push to your repo. Read more here about Codespaces.\nIn this lab, we use Codespaces to get you familiar with the Linux terminal.\nYou will not use Codespaces for most of your work this semester. If you were just exploring code or making minor changes to a project, Codespaces could be a simple solution."
  },
  {
    "objectID": "labs/01-labs.html#launching-codespaces",
    "href": "labs/01-labs.html#launching-codespaces",
    "title": "Lab 1",
    "section": "Launching Codespaces",
    "text": "Launching Codespaces\nOpen your Git repo for the lab. Launch Codespaces from your repo by clicking on the green code button, then select “codespaces” then click “Create codespace on main”.\n\nWait for the workspace to be created. It will set up the computing instance with a screen like this."
  },
  {
    "objectID": "labs/01-labs.html#linux-terminal-in-codespaces",
    "href": "labs/01-labs.html#linux-terminal-in-codespaces",
    "title": "Lab 1",
    "section": "Linux Terminal in Codespaces",
    "text": "Linux Terminal in Codespaces\nYou now have full VS Code capabilities. The terminal in Cloud9 is the lower window. You are connected to a cloud computing instance and are able to run BASH commands (along with other programming languages) there.\n\nCheck out the cheat sheet on Linux commands here."
  },
  {
    "objectID": "labs/02-labs.html",
    "href": "labs/02-labs.html",
    "title": "Lab 2 Amazon Web Services (AWS) Setup Instructions",
    "section": "",
<<<<<<< HEAD
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet.\nFollow these instructions step-by-step to setup your AWS environment. The screenshots may look a bit different than what you are seeing, but the flow is the same."
=======
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet. #\nFollow these instructions step-by-step to setup your AWS environment. The screenshots may look a bit different than what you are seeing, but the flow is the same."
>>>>>>> 5cfd7cc16e257c0a05ca9589ee545fcbfa2d7d0d
  },
  {
    "objectID": "labs/02-labs.html#login-into-the-aws-console",
    "href": "labs/02-labs.html#login-into-the-aws-console",
    "title": "Lab 2 Amazon Web Services (AWS) Setup Instructions",
    "section": "Login into the AWS Console",
    "text": "Login into the AWS Console\nThe AWS Console is your entry point into the AWS cloud.\n\nClick on the AWS link alongside the ⬤. \nA new tab will open in your browser, this is the AWS Console. \nNote the URL in your browser’s address bar, it will start with the name of the AWS region (such as us-east-1) in which your cloud resources are hosted.\nNote the username on the top right hand corner, this is your Federated Identity. Also note that the you did not have to provide any credentials (username/password) to login into the AWS console. How did this happen?"
  },
  {
    "objectID": "labs/02-labs.html#the-sso-link-to-login-into-the-aws-console-optional",
    "href": "labs/02-labs.html#the-sso-link-to-login-into-the-aws-console-optional",
    "title": "Lab 2 Amazon Web Services (AWS) Setup Instructions",
    "section": "The SSO link to login into the AWS Console (Optional)",
    "text": "The SSO link to login into the AWS Console (Optional)\n\nClick on the AWS Details link on the ribbon, this will refresh the text in the panel below, click on AWS SSO button that is now visible on this panel. SSO stands for Single Sign-On i.e. you do not need to provide your credentials everytime you want to login into this page. Clicking on the AWS SSO will download a file on your laptop, keep this file somewhere handy as you will need it in future. \nCopy the contents of the ssourl.txt file that you just downloaded, open an incognito browser window and paste them into the address bar of this incognito browser window. This will log you in into the AWS Console. You will notice that the URL in the address bar changes to the same URL as you had in the previous step.\n\n\n\n\n\n\n\nNote\n\n\n\nThe credentials in the SSO link are short-lived, meaning that they are only valid for a short duration and therefore you cannot use them for a different session.\n\n\n\nSaving AWS CLI credentials for programmatically talking to other AWS services\n\nClick on the AWS CLI button on the AWS Details. \nClick on the Show button. Copy paste the contents you see in a text editor on your laptop and save the file, we will be using these credentials from your EC2 Instance to connect to other AWS services."
  },
  {
    "objectID": "labs/02-labs.html#logging-into-aws-academy-at-a-later-time",
    "href": "labs/02-labs.html#logging-into-aws-academy-at-a-later-time",
    "title": "Lab 2 Amazon Web Services (AWS) Setup Instructions",
    "section": "Logging into AWS Academy at a later time",
    "text": "Logging into AWS Academy at a later time\n\nOpen the https://www.awsacademy.com/LMS_Login link in your browser window and click on Student Login.\n\n\n\n\n\n\n\nLogging into the AWS Console at a later time\nTo access the AWS Console in the future, login to https://www.awsacademy.com/LMS_Login, go to Learner Lab -> Modules -> Start Lab.\n\n\n\n\n\n\nNote\n\n\n\nIf you already had an AWS account prior to logging into AWS Academy you would need to login into the AWS Educate AWS account via an Incognito Browser Window."
  },
  {
    "objectID": "labs/02-labs.html#shutting-down-sagemaker-studio",
    "href": "labs/02-labs.html#shutting-down-sagemaker-studio",
    "title": "Lab 2 Amazon Web Services (AWS) Setup Instructions",
    "section": "Shutting down SageMaker Studio",
    "text": "Shutting down SageMaker Studio\nIt is important to shutdown SageMaker Studio when not in use so that you do not get billed for it when you are not using it.\n\nClick on File -> Shutdown. \nSelect Shutdown All. \n\n\n\n\n\n\n\nImportant\n\n\n\nAt the end of this lab:\n\nMake sure you shutdown SageMaker Studio.\nMake sure you shutdown EC2 VM.\nMake sure you have ended the lab in AWS Educate by pressing the End Lab button."
  },
  {
    "objectID": "labs/02-labs.html#github-classroom",
    "href": "labs/02-labs.html#github-classroom",
    "title": "Lab 2 Amazon Web Services (AWS) Setup Instructions",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link"
  },
  {
    "objectID": "labs/03-labs.html",
    "href": "labs/03-labs.html",
    "title": "Lab 3",
    "section": "",
<<<<<<< HEAD
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet."
=======
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet. #"
>>>>>>> 5cfd7cc16e257c0a05ca9589ee545fcbfa2d7d0d
  },
  {
    "objectID": "labs/03-labs.html#github-classroom",
    "href": "labs/03-labs.html#github-classroom",
    "title": "Lab 3",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link"
  },
  {
<<<<<<< HEAD
    "objectID": "labs/04-labs.html",
    "href": "labs/04-labs.html",
    "title": "EMR Cluster Setup Instructions",
    "section": "",
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet.\nAmazon Elastic MapReduce (EMR) is the industry-leading cloud big data solution for petabyte-scale data processing, interactive analytics, and machine learning using open-source frameworks such as Apache Spark, Apache Hive, and Presto. Follow these instructions step-by-step to setup an EMR cluster."
  },
  {
    "objectID": "labs/04-labs.html#github-classroom",
    "href": "labs/04-labs.html#github-classroom",
    "title": "EMR Cluster Setup Instructions",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link"
=======
    "objectID": "labs/04-labs.html#hello-world-lambda",
    "href": "labs/04-labs.html#hello-world-lambda",
    "title": "Docker",
    "section": "“Hello World” Lambda",
    "text": "“Hello World” Lambda\nLaunch AWS Academy and get to the AWS Console. Find the Lambda service from the search bar. \nThe dashboard shows the Lambda functions that have been made, some metrics on Lambda usage. Click on the orange Create Function button.\n\n\n\n\n\nHere you have to fill out the details for your Lambda function. There are several parts to set up.\n\nYou will leave the default option Author from scratch so that you can code directly from the Lambda service.\nSet your Function name as test-lambda.\nChoose your Runtime as Python 3.11\nClick on the Change default execution role dropdown, then select Use an existing role option, and finally pick the existing role LabRole.\n\n\n\n\n\n\n\nUnder Advanced Settings check the Enable Function URL checkbox and select None for Auth type. This will create an HTTPS endpoint that you can access from your web browser or cURL command without authentication (in a real world scenario the authentication piece is usually handled by an API Gateway).\n\n\n\n\n\n\n\nNow click on the orange Create function button.\n\nYou now have your environment for Lambda! In the upper function overview tab, you can select a variety of triggers and destinations for the Lambda. We will leave these alone for now. You can explore both on your own time to see the options.\n\n\n\n\n\nLet’s start with the basic test of the “Hello World” code that was provided in the Python code. Click on the blue Test button.\n\n\n\n\n\nThis will launch a popup to configure your event. You can submit a JSON payload to the test that will mimic input data that the Lambda function can process. Start off by setting Event name to mytest. Then you can leave the Event JSON for now, but you will come back to it for future iterations of experimentation. Click on the orange save button.\n\n\n\n\n\nClick on the blue Test button again. If you click on the arrow then you can choose to change or make a new test environment like you did on the previous step.\n\n\n\n\n\nYour test will execute, and the results will be shown. Several pieces of info are important:\n\nName of the test that was conducted\nResponse object that the function returned\nFunction logs that include the duration of the function, billed direction, memory max (for pricing), and actual memory used\nStatus in the upper right\n\n\n\n\n\n\nYou are now ready to invoke your newly deployed Lambda function through your web browser or through cURL. Copy the function URL and paste it in your browser’s address bar.\n\n\n\n\n\nNow try invoking the Lambda from a cURL command (this requires cURL to be available on your machine).\n\n\n\n\n\nOPTIONAL - If you wanted to set your Lambda to run on a regular schedule, like a crontab, you would add a trigger with the Add trigger button in the Function Overview and select EventBridge (CloudWatch Events). The Trigger add would look like this for setting a job to run every day at 10:15am UTC."
  },
  {
    "objectID": "labs/04-labs.html#exploring-the-lambda-file-system---lab-submission-component",
    "href": "labs/04-labs.html#exploring-the-lambda-file-system---lab-submission-component",
    "title": "Docker",
    "section": "Exploring the Lambda File System - LAB SUBMISSION COMPONENT",
    "text": "Exploring the Lambda File System - LAB SUBMISSION COMPONENT\nIn this section, you will a make use of AWS Lambda to see how serverless infrastructure works. For this assignment, you will need to submit a set of JSON files along with other required files.\nIn AWS Lambda, for an output to be generated, you will may use json.dumps to dump a dictionary to the body value in the return statement. This is one of many methods you can use! We encourage you to experiment with few different methods and choose what works the best given your case. A screenshot is attached below for reference. Also note that indent=2 argument for json.dumps, it comes in handy for producting a pretty printed output.\nA dictionary is basically defined as a key:value pair, this is also the building block of json format.\nEach time you make a change to the code, you will have to click on the Deploy button and then the blue Test button.\n\nNote the use of the dict constructor for creating the dictionary. This is an alternate, more readable (arguably slower though) way of creating a Python dictionary. Also note that indent=2 argument for json.dumps, it comes in handy for producting a pretty printed output.\n\n\n\n\n\n\n\nUse the pathlib library and its iterdir() method in Python to view the contents of the root directory (the / is referred to as the root directory). Make a new key in the Lambda called root in the return JSON and send the contents of the root directory. This might take a few tries! How do you deal with objects that need to become strings?\n\n\nHint!: Think how you can pass objects in a string? For this you can use something like this:\n\n\nf'string here {object}'\n\nReturn the contents of the event input variable to the lambda_handler function as additional item in the return JSON as event.\nReturn the python version using the executable() method in the sys library. The key should be py_version.\nReturn the current username using the subprocess library and the whoami shell command. The key should be username.\nNote that all the new keys you are adding to the response should be nested as part of the body. So essentially, the response contains of two keys: a statusCode which specifies if the call successed or failed (a statusCode value other than 200 indicates a failure) and a body key which has the contents of the response.\n\nAfter these have been executed, copy the function URL and put it in lambda-test-url.json\nThe response from the server should be like this:\n{ \"statusCode\": 200,\n  \"body\": {\n    \"message\": \"Hello from Lambda!\",\n    \"root\": \"...\"\n    \"event\": \"...\",\n    \"py_version\": \"...\",\n    \"username\": \"...\"\n  }\n}"
  },
  {
    "objectID": "labs/04-labs.html#part-2--cloud9",
    "href": "labs/04-labs.html#part-2--cloud9",
    "title": "Docker",
    "section": "Part-2- Cloud9",
    "text": "Part-2- Cloud9"
  },
  {
    "objectID": "labs/04-labs.html#creating-cloud9-environment",
    "href": "labs/04-labs.html#creating-cloud9-environment",
    "title": "Docker",
    "section": "Creating Cloud9 Environment",
    "text": "Creating Cloud9 Environment\n\nSearch for cloud9 in the search bar of your AWS console as shown in the figure below.\n\n\n\n\n\n\n\nOnce on the Cloud9 splash screen, click on the orange button Create environment.\n\n\n\n\n\n\n\nEnter a Name for your environment. Leave the description blank. The figure below shows sample text you could use. Once you enter your name, scroll down to the next section.\n\n\n\n\n\n\n\nThere are a few options here. You have to make a few changes.\n\nThe Instance type section is to select how large an instance for Cloud9. Select the t3.small instance type\nThe Platform section is for selecting the operating system for your new instance. Leave as the default\nThe Timeout option is set so your instance will hibernate after 30 minutes so you are not charged for the instance 24/7. This is a major problem for cloud services because you can run up a bill quite quickly! Leave as the default\nIn Network Settings, Connection is how to connect to your instance. Select Secure Shell (SSH)\nFinally, click the orange button Create\n\n\n\n\n\n\n\n\nYou will be sent to the environments page of the Cloud9 service. Your environment is now building. In the table below, for your environment (the row), click on the Open button in Cloud9 IDE column. In the screenshot, we named it cloud9-env.\n\n\n\n\n\n\n\nThe environment will be configured for you. This takes a few minutes.\n\n\n\n\n\n\nOnce the environment setup screen goes away then you are ready to use Cloud9. If you get a warning message, just click “OK”."
  },
  {
    "objectID": "labs/04-labs.html#part-3--docker-lambda",
    "href": "labs/04-labs.html#part-3--docker-lambda",
    "title": "Docker",
    "section": "Part-3- Docker-Lambda",
    "text": "Part-3- Docker-Lambda"
  },
  {
    "objectID": "labs/04-labs.html#setting-up-basic-docker-images-in-cloud9",
    "href": "labs/04-labs.html#setting-up-basic-docker-images-in-cloud9",
    "title": "Docker",
    "section": "Setting up Basic Docker Images in Cloud9",
    "text": "Setting up Basic Docker Images in Cloud9\n\n\n\n\n\n\nImportant:\n\n\n\nThis lab will take a lot of time! You will need atleast 5-6 hours on average to work on this lab. Please make sure you start this assignment as soon as possible\n\n\nDocker image building in Cloud9 is easy since the docker package is already set up. You just have to write some code and run Linux commands!\n\nIn Cloud9, start off by cloning your git repository from either the source control button on the lefthand sidebar or through the terminal.\nIn the root of your repository, create three empty text files in that folder called Dockerfile, app.py, and requirements.txt.\nThe results should look like below and have the symbols change automatically:\n\n\n\nOpen up the Dockerfile and add the following text (note the # lines are comments just like python!)\n\n# syntax=docker/dockerfile:1\n\n# adapted from https://www.philschmid.de/aws-lambda-with-custom-docker-image\n# https://docs.aws.amazon.com/lambda/latest/dg/python-image.html\nFROM python:3.11-slim-buster\n\nCMD [\"python\", \"-c\", \"import platform; print(f\\\"version: {platform.python_version()}\\\")\"]\n\nGo to the terminal and change directories to the location of your Dockerfile. Run the command docker build ./ -t test\n\n\n\nRun the command docker run test to see if your Dockerfile worked!"
  },
  {
    "objectID": "labs/04-labs.html#lambda-docker-imagelab-demonstration",
    "href": "labs/04-labs.html#lambda-docker-imagelab-demonstration",
    "title": "Docker",
    "section": "Lambda Docker Image(Lab-Demonstration)",
    "text": "Lambda Docker Image(Lab-Demonstration)\nNote that this Dockerfile is invoking your requirements.txt file to install any packages from pip and the app.py lambda_handler function to run the python code.\n\nNow you might think how does this requirements.txt file work? Each library which needs to be installed will be listed here. Think of Docker as a virtual environment where you can install any package you need and then you would list them in the requirements.txt file. A small example of this is given in the following screenshot below. When the requirements.txt file has been changed, you will have to build and redeploy the docker image.\n\n\n\n\nrequirements.txt\n\n\n\nUse the new Dockerfile contents below for your Dockerfile.\n\nA few examples of how to build a docker file along with some documentation is given below in this link: https://spacelift.io/blog/dockerfile. You can scroll down to see how a docker commands work and what they do. This will be useful in making this a relatively simple task.\n# syntax=docker/dockerfile:1\n\n# adapted from https://www.philschmid.de/aws-lambda-with-custom-docker-image\n# https://docs.aws.amazon.com/lambda/latest/dg/python-image.html\nFROM public.ecr.aws/lambda/python:3.11\n\n##### copy requirements file and install necessary packages\n\n# ***CODE TO DO***\n# ADD the requirements.txt into the ${LAMBDA_TASK_ROOT} directory in the container\n\nRUN pip3 install -r ${LAMBDA_TASK_ROOT}/requirements.txt --target \"${LAMBDA_TASK_ROOT}\"\n\n##### Copy function code to docker container\n\n# ***CODE TO DO***\n# ADD the app.py file into the ${LAMBDA_TASK_ROOT} directory in the container\n\n##### SET THE COMMAND OF THE CONTAINER FOR THE LAMBDA HANDLER\n# app (name of py file)\n# handler (name of function to execute for lambda job)\nCMD [ \"app.lambda_handler\" ]\n\nNote that the ADD and COPY commands in Docker for this instance are similar. The ADD function is more advanced and can auto-extract compressed files into the image. Please use the given python version for this assignment. This assignment was mainly designed to be used with python 3.11. If you a version of python which is lower, we cannot say if it would be compatible.\nSet up your python file app.py with a function called lambda_handler that accepts the event and context arguments. Wait, we have already done this in basic Lambda! Copy your function from the Lambda service. This will ensure that the response is the same through basic Lambda and through the Docker Lambda.\nSince you made changes to the Dockerfile and your app.py files, you need to build a new Docker image. Run the command docker build ./ -t lambda-test so that you name the image something new.\nThis has to be done every time you make changes to the app.py file or the Dockerfile.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe syntax of docker build is as follows:\ndocker build PATH -t 'CONTAINER NAME'\n\n#Container name can be changed in this instance, but lambda-test is preferred.\n#  -t is a flag which tags the container with a name \n#  In the above command, ./ is the path where the container would be built. \n#  Can you recall where does ./ lead to?\n\n\n\n\n\n\n\n\n\nHint\n\n\n\nTry running the command docker images to see the images you have in your local environment.\n\n\n\n\n“Running” the python script requires two steps because the Lambda container is built as a listening service that will execute when there is a payload provided to it.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe syntax of docker run is as follows:\n\ndocker run -p PORT CONTAINER NAME\n\n#-p flag specifies which port needs to be used for the container to start running.\n# CONTAINER NAME can be anything, we defined it to be lambda-test in this scenario.\n\n\n\nRun the command docker run -p 8080:8080 lambda-test to set up the service on your first terminal tab. This will run the service and listen for triggers. Next, click on the green plus icon and choose New Terminal to launch a new bash terminal.\n\n\n\nIn this second terminal, run the command curl -XPOST \"http://localhost:8080/2015-03-31/functions/function/invocations\" -d '{\"payload\":\"hello world!\"}'. This should return the same response as what you saw in the Lambda service. Also, go back to the first terminal tab to see the summary of execution message."
  },
  {
    "objectID": "labs/04-labs.html#python-setup",
    "href": "labs/04-labs.html#python-setup",
    "title": "Docker",
    "section": "Python Setup",
    "text": "Python Setup\nReturn the price of any stock symbol that is submitted through the payload value for Lambda. For example, the goal is to get the DOW stock price if I run the command: http://localhost:8080/2015-03-31/functions/function/invocations\" -d '{\"payload\":\"DOW\"}'\n\nThe url has to be dynamic based on the input stock symbol: https://finance.yahoo.com/quote/DOW\nUse the requests and beautifulsoup packages to build the function. Note you will need to add these libraries to the requirements.txt file.\nStart your app.py file with this start code.\n\nimport os\nimport json\nimport requests\nimport traceback\nfrom bs4 import BeautifulSoup\n\nurl = f\"https://finance.yahoo.com/quote/DOW\"\n\n# need headers to get pull from yahoo finance\nheader = {'Connection': 'keep-alive',\n          'Expires': '-1',\n          'Upgrade-Insecure-Requests': '1',\n          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) \\\n           AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36'\n          }\n\nresponse = requests.get(url, headers=header)\nsoup = BeautifulSoup(response.text, \"html.parser\")\nprice = soup.find(\"fin-streamer\", {'data-field':\"regularMarketPrice\", 'data-symbol' : stock.upper()}).text\n\nprint(f\"price={price}\")"
  },
  {
    "objectID": "labs/04-labs.html#coding-requirements",
    "href": "labs/04-labs.html#coding-requirements",
    "title": "Docker",
    "section": "Coding Requirements:",
    "text": "Coding Requirements:\n\nAdd a try-except framework if any part of your code errors. Use the command error=traceback.format_exc() to capture the error. What do you return when the function errors instead?\nEnsure that if your function does not receive an input or if it receives an invalid stock ticker that it returns a 404 status code. For no input use the message \"No stock provided\" and for an invalid ticker choose \"Invalid stock provided\"\nMake the url dynamic to the input stock symbol specified\nIntegrate your code into the Lambda framework - event input and response output\nEnsure the response object for a successful request looks like {\"statusCode\" : 200, \"body\" : {\"stock\" : \"A\", \"price\" : \"#####\"}}.\n\n\n\n\n\n\n\nImportant:\n\n\n\nPlease use indent = 2 when dumping the response to make sure the response is easily readable\n\n\n\n\n\n\n\n\nHint\n\n\n\nHint #1: Try developing using the python console in Cloud9 before integrating into your app.py file. You don’t want to have to build a Docker image every code change, right?\n\nThis process is mainly only for prototyping. You would do this by writing the entire python code first in app.py file, then running the following command in the terminal :\n\n\npython app.py --payload {'payload': 'stock'}\nHint #2: Once you put the code into the Lambda framework, you will have to build and run to complete a development integration.\nHint #3: Implement a basic logger function to see where you might be going wrong? is ther a certain way to pass an input string to the event handler function? Try printing the event out to the console.\n\n\n\nUse the following test inputs to confirm your function can handle all the errors gracefully: APPL, AAPL, appl, DOW, dow. Unknown tickers need to be handled and case of the ticker should not matter.\nSubmit the bash history where this function is implemented in the file lambda-local-test.txt.\nOnce your code is ready to go with Lambda, add, commit, and push the files (app.py, Dockerfile, requirements.txt) to GitHub."
  },
  {
    "objectID": "labs/04-labs.html#posting-docker-image-to-ecr",
    "href": "labs/04-labs.html#posting-docker-image-to-ecr",
    "title": "Docker",
    "section": "Posting Docker Image to ECR",
    "text": "Posting Docker Image to ECR\nECR stands for Elastic Container Registry.\n\nRun the command aws ecr create-repository --repository-name docker-lambda to make a new repo in the elastic container registry to store your new containers.\nRun the commands to grab info on your AWS account and region.\naws_region=$(aws configure get region)\naws_account_id=$(aws sts get-caller-identity --query 'Account' --output text)\nRun the following command to configure your authentication to talk to the ECR service. Note how we use BASH variable with the $ so that you don’t have to manually enter your region or account id.\naws ecr get-login-password \\\n--region $aws_region \\\n| docker login \\\n--username AWS \\\n--password-stdin $aws_account_id.dkr.ecr.$aws_region.amazonaws.com\nTag the image in the ECR registry by running the command docker tag lambda-docker-build $aws_account_id.dkr.ecr.$aws_region.amazonaws.com/docker-lambda\n\nThe final docker-lambda is referring to the new repository you just built a few commands ago.\n\nPush the image to docker by running the command docker push $aws_account_id.dkr.ecr.$aws_region.amazonaws.com/docker-lambda\n\n\nRead more about pushing a Docker image to ECR here."
  },
  {
    "objectID": "labs/04-labs.html#docker-setup-in-lambda",
    "href": "labs/04-labs.html#docker-setup-in-lambda",
    "title": "Docker",
    "section": "Docker Setup in Lambda",
    "text": "Docker Setup in Lambda\nGo back to the Lambda dashboard by going to this link. Make a new function by clicking on the orange Create function button.\n\nYou must select the Container image option that is the third item on the top row of options for Lambda.\nName your function container-test\nSet your Execution role like we did earlier so that you use LabRole\nClick on the Browse images button to find the container you just uploaded!\n\n\n\nA popup will launch and you have to select the repository (“docker lambda”) and then your image, which will be called “latest” by default. Click on the orange Select image button.\n\n\nNow you see the same overview page for the Lambda. Since this is a container image and not simple code, we cannot actually preview anything. Just click on the Test tab.\n\nSet a name for your test aapl-test and change the event JSON to look like {\"payload\" : \"AAPL\"}. Once you are satisfied, click on the Save button and then the orange Test button.\n\nThe result of your test will be shown in a green box, and just click on the Details arrow to see the summary. Note that the stock price came back successfully. The billed duration in the example is 2578 ms, with “Init duration” contributing 709.68 ms and the code execution contributing 1867.85 ms. The results are rounded to the nearest millisecond, but are calculated at the 10 microsecond level, WOW!"
>>>>>>> 5cfd7cc16e257c0a05ca9589ee545fcbfa2d7d0d
  },
  {
    "objectID": "labs/05-labs.html",
    "href": "labs/05-labs.html",
    "title": "Lab 5",
    "section": "",
<<<<<<< HEAD
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet."
=======
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet. #"
>>>>>>> 5cfd7cc16e257c0a05ca9589ee545fcbfa2d7d0d
  },
  {
    "objectID": "labs/05-labs.html#github-classroom",
    "href": "labs/05-labs.html#github-classroom",
    "title": "Lab 5",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link"
  },
  {
    "objectID": "labs/06-labs.html",
    "href": "labs/06-labs.html",
    "title": "Lab 6",
    "section": "",
<<<<<<< HEAD
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet."
=======
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet. #"
>>>>>>> 5cfd7cc16e257c0a05ca9589ee545fcbfa2d7d0d
  },
  {
    "objectID": "labs/06-labs.html#github-classroom",
    "href": "labs/06-labs.html#github-classroom",
    "title": "Lab 6",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link"
  },
  {
    "objectID": "labs/07-labs.html",
    "href": "labs/07-labs.html",
    "title": "Lab 7 - SparkSQL",
    "section": "",
<<<<<<< HEAD
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet."
=======
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet. #"
>>>>>>> 5cfd7cc16e257c0a05ca9589ee545fcbfa2d7d0d
  },
  {
    "objectID": "labs/07-labs.html#the-right-data-tool",
    "href": "labs/07-labs.html#the-right-data-tool",
    "title": "Lab 7 - SparkSQL",
    "section": "The right data tool…",
    "text": "The right data tool…\n\nSometimes you only need simple solutions to problems. Perfection is the enemy of the good. We’ll walk through a few options when considering a big data job.\nHere are a few options when your data is local:\n\nUsing a single process in R/python\nWriting an R/python function to run in parallel on a single node\nUsing Python cluster tools like Ray or Dask\nUsing PySpark on a single node to development with multiple workers\n\nHere are a few options when your data is distributed\n\nUsing Dask on a cluster\nUsing PySpark on a cluster\n\nHere are a few options for task-specific needs:\n\nHarnessing a specialized solution like GPU data science with RAPIDS. This software is developed so you can conduct your entire data science pipeline on a GPU.\nTensorFlow distributed netural network training with Spark!\nLarge language model processing with Spark!"
  },
  {
    "objectID": "labs/07-labs.html#rdd-vs-pysparksql-dataframe",
    "href": "labs/07-labs.html#rdd-vs-pysparksql-dataframe",
    "title": "Lab 7 - SparkSQL",
    "section": "RDD vs PySparkSQL DataFrame",
    "text": "RDD vs PySparkSQL DataFrame\nRDD (resilient distributed dataset) is an immutable collection of records (rows) of data\n\ngreat for unstructured data\ndoing low-level transformations\n\nPySpark DataFrame is organized into a table with rows and columns. This is just like the format when working in a relational database like Hive.\n\nTwo dimensional table format, great for “standard” datasets\nColumn format means consistent metadata and data typing\nQuery optimization\nSchema is created automatically!"
  },
  {
    "objectID": "labs/07-labs.html#common-actions-in-r-python-pandas-and-pyspark",
    "href": "labs/07-labs.html#common-actions-in-r-python-pandas-and-pyspark",
    "title": "Lab 7 - SparkSQL",
    "section": "Common Actions in R, Python Pandas, and PySpark",
    "text": "Common Actions in R, Python Pandas, and PySpark\nAnything you can do, I can do in parallel!\nTDS big data options\n\nhead(starwars)\n\n# A tibble: 6 × 14\n  name         height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n  <chr>         <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n1 Luke Skywal…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n2 C-3PO           167    75 <NA>    gold    yellow    112   none  mascu… Tatooi…\n3 R2-D2            96    32 <NA>    white,… red        33   none  mascu… Naboo  \n4 Darth Vader     202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n5 Leia Organa     150    49 brown   light   brown      19   fema… femin… Aldera…\n6 Owen Lars       178   120 brown,… light   blue       52   male  mascu… Tatooi…\n# … with 4 more variables: species <chr>, films <list>, vehicles <list>,\n#   starships <list>, and abbreviated variable names ¹​hair_color, ²​skin_color,\n#   ³​eye_color, ⁴​birth_year, ⁵​homeworld\n# ℹ Use `colnames()` to see all variable names\n\n\n\nReading in data\n\nR\n\nlibrary(arrow)\nlibrary(dplyr)\n\nstarwars <- arrow::read_parquet('starwars.parquet')\n\n\n\nPandas\n\nimport os\nimport pandas as pd\nstarwars = pd.read_parquet('starwars.parquet')\n\n\n\nPySpark\n\nimport pyspark\nfrom pyspark.sql.functions import udf, lit, col\nfrom pyspark.sql import SparkSession\n\n# configuration for workers\nconfig = pyspark.SparkConf().setAll([('spark.executor.memory', '10g'),\n                             ('spark.executor.cores', '2'),\n                             ('spark.cores.max', '16'),])\n# launch cluster connection\nsc = pyspark.SparkContext(conf = config)\n\n# set up pyspark session\nspark = pyspark.SparkSession.builder.appName('my-test').getOrCreate()\n\nstarwars = spark.read.load('starwars.parquet')\n\n\n\n\nSelecting data variables\n\nR\n\nstarwars_select <- starwars %>% select(name, height, mass)\n\n\n\nPandas\n\nstarwars_select = starwars[['name','height','mass']]\n\n\n\nPySparkSQL\n\nstarwars_select = starwars.select(['name','height','mass'])\n\n\n\n\nFiltering data rows\n\nR\n\nstarwars_filter <- starwars %>% filter(height > 110, \n                                       homeworld == \"Tatooine\")\n\n\n\nPandas\n\nstarwars_filter = starwars[(starwars.height > 110) & \n                        (starwars.homeworld == \"Tatooine\")]\n\n\n\nPySpark\n\nstarwars_filter = starwars[(col('height') > 110) &\n                    (col('homeworld') == \"Tatooine\")]\n\n\n\n\nManipulating data\n\nR\n\nstarwars <- starwars %>% \n    mutate(tatooine_dummy = if_else(homeworld == 'Tatooine',\n                                    TRUE,\n                                    FALSE))\n\n\n\nPandas\n\nstarwars['tatooine_dummy'] = starwars.apply(\n                                  lambda x: True if x.homeworld == 'Tatooine' \n                                        else False, \n                                                axis = 1)\n\n\n\nPySpark\n\nfrom pyspark.sql.types import BooleanType\n@udf(returnType=BooleanType())\ndef dummy_tatooine(x):\n    if x == 'Tatooine':\n        return True\n    else:\n        return False\n\nstarwars = starwars.withColumn('tatooine_dummy', \n                dummy_tatooine(col('homeworld')))\n\n\n\n\nView the head of the data\n\nR\n\nstarwars %>% head(5)\n\n\n\nPandas\n\nstarwars.head(5)\n\n\n\nPySpark\n\nstarwars.take(5) # RDD version\n\nstarwars.show(5) # SQL version\n\n\n\n\nGroup-by mean data\n\nR\n\nstarwars %>% group_by(species) %>% summarize(mean_height = mean(height))\n\n\n\nPandas\n\nstarwars.groupby('species')['height'].mean()\n\n\n\nPySpark\n\nstarwars.groupBy('species').mean('height').collect()\n\n\n\n\nTallest character from each species\n\nR\n\nstarwars %>% group_by(species) %>% filter(height = max(height))\n\n\n\nPandas\n\nstarwars.iloc[starwars.groupby('species').height.idxmax().tolist(),:]\n\nstarwars.sort_values('height',ascending=False).groupby('species').first()\n\n\n\nPySpark\n\ntemp_df = starwars.groupBy('species').agg(f.max('height').alias('height'))\nstarwars.groupBy.join(temp_df,on='height',how='leftsemi').show()\n\nfrom pyspark.sql import Window\nw = Window.partitionBy('species')\nstarwars.withColumn('maxheight', f.max('height').over(w))\\\n    .where(f.col('height') == f.col('maxheight'))\\\n    .drop('maxheight')\\\n    .show()"
  },
  {
    "objectID": "labs/07-labs.html#collecting-data",
    "href": "labs/07-labs.html#collecting-data",
    "title": "Lab 7 - SparkSQL",
    "section": "Collecting Data",
    "text": "Collecting Data\nBe extra careful when using the .collect() function. If you have massive amounts of data, then your spark driver is going to have trouble.\n\nIn general, always run a .count() function to check the number of rows before running .collect(). Alternatively, you can run the command .show(5) or .take(5) to only see the first few rows of data. You never want to bring 10s of millions of rows to your local session. Let the big data live in big data land."
  },
  {
    "objectID": "labs/07-labs.html#github-classroom",
    "href": "labs/07-labs.html#github-classroom",
    "title": "Lab 7 - SparkSQL",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link"
  },
  {
    "objectID": "labs/08-labs.html",
    "href": "labs/08-labs.html",
    "title": "Lab 8",
    "section": "",
<<<<<<< HEAD
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet."
=======
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet. #"
>>>>>>> 5cfd7cc16e257c0a05ca9589ee545fcbfa2d7d0d
  },
  {
    "objectID": "labs/08-labs.html#github-classroom",
    "href": "labs/08-labs.html#github-classroom",
    "title": "Lab 8",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link"
  },
  {
    "objectID": "labs/09-labs.html",
    "href": "labs/09-labs.html",
    "title": "Lab 9",
    "section": "",
<<<<<<< HEAD
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet."
=======
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet. #"
>>>>>>> 5cfd7cc16e257c0a05ca9589ee545fcbfa2d7d0d
  },
  {
    "objectID": "labs/09-labs.html#github-classroom",
    "href": "labs/09-labs.html#github-classroom",
    "title": "Lab 9",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link"
  },
  {
    "objectID": "labs/10-labs.html",
    "href": "labs/10-labs.html",
    "title": "Lab 10",
    "section": "",
<<<<<<< HEAD
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet."
=======
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet. #"
>>>>>>> 5cfd7cc16e257c0a05ca9589ee545fcbfa2d7d0d
  },
  {
    "objectID": "labs/10-labs.html#github-classroom",
    "href": "labs/10-labs.html#github-classroom",
    "title": "Lab 10",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link"
  },
  {
    "objectID": "labs/11-labs.html",
    "href": "labs/11-labs.html",
    "title": "Lambda & Docker",
    "section": "",
<<<<<<< HEAD
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet."
=======
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet. #"
>>>>>>> 5cfd7cc16e257c0a05ca9589ee545fcbfa2d7d0d
  },
  {
    "objectID": "labs/11-labs.html#hello-world-lambda",
    "href": "labs/11-labs.html#hello-world-lambda",
    "title": "Lambda & Docker",
    "section": "“Hello World” Lambda",
    "text": "“Hello World” Lambda\nLaunch AWS Academy and get to the AWS Console. Find the Lambda service from the search bar. \nThe dashboard shows the Lambda functions that have been made, some metrics on Lambda usage. Click on the orange Create Function button.\n\nHere you have to fill out the details for your Lambda function. There are several parts to set up.\n\nYou will leave the default option Author from scratch so that you can code directly from the Lambda service.\nSet your Function name as my-first-lambda.\nChoose your Runtime as Python 3.9\nClick on the Change default execution role dropdown, then select Use an existing role option, and finally pick the existing role LabRole. Once you have done these four things, click on the orange Create function button.\n\n\nYou now have your environment for Lambda! In the upper function overview tab, you can select a variety of triggers and destinations for the Lambda. We will leave these alone for now. You can explore both on your own time to see the options.\nLet’s start with the basic test of the “Hello World” code that was provided in the Python code. Click on the orange Test button.\n\nThis will launch a popup to configure your event. You can submit a JSON payload to the test that will mimic input data that the Lambda function can process. Start off by setting Event name to mytest. Then you can leave the Event JSON for now, but you will come back to it for future iterations of experimentation. Click on the orange save button.\n\nClick on the orange test button again. If you click on the arrow then you can choose to change or make a new test environment like you did on the previous step.\n\nYour test will execute, and the results will be shown. Several pieces of info are important:\n\nName of the test that was conducted\nResponse object that the function returned\nFunction logs that include the duration of the function, billed direction, memory max (for pricing), and actual memory used\nStatus in the upper right\n\n\nOPTIONAL - If you wanted to set your Lambda to run on a regular schedule, like a crontab, you will need to add a trigger using EventBridge (CloudWatch Events). The Trigger add would look like this for setting a job to run every day at 10:15am UTC."
  },
  {
    "objectID": "labs/11-labs.html#exploring-the-lambda-file-system---todo",
    "href": "labs/11-labs.html#exploring-the-lambda-file-system---todo",
    "title": "Lambda & Docker",
    "section": "Exploring the Lambda File System - TODO",
    "text": "Exploring the Lambda File System - TODO\nIn this section, you will store three screenshots in your Word doc to show the Lambda responses from a variety of code changes to your Lambda handler function.\nEach time you make a change to the code, you will have to click on the Deploy button and then the orange Test button.\n\n\n1. Use the os or subprocess library in python to view the contents of the root directory. Make a new key in the return dictionary and send as the value the contents of the root directory.\n\n\n2. Return the contents of the event input variable to the lambda_handler function as additional item in the return dictionary\n\n\n3. Return the contents of the context input variable to the lambda_handler function as additional item in the return dictionary. This might take a few tries! How do you deal with objects that need to become strings?"
  },
  {
    "objectID": "labs/11-labs.html#creating-cloud9-environment",
    "href": "labs/11-labs.html#creating-cloud9-environment",
    "title": "Lambda & Docker",
    "section": "Creating Cloud9 Environment",
    "text": "Creating Cloud9 Environment\n\nSearch for cloud9 in the search bar of your AWS console as shown in the figure below.\n\n\n\nOnce on the Cloud9 splash screen, click on the orange button Create environment.\n\n\n\nEnter a Name and description for your environment. The figure below shows sample text you could use. Once you enter your name and description click the orange button Next step.\n\n\n\nThere are a few options here. You can leave all of the defaults. Click the orange Next step button.\n\nThe Environment type section lets you pick if you want to spin up a new EC2 machine or connect to existing resources.\nThe Instance type section is to select how large an instance for Cloud9. The small t2.micro instance is fine for Cloud9.\nThe Platform section is for selecting the operating system for your new instance.\nThe Cost-saving setting option is set so your instance will hibernate after 30 minutes so you are not charged for the instance 24/7. This is a major problem for cloud services because you can run up a bill quite quickly!\nThe IAM role is for managing permissions to AWS resources like S3. Cloud9 setup will make a new role automatically.\n\n\n\n\nThis screen shows the summary of the selections made for naming and configuring the environment. Click the orange Create environment button.\n\n\n\nThe environment will be configured for you. This takes a few minutes.\n\n\nOnce the environment setup screen goes away then you are ready to use Cloud9. If you get a warning message, just click “OK”."
  },
  {
    "objectID": "labs/11-labs.html#setting-up-basic-docker-images-in-cloud9",
    "href": "labs/11-labs.html#setting-up-basic-docker-images-in-cloud9",
    "title": "Lambda & Docker",
    "section": "Setting up Basic Docker Images in Cloud9",
    "text": "Setting up Basic Docker Images in Cloud9\nDocker image building in Cloud9 is easy since the docker package is already set up. You just have to write some code and run Linux commands!\n\nStart off by making a new folder on the lefthand folder sidebar. Call it something simple like docker-lambda-env.\nOnce you have the folder created, create three files in that folder called Dockerfile, app.py, and requirements.txt.\nThe results should look like the below and have the symbols change automatically:\n\n\n\nOpen up the Dockerfile and add the following text (note the # lines are comments just like python!)\n\n# syntax=docker/dockerfile:1\n\n# adapted from https://www.philschmid.de/aws-lambda-with-custom-docker-image\n# https://docs.aws.amazon.com/lambda/latest/dg/python-image.html\nFROM python:3.9-slim-buster\n\nCMD [\"python\", \"-c\", \"import platform; print(f\\\"version: {platform.python_version()}\\\")\"]\n\nGo to the terminal and change directories to the location of your Dockerfile. Run the command docker build ./ -t test\n\n\n\nRun the command docker run test to see if your Dockerfile worked!"
  },
  {
    "objectID": "labs/11-labs.html#lambda-docker-image",
    "href": "labs/11-labs.html#lambda-docker-image",
    "title": "Lambda & Docker",
    "section": "Lambda Docker Image",
    "text": "Lambda Docker Image\n\nUse the new Dockerfile contents below for your Dockerfile. Note that this Dockerfile is invoking your requirements.txt file to install any packages from pip and the app.py lambda_handler function to run the python code.\n\n# syntax=docker/dockerfile:1\n\n# adapted from https://www.philschmid.de/aws-lambda-with-custom-docker-image\n# https://docs.aws.amazon.com/lambda/latest/dg/python-image.html\nFROM public.ecr.aws/lambda/python:3.9\n\n# copy requirements file and install necessary packages\nADD requirements.txt ${LAMBDA_TASK_ROOT}\nRUN pip3 install -r ${LAMBDA_TASK_ROOT}/requirements.txt --target \"${LAMBDA_TASK_ROOT}\"\n\n# Copy function code to docker container\nCOPY app.py ${LAMBDA_TASK_ROOT}\n\n# app (name of py file)\n# handler (name of function to execute for lambda job)\nCMD [ \"app.lambda_handler\" ]\n\nNote that the ADD and COPY commands in Docker for this instance are similar. The ADD function is more advanced and can auto-extract compressed files into the image.\nSet up your python file app.py with a function called lambda_handler that accepts the event and context arguments. Wait, we have already done this in basic Lambda! Copy your function from the Lambda service. This will ensure that the response is the same through basic Lambda and through the Docker Lambda.\n\n\n\nSince you made changes to the Dockerfile and your app.py files, you need to build a new Docker image. Run the command docker build ./ -t lambda-test.\n\n\n\n“Running” the python script requires two steps because the Lambda container is built as a listening service that will execute when there is a payload provided to it.\n\n\nRun the command docker run -p 8080:8080 lambda-test to set up the service on your first terminal tab. This will run the service and listen for triggers. Next, click on the green plus icon and choose New Terminal to launch a new bash terminal.\n\n\n\nIn this second terminal, run the command curl -XPOST \"http://localhost:8080/2015-03-31/functions/function/invocations\" -d '{\"payload\":\"hello world!\"}'. This should return the same response as what you saw in the Lambda service. Also, go back to the first terminal tab to see the summary of execution message."
  },
  {
    "objectID": "labs/11-labs.html#python-setup",
    "href": "labs/11-labs.html#python-setup",
    "title": "Lambda & Docker",
    "section": "Python Setup",
    "text": "Python Setup\nReturn the price of any stock symbol that is submitted through the payload value for Lambda. For example, I would get the DOW stock price if I ran the command: http://localhost:8080/2015-03-31/functions/function/invocations\" -d '{\"payload\":\"DOW\"}'\n\nThe url has to be dynamic based on the input stock symbol: https://finance.yahoo.com/quote/DOW\nUse the requests and beautifulsoup packages to build the function. Note you will need to add these libraries to the requirements.txt file.\nThe starter code looks like this:\n\n# import libraries\nimport requests\nfrom bs4 import BeautifulSoup\n\n# set url\nurl = f\"https://finance.yahoo.com/quote/DOW\"\n\n# get the url page results\nresponse = requests.get(url)\n\n# try to parse Beautiful Soup\ntry:\n    soup = BeautifulSoup(response.text, \"html.parser\")\nexcept Exception as e: # handle error gracefully\n    return {\n        'statusCode': 200,\n        'body': json.dumps(f'Here is the error message: {e}'),\n        } # send the error message back to the user\n\n# find the price\nprice = soup.find(\"fin-streamer\", {'data-test':\"qsp-price\"}).text\n\nprint(price)\n\nCoding Goals:\n\nAdd try-except framework if the find does not work. What do you return instead?\nMake the url dynamic to the input stock symbol specified\nIntegrate your code into the Lambda framework - event input and response output\n\nHint #1: Try developing using the python console in Cloud9 before integrating into your app.py file.\nHint #2: Once you put the code into the Lambda framework, you will have to build and run to run a development integration.\n\n\nUse the following test inputs to confirm your function can handle all the errors gracefully: APPL, AAPL, appl, DOW, dow.\n\n\nTake a screenshot of your terminal with all 5 test cases and their result and place into your Word doc\n\n\nOnce your code is ready to go with Lambda, add, commit, and push the files (app.py, Dockerfile, requirements.txt) to GitHub. Easiest way to do this is by downloading and uploading through the GitHub website."
  },
  {
    "objectID": "labs/11-labs.html#posting-docker-image-to-ecr",
    "href": "labs/11-labs.html#posting-docker-image-to-ecr",
    "title": "Lambda & Docker",
    "section": "Posting Docker Image to ECR",
    "text": "Posting Docker Image to ECR\nECR stands for Elastic Container Registry.\n\nRun the command aws ecr create-repository --repository-name docker-lambda to make a new repo in the elastic container registry to store your new containers.\nRun the command $(aws ecr get-login --no-include-email --region us-east-1) to grab the login information for your AWS account and store on the Cloud9 EC2 instance. This is bad practice for important accounts, but this account is just for experimenting!\nRun the command cat /home/ec2-user/.docker/config.json to see the contents of the authentication file. Copy the address that looks similar to 565177075063.dkr.ecr.us-east-1.amazonaws.com\nTag the image with the ECR registry by running the command docker tag lambda-test [[THE URL YOU FOUND IN THE LAST STEP]]/docker-lambda\n\nThe example looks like docker tag lambda-test 565177075063.dkr.ecr.us-east-1.amazonaws.com/docker-lambda\nNote that you have to use your own account id, not the one in the example text!\nThe final docker-lambda is referring to the new repository you just built a few commands ago.\n\nPush the image to docker by running the command docker push 565177075063.dkr.ecr.us-east-1.amazonaws.com/docker-lambda\n\n\nRead more about pushing a Docker image to ECR here."
  },
  {
    "objectID": "labs/11-labs.html#docker-setup-in-lambda",
    "href": "labs/11-labs.html#docker-setup-in-lambda",
    "title": "Lambda & Docker",
    "section": "Docker Setup in Lambda",
    "text": "Docker Setup in Lambda\nGo back to the Lambda dashboard by going to this link: https://us-east-1.console.aws.amazon.com/lambda/home?region=us-east-1#/discover. Make a new function by clicking on the orange Create function button.\n\nYou must select the Container image option that is the third item on the top row of options for Lambda.\nName your function container-test\nSet your Execution role like we did earlier so that you use LabRole\nClick on the Browse images button to find the container you just uploaded!\n\n\n\nA popup will launch and you have to select the repository (“docker lambda”) and then your image, which will be called “latest” by default. Click on the orange Select image button.\n\n\nNow you see the same overview page for the Lambda. Since this is a container image and not simple code, we cannot actually preview anything. Just click on the Test tab.\n\nSet a name for your test aapl-test and change the event JSON to look like {\"payload\" : \"AAPL\"}. Once you are satisfied, click on the Save button and then the orange Test button.\n\nThe result of your test will be shown in a green box, and just click on the Details arrow to see the summary. Note that the stock price came back successfully. The billed duration in the example is 2578 ms, with “Init duration” contributing 709.68 ms and the code execution contributing 1867.85 ms. The results are rounded to the nearest millisecond, but are calculated at the 10 microsecond level, WOW!\n\n\nTake a screenshot of the success output from the test you made in Lambda into your Word doc\n\n\nEXTRA CREDIT - 1 points. Modify your app.py to accept multiple stocks comma separated like {“payload” : “AAPL,DOW,MSFT”}"
  },
  {
    "objectID": "labs/11-labs.html#github-classroom",
    "href": "labs/11-labs.html#github-classroom",
    "title": "Lambda & Docker",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link"
  },
  {
    "objectID": "labs/13-labs.html",
    "href": "labs/13-labs.html",
    "title": "Lab 13",
    "section": "",
<<<<<<< HEAD
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet."
=======
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet. #"
>>>>>>> 5cfd7cc16e257c0a05ca9589ee545fcbfa2d7d0d
  },
  {
    "objectID": "labs/13-labs.html#github-classroom",
    "href": "labs/13-labs.html#github-classroom",
    "title": "Lab 13",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nGitHub Classroom Link - Part 1"
  },
  {
    "objectID": "labs/index.html",
    "href": "labs/index.html",
    "title": "Labs",
    "section": "",
    "text": "Lab\nThe lab for the class would involve a hands-on coding assignment provided through GitHub Classroom. You will start the lab in-class, myself and the TAs would be helping you with any questions with the lab and then you would need to turn in the lab by checking in your code and results in the GitHub repo (you will have until next class for this, but usually you would be able to do this much sooner)."
  },
  {
    "objectID": "labs/login-to-saxanet.html",
    "href": "labs/login-to-saxanet.html",
    "title": "PPOL 5206",
    "section": "",
<<<<<<< HEAD
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet."
=======
    "text": "Important\n\n\n\nMake sure that you are connected to the Saxanet WiFi network and not the GuestNet network. SSH (TCP port 22) is blocked on GuesNet which means if you are on GuesNet you wil not be able to connect to your cloud VMs or clone repos from GitHub via SSH. Use SaxaNet. #"
>>>>>>> 5cfd7cc16e257c0a05ca9589ee545fcbfa2d7d0d
  },
  {
    "objectID": "resources/index.html",
    "href": "resources/index.html",
    "title": "Resources",
    "section": "",
    "text": "Here are a bunch of resources that will help you in this class. This is a tough and challenging class, but if you put in the hard yards you will learn a lot. I promise you, that you will find me willing to work with you every step of the way.\n\nDue dates are closer than they appear.\n\n\n\n\n\n\n\nPay Attention\n\n\n\nDue dates are closer than they appear. Working with Big Data is tricky, start homeworks early to account for the unknown unknowns that will inevitably show up.\n\n\n\n\nFollow instructions for in the homeworks and lab assignments. The homeworks and labs require the output generated by your code to be in a specific format (JSON usually), in a specifically named file with specifically named fields. Your homeworks are graded by an autograder program (so I wrote code to grade the output of your code) and the autograder expects to see the output in a certain prescribed way and if it does not find that then it deducts points. I do manually review all grades once the autograder has done its job. This is important not only for this class but probably more so going forward when the output your code produces will be consumed by both humans and machines."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Here’s your roadmap for the semester!\n\nContent (): This page contains the lecture slides for the week.\nLab (): This page contains lab for the week. This is usually a step-by-step instruction accompanied by a GitHub repo. A GitHub classroom link would be provided, once you accept an assignment (in GitHub classroom parlance everything is an assignment) a new repo would be created for you to work in. You will be starting this activity in class and submitting it before the due-date (Wednesday of next week 11:59PM unless otherwise stated).\nAssignment (): This page contains the instructions for each assignment. Similar to the labs, there would be a GitHub classroom link provided to you for each assignment. Unlike the labs the assignments are more detailed and require significantly more coding effort. Assignments are also due by Wednesday of next week 11:59PM unless otherwise stated.\n\n\n\n\n\n\n\nSubscribe!\n\n\n\nYou can subscribe to this calendar URL in Outlook, Google Calendar, or Apple Calendar:\n\n\n\n Download\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Schedule for the semester PPOL 5206 Spring 2024\n    \n    \n      This course is divided into 3 phases: Intro to Big Data and the Cloud, Apache Spark Ecosystem, Other Big Data technologies & misc. topics.\n    \n  \n  \n    \n      \n      Date\n      Module\n      Lecture\n      Lab\n      Notes\n    \n  \n  \n    \n      Phase 1\n    \n    Intro to Big Data and the Cloud\n2024-01-11\nCourse overview\n\nLecture 1\n\nLab 1\n\nCourse overview. Introduction to big data concepts. The Cloud.\n\n    Intro to Big Data and the Cloud\n2024-01-18\nCloud services\n\nLecture 2\n\nLab 2\n\nSetting up your AWS account.\n\n    Intro to Big Data and the Cloud\n2024-01-25\nParallelization\n\nLecture 3\n\nLab 3\n\nExplore parallelization with Python multiprocessing.\n\n    Intro to Big Data and the Cloud\n2024-02-01\nEvent driven cloud processing, containers and code portability\n\nContainers and Lambda\n\nLecture 4\n\nLab 4,\"Building Docker containers and working with AWS Lambda.\"\n\n    Intro to Big Data and the Cloud\n2024-02-08\nDuckDB, Polars and file formats\n\nLecture 5\n\nLab 5\n\nWorking with Big Data on a single machine wht DuckDB and Python Polars.\n\n    \n      Phase 2\n    \n    Apache Spark Ecosystem\n2024-02-15\nIntro to Apache Spark - Spark RDD\n\nLecture 6\n\nLab 6\n\nSetting up Apache Spark on Amazon SageMaker & EMR. Analyze datasets using Spark Resilient Distributed Datasets (RDD).\n\n    Apache Spark Ecosystem\n2024-02-22\nSpark SQL & Spark Dataframes\n\nLecture 7\n\nLab 7\n\nStructured data processing with Spark SQL and the Spark DataFrame API.\n\n    \n2024-03-07\nNO CLASS - Spring Break\n\n\n\n\n    Apache Spark Ecosystem\n2024-03-14\nSpark ML\n\nLecture 8\n\nLab 8\n\nScaling machine learning with Spark.\n\n    Apache Spark Ecosystem\n2024-03-21\nSpark NLP\n\nLecture 9\n\nLab 9\n\nNatural Language Propcessing Using Spark.\n\n    Apache Spark Ecosystem\n2024-04-11\nAccelerate Python workloads with Ray, RAPIDS\n\nLecture 10\n\nLab 10\n\nUsing Ray and RAPIDS (GPUs for big data analytics).\n\n    \n      Phase 3\n    \n    \n2024-03-28\nNO CLASS - Easter Break\n\n\n\n\n    Apache Spark Ecosystem\n2024-04-04\nSpark Streaming\n\nLecture 11\n\nLab 11\n\nWorking with streaming data using Spark Streaming.\n\n    Other Big Data technologies & misc. topics\n2024-04-18\nML Feature Store\n\nLecture 12\n\n\nPreparing and sotring ML ready data with feature stores.\n\n    Other Big Data technologies & misc. topics\n2024-04-25\nVector Databases\n\nLecture 13\n\nLab 13\n\nBig data for generative AI.\n\n    Other Big Data technologies & misc. topics\n2024-05-02\nProject discussion & Open Session\n\nLecture 14\n\n\nLast class for the semester"
  },
  {
    "objectID": "slides/01-slides.html#agenda-for-todays-session",
    "href": "slides/01-slides.html#agenda-for-todays-session",
    "title": "Lecture 1",
    "section": "Agenda for today’s session",
    "text": "Agenda for today’s session\n\nCourse and syllabus overview\nBig Data Concepts\n\nDefinition\nChallenges\nApproaches\n\nData Engineering\nIntroduction to bash\n\nLab: Linux command line"
  },
  {
    "objectID": "slides/01-slides.html#bookmark-these-links",
    "href": "slides/01-slides.html#bookmark-these-links",
    "title": "Lecture 1",
    "section": "Bookmark these links!",
    "text": "Bookmark these links!\n\nCourse website: https://aa1603.georgetown.edu/ppol5206\nGitHub Organization for your deliverables: https://github.com/gu-ppol/\nSlack Workspace: https://georgetown-ime2320.slack.com\nInstructors email: ppol5206-spring-2023@georgetown.edu\nCanvas: https://georgetown.instructure.com/courses/187746\n\n\n\n\n\n\n\nThese are also pinned on the Slack main channel"
  },
  {
    "objectID": "slides/01-slides.html#instructional-team---professor",
    "href": "slides/01-slides.html#instructional-team---professor",
    "title": "Lecture 1",
    "section": "Instructional Team - Professor",
    "text": "Instructional Team - Professor\n\n\nAmit Arora\n\nPrincipal Solutions Architect - AI/ML at AWS\nAdjunct Professor at Georgetown University\nMultiple patents in telecommunications and applications of ML in telecommunications\n\nFun Facts\n\nI am a self-published author https://blueberriesinmysalad.com/\nMy book “Blueberries in my salad: my forever journey towards fitness & strength” is written as code in R and Markdown\nI love to read books about health and human performance, productivity, philosophy and Mathematics for ML. My reading list is online!"
  },
  {
    "objectID": "slides/01-slides.html#instructional-team---ta",
    "href": "slides/01-slides.html#instructional-team---ta",
    "title": "Lecture 1",
    "section": "Instructional Team - TA",
    "text": "Instructional Team - TA\n\n\nDheeraj Oruganty\n\nEducational Background: Achieved a Bachelor’s Degree in Computer Science from Jawaharlal Nehru Institute of Technology in Hyderabad.\nInternship: Served as a Machine Learning Intern at Cluzters.ai, contributing to the development of machine vision models tailored for edge devices.\nPersonal Interests: Finds joy participating in hikes, meeting new people and embarking on road trips.\nInteresting Tidbits: Explored 6 different countries in the past 90 days.\nAdventurous Journey: Undertook a remarkable 15-hour road trip on a motorcycle in India!"
  },
  {
    "objectID": "slides/01-slides.html#course-description",
    "href": "slides/01-slides.html#course-description",
    "title": "Lecture 1",
    "section": "Course Description",
    "text": "Course Description\nData is everywhere! Many times, it’s just too big to work with traditional tools. This is a hands-on, practical workshop style course about using cloud computing resources to do analysis and manipulation of datasets that are too large to fit on a single machine and/or analyzed with traditional tools. The course will focus on Spark, MapReduce, the Hadoop Ecosystem and other tools.\nYou will understand how to acquire and/or ingest the data, and then massage, clean, transform, analyze, and model it within the context of big data analytics. You will be able to think more programmatically and logically about your big data needs, tools and issues.\n\nAlways refer to the syllabus and calendar in the course website for class policies."
  },
  {
    "objectID": "slides/01-slides.html#learning-objectives",
    "href": "slides/01-slides.html#learning-objectives",
    "title": "Lecture 1",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nSetup, operate and manage big data tools and cloud infrastructure, including Spark, MapReduce, DataBricks, Hadoop on Microsoft Azure and Amazon Web Services\nUse ancillary tools that support big data processing, including git and the Linux command line\nExecute a big data analytics exercise from start to finish: ingest, wrangle, clean, analyze, store, and present\nDevelop strategies to break down large problems and datasets into manageable pieces\nIdentify broad spectrum resources and documentation to remain current with big data tools and developments\nCommunicate and interpret the big data analytics results through written and verbal methods"
  },
  {
    "objectID": "slides/01-slides.html#evaluation",
    "href": "slides/01-slides.html#evaluation",
    "title": "Lecture 1",
    "section": "Evaluation",
    "text": "Evaluation\n\nGroup project : 40%\nAssignments : 30%\nLab completions : 20%\nQuizzes : 10%"
  },
  {
    "objectID": "slides/01-slides.html#course-materials",
    "href": "slides/01-slides.html#course-materials",
    "title": "Lecture 1",
    "section": "Course Materials",
    "text": "Course Materials\n\nSlides/labs/assignment on Website/GitHub\nQuizzes and readings in Canvas"
  },
  {
    "objectID": "slides/01-slides.html#communication",
    "href": "slides/01-slides.html#communication",
    "title": "Lecture 1",
    "section": "Communication",
    "text": "Communication\n\nSlack is the primary form of communication\nInstructional team email: ppol5206-spring-2023@georgetown.edu"
  },
  {
    "objectID": "slides/01-slides.html#slack-rules",
    "href": "slides/01-slides.html#slack-rules",
    "title": "Lecture 1",
    "section": "Slack rules:",
    "text": "Slack rules:\n\nPost any question/comment about the course, assignments or any technical issue.\nDMs are to be used sparingly\nYou may not DM multiple people in the instructional team at the same time for the same issue\nKeep an eye on the questions posted in Slack. Use the search function. It’s very possible that we have already answered a questions\nYou may DM us back only if we DM you first on a given issue\nLab/assignment/project questions will only be answered up to 6 hours before something is due (i.e. 6pm on Mondays)"
  },
  {
    "objectID": "slides/01-slides.html#project",
    "href": "slides/01-slides.html#project",
    "title": "Lecture 1",
    "section": "Project",
    "text": "Project\n\nGroups of 3-4 students\nUse an archive of Reddit data, augmented with external data\nExploratory analysis\nNLP\nMachine Learning\nWriteup\n\nData sourcing and ingesting\nExploratory analysis\nModeling\nChallenges and Learnings\nConclusions\nFuture work"
  },
  {
    "objectID": "slides/01-slides.html#where-does-it-come-from-how-is-it-being-created",
    "href": "slides/01-slides.html#where-does-it-come-from-how-is-it-being-created",
    "title": "Lecture 1",
    "section": "Where does it come from?How is it being created?",
    "text": "Where does it come from?How is it being created?"
  },
  {
    "objectID": "slides/01-slides.html#in-one-minute-of-time-2018",
    "href": "slides/01-slides.html#in-one-minute-of-time-2018",
    "title": "Lecture 1",
    "section": "In one minute of time (2018)",
    "text": "In one minute of time (2018)"
  },
  {
    "objectID": "slides/01-slides.html#in-one-minute-of-time-2019",
    "href": "slides/01-slides.html#in-one-minute-of-time-2019",
    "title": "Lecture 1",
    "section": "In one minute of time (2019)",
    "text": "In one minute of time (2019)"
  },
  {
    "objectID": "slides/01-slides.html#in-one-minute-of-time-2020",
    "href": "slides/01-slides.html#in-one-minute-of-time-2020",
    "title": "Lecture 1",
    "section": "In one minute of time (2020)",
    "text": "In one minute of time (2020)"
  },
  {
    "objectID": "slides/01-slides.html#in-one-minute-of-time-2021",
    "href": "slides/01-slides.html#in-one-minute-of-time-2021",
    "title": "Lecture 1",
    "section": "In one minute of time (2021)",
    "text": "In one minute of time (2021)"
  },
  {
    "objectID": "slides/01-slides.html#a-lot-of-it-is-hapenning-online.",
    "href": "slides/01-slides.html#a-lot-of-it-is-hapenning-online.",
    "title": "Lecture 1",
    "section": "A lot of it is hapenning online.",
    "text": "A lot of it is hapenning online.\nWe can record every: * click * ad impression * billing event * video interaction * server request * transaction * network message * fault * …"
  },
  {
    "objectID": "slides/01-slides.html#it-can-also-be-user-generated-content-e.g.",
    "href": "slides/01-slides.html#it-can-also-be-user-generated-content-e.g.",
    "title": "Lecture 1",
    "section": "It can also be user-generated content, e.g.:",
    "text": "It can also be user-generated content, e.g.:\n\nInstagram posts\nTweets\nVideos\nYelp reviews\nFacebook posts\nStack Overflow posts\n…"
  },
  {
    "objectID": "slides/01-slides.html#but-health-and-scientific-computing-create-a-lot-too",
    "href": "slides/01-slides.html#but-health-and-scientific-computing-create-a-lot-too",
    "title": "Lecture 1",
    "section": "But health and scientific computing create a lot too!",
    "text": "But health and scientific computing create a lot too!\n\n???"
  },
  {
    "objectID": "slides/01-slides.html#theres-lots-of-graph-data-too",
    "href": "slides/01-slides.html#theres-lots-of-graph-data-too",
    "title": "Lecture 1",
    "section": "There’s lots of graph data too",
    "text": "There’s lots of graph data too\nMany interesting datasets have a graph structure:\n\nSocial networks\nGoogle’s knowledge graph\nTelecom networks\nComputer networks\nRoad networks\nCollaboration/relationships\n\nSome of these are HUGE"
  },
  {
    "objectID": "slides/01-slides.html#apache-web-server-log-files",
    "href": "slides/01-slides.html#apache-web-server-log-files",
    "title": "Lecture 1",
    "section": "Apache (web server) log files",
    "text": "Apache (web server) log files"
  },
  {
    "objectID": "slides/01-slides.html#system-log-files",
    "href": "slides/01-slides.html#system-log-files",
    "title": "Lecture 1",
    "section": "System log files",
    "text": "System log files"
  },
  {
    "objectID": "slides/01-slides.html#internet-of-things-iot",
    "href": "slides/01-slides.html#internet-of-things-iot",
    "title": "Lecture 1",
    "section": "Internet of Things (IoT)",
    "text": "Internet of Things (IoT)\nSensors everywhere…"
  },
  {
    "objectID": "slides/01-slides.html#smartphones-collecting-our-information",
    "href": "slides/01-slides.html#smartphones-collecting-our-information",
    "title": "Lecture 1",
    "section": "Smartphones collecting our information",
    "text": "Smartphones collecting our information"
  },
  {
    "objectID": "slides/01-slides.html#where-else",
    "href": "slides/01-slides.html#where-else",
    "title": "Lecture 1",
    "section": "Where else?",
    "text": "Where else?\n\nThe Internet\nTransactions\nDatabases\nExcel\nPDF Files\nAnything digital (music, movies, apps)\nSome old floppy disk lying around the house"
  },
  {
    "objectID": "slides/01-slides.html#typical-real-world-scenario",
    "href": "slides/01-slides.html#typical-real-world-scenario",
    "title": "Lecture 1",
    "section": "Typical real world scenario",
    "text": "Typical real world scenario\nYou have a laptop with 16GB of RAM and a 256GB Solid State drive. You are given a 1TB dataset in text files, where every file is slightly different. Oh no, what do you do?\nAD: This was the situation I experienced during my AstraZeneca interview. We had to do a data analysis for our interview, and the data given was the FDA drug adverse event reporting database."
  },
  {
    "objectID": "slides/01-slides.html#lets-discuss",
    "href": "slides/01-slides.html#lets-discuss",
    "title": "Lecture 1",
    "section": "Let’s discuss!",
    "text": "Let’s discuss!\n\nExponential data growth"
  },
  {
    "objectID": "slides/01-slides.html#big-data-definitions",
    "href": "slides/01-slides.html#big-data-definitions",
    "title": "Lecture 1",
    "section": "Big Data Definitions",
    "text": "Big Data Definitions\nWikipedia\n“In essence, is a term for a collection of datasets so large and complex that it becomes difficult to process using traditional tools and applications. Big Data technologies describe a new generation of technologies and architectures designed to economically extract value from very large volumes of a wide variety of data, by enabling high-velocity capture, discover and/or analysis”\nO’Reilly\n“Big data is when the size of the data itself becomes part of the problem”\nEMC/IDC\n“Big data technologies describe a new generation of technologies and architectures, designed to economically extract value from very large volumes of a wide variety of data, by enabling high-velocity capture, discovery, and/or analysis.”"
  },
  {
    "objectID": "slides/01-slides.html#frameworks-for-thinking-about-big-data",
    "href": "slides/01-slides.html#frameworks-for-thinking-about-big-data",
    "title": "Lecture 1",
    "section": "Frameworks for thinking about Big Data",
    "text": "Frameworks for thinking about Big Data\nIBM: (The famous 3-V’s definition)\n\nVolume (Gigabytes -> Exabytes)\nVelocity (Batch -> Streaming Data)\nVariety (Structured, Semi-structured, & Unstructured)\n\nAdditional V’s\n\nVariability\nVeracity\nVisualization\nValue"
  },
  {
    "objectID": "slides/01-slides.html#think-of-data-size-as-a-function-of-processing-and-storage",
    "href": "slides/01-slides.html#think-of-data-size-as-a-function-of-processing-and-storage",
    "title": "Lecture 1",
    "section": "Think of data size as a function of processing and storage",
    "text": "Think of data size as a function of processing and storage\n\nCan you analyze/process your data on a single machine?\nCan you store (or is it stored) on a single machine?\n\nIf any of of the answers is no then you have a big-ish data problem!"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes",
    "href": "slides/01-slides.html#relative-data-sizes",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes-1",
    "href": "slides/01-slides.html#relative-data-sizes-1",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes-2",
    "href": "slides/01-slides.html#relative-data-sizes-2",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes-3",
    "href": "slides/01-slides.html#relative-data-sizes-3",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes-4",
    "href": "slides/01-slides.html#relative-data-sizes-4",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes-5",
    "href": "slides/01-slides.html#relative-data-sizes-5",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#data-types",
    "href": "slides/01-slides.html#data-types",
    "title": "Lecture 1",
    "section": "Data Types",
    "text": "Data Types\n\nStructured\nUnstructured\nNatural language\nMachine-generated\nGraph-based\nAudio, video, and images\nStreaming"
  },
  {
    "objectID": "slides/01-slides.html#big-data-vs.-small-data",
    "href": "slides/01-slides.html#big-data-vs.-small-data",
    "title": "Lecture 1",
    "section": "Big Data vs. Small Data",
    "text": "Big Data vs. Small Data\n\n\n\n\n \n  \n     \n    Small Data is usually... \n    On the other hand, Big Data... \n  \n \n\n  \n    Goals \n    gathered for a specific goal \n    may have a goal in mind when it's first started, but things can evolve or take unexpected directions \n  \n  \n    Location \n    in one place, and often in a single computer file \n    can be in multiple files in multiple servers on computers in different geographic locations \n  \n  \n    Structure/Contents \n    highly structured like an Excel spreadsheet, and it's got rows and columns of data \n    can be unstructured, it can have many formats in files involved across disciplines, and may link to other resources \n  \n  \n    Preparation \n    prepared by the end user for their own purposes \n    is often prepared by one group of people, analyzed by a second group of people, and then used by a third group of people, and they may have different purposes, and they may have different disciplines \n  \n  \n    Longevity \n    kept for a specific amount of time after the project is over because there's a clear ending point. In the academic world it's maybe five or seven years and then you can throw it away \n    contains data that must be stored in perpetuity. Many big data projects extend into the past and future \n  \n  \n    Measurements \n    measured with a single protocol using set units and it's usually done at the same time \n    is collected and measured using many sources, protocols, units, etc \n  \n  \n    Reproducibility \n    be reproduced in their entirety if something goes wrong in the process \n    replication is seldom feasible \n  \n  \n    Stakes \n    if things go wrong the costs are limited, it's not an enormous problem \n    can have high costs of failure in terms of money, time and labor \n  \n  \n    Access \n    identified by a location specified in a row/column \n    unless it is exceptionally well designed, the organization can be inscrutable \n  \n  \n    Analysis \n    analyzed together, all at once \n    is ordinarily analyzed in incremental steps"
  },
  {
    "objectID": "slides/01-slides.html#traditional-data-analysis-tools-like-r-and-python-are-single-threaded",
    "href": "slides/01-slides.html#traditional-data-analysis-tools-like-r-and-python-are-single-threaded",
    "title": "Lecture 1",
    "section": "Traditional data analysis tools like R and Python are single threaded",
    "text": "Traditional data analysis tools like R and Python are single threaded"
  },
  {
    "objectID": "slides/01-slides.html#tools-at-a-glance",
    "href": "slides/01-slides.html#tools-at-a-glance",
    "title": "Lecture 1",
    "section": "Tools at-a-glance",
    "text": "Tools at-a-glance\n\n\nLanguages, libraries, and projects\n\nPython\n\npandas\npolars\nPySpark\nduckdb\ndask\nray\n\nApache Arrow\nApache Spark\nSQL\n\nWe’ll talk briefly about Apache Hadoop today but we will not cover it in this course.\n\nCloud Services\n\nAmazon Web Services (AWS)\n\nAWS Sagemaker\nAmazon S3\n\nAzure\n\nAzure Blob\nAzure Machine Learning\n\n\nOther:\n\nAWS Elastic MapReduce (EMR)"
  },
  {
    "objectID": "slides/01-slides.html#additional-links-of-interest",
    "href": "slides/01-slides.html#additional-links-of-interest",
    "title": "Lecture 1",
    "section": "Additional links of interest",
    "text": "Additional links of interest\n\nMatt Turck’s Machine Learning, Artificial Intelligence & Data Landscape (MAD)\n\nArticle\nInteractive Landscape\n\nIs there life after Hadoop?\n10 Best Big Data Tools for 2023"
  },
  {
    "objectID": "slides/01-slides.html#difference-between-data-scientist-and-data-engineer",
    "href": "slides/01-slides.html#difference-between-data-scientist-and-data-engineer",
    "title": "Lecture 1",
    "section": "Difference between Data Scientist and Data Engineer",
    "text": "Difference between Data Scientist and Data Engineer\nIn this course, you’ll be doing a little data engineering!"
  },
  {
    "objectID": "slides/01-slides.html#responsibilities",
    "href": "slides/01-slides.html#responsibilities",
    "title": "Lecture 1",
    "section": "Responsibilities",
    "text": "Responsibilities"
  },
  {
    "objectID": "slides/01-slides.html#data-engineering-falls-into-levels-2-and-3-primarily",
    "href": "slides/01-slides.html#data-engineering-falls-into-levels-2-and-3-primarily",
    "title": "Lecture 1",
    "section": "Data Engineering falls into levels 2 and 3 primarily",
    "text": "Data Engineering falls into levels 2 and 3 primarily"
  },
  {
    "objectID": "slides/01-slides.html#as-an-analystdata-scientist-you-really-need-both",
    "href": "slides/01-slides.html#as-an-analystdata-scientist-you-really-need-both",
    "title": "Lecture 1",
    "section": "As an analyst/data scientist, you really need both",
    "text": "As an analyst/data scientist, you really need both"
  },
  {
    "objectID": "slides/01-slides.html#architecture",
    "href": "slides/01-slides.html#architecture",
    "title": "Lecture 1",
    "section": "Architecture",
    "text": "Architecture"
  },
  {
    "objectID": "slides/01-slides.html#storage",
    "href": "slides/01-slides.html#storage",
    "title": "Lecture 1",
    "section": "Storage",
    "text": "Storage"
  },
  {
    "objectID": "slides/01-slides.html#source-control",
    "href": "slides/01-slides.html#source-control",
    "title": "Lecture 1",
    "section": "Source control",
    "text": "Source control"
  },
  {
    "objectID": "slides/01-slides.html#orchestration",
    "href": "slides/01-slides.html#orchestration",
    "title": "Lecture 1",
    "section": "Orchestration",
    "text": "Orchestration"
  },
  {
    "objectID": "slides/01-slides.html#processing",
    "href": "slides/01-slides.html#processing",
    "title": "Lecture 1",
    "section": "Processing",
    "text": "Processing"
  },
  {
    "objectID": "slides/01-slides.html#analytics",
    "href": "slides/01-slides.html#analytics",
    "title": "Lecture 1",
    "section": "Analytics",
    "text": "Analytics"
  },
  {
    "objectID": "slides/01-slides.html#machine-learning",
    "href": "slides/01-slides.html#machine-learning",
    "title": "Lecture 1",
    "section": "Machine Learning",
    "text": "Machine Learning"
  },
  {
    "objectID": "slides/01-slides.html#governance",
    "href": "slides/01-slides.html#governance",
    "title": "Lecture 1",
    "section": "Governance",
    "text": "Governance"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-1",
    "href": "slides/01-slides.html#linux-command-line-1",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nTerminal\n\n\nTerminal access was THE ONLY way to do programming\nNo GUIs! No Spyder, Jupyter, RStudio, etc.\nCoding is still more powerful than graphical interfaces for complex jobs\nCoding makes work repeatable"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-2",
    "href": "slides/01-slides.html#linux-command-line-2",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nBASH\n\n\nCreated in 1989 by Brian Fox\nBrian Fox also built the first online interactive banking software\nBASH is a command processor\nConnection between you and the machine language and hardware"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-3",
    "href": "slides/01-slides.html#linux-command-line-3",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nThe Prompt\nusername@hostname:current_directory $\nWhat do we learn from the prompt?\n\nWho you are - username\nThe machine where your code is running - hostname\nThe directory where your code is running - current_directory\nThe shell type - $ - this symbol means BASH"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-4",
    "href": "slides/01-slides.html#linux-command-line-4",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nSyntax\nCOMMAND -F --FLAG * COMMAND is the program * Everything after that are arguments * F is a single letter flag * FLAG is a single word or words connected by dashes flag. A space breaks things into a new argument. + Sometimes single letter and long form flags (e.g. F and FLAG) can refer to the same argument\nCOMMAND -F --FILE file1\nHere we pass an text argument “file1” into the FILE flag\nThe -h flag is usually to get help. You can also run the man command and pass the name of the program as the argument to get the help page.\nLet’s try basic commands:\n\ndate to get the current date\nwhoami to get your user name\necho \"Hello World\" to print to the console"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-5",
    "href": "slides/01-slides.html#linux-command-line-5",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nExamining Files\nFind out your Present Working Directory pwd\nExamine the contents of files and folders using the ls command\nMake new files from scratch using the touch command\nGlobbing - how to select files in a general way\n\n\\* for wild card any number of characters\n\\? for wild card for a single character\n[] for one of many character options\n! for exclusion\nspecial options [:alpha:], [:alnum:], [:digit:], [:lower:], [:upper:]\n\nReference material Reference material: Shell Lesson 1,2,4,5"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-6",
    "href": "slides/01-slides.html#linux-command-line-6",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nNavigating Directories\nKnowing where your terminal is executing code ensures you are working with the right inputs and making the right outputs.\nUse the command pwd to determine the Present Working Directory.\nLet’s say you need to change to a folder called “git-repo”. To change directories you can use a command like cd git-repo.\n\n. refers to the current directory, such as ./git-repo\n.. can be used to move up one folder, use cd .., and can be combined to move up multiple levels ../../my_folder\n/ is the root of the Linux OS, where there are core folders, such as system, users, etc.\n~ is the home directory. Move to folders referenced relative to this path by including it at the start of your path, for example ~/projects.\n\nTo view the structure of directories from your present working directory, use the tree command\nReference link"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-7",
    "href": "slides/01-slides.html#linux-command-line-7",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nInteracting with Files\nNow that we know how to navigate through directories, we need to learn the commands for interacting with files\n\nmv to move files from one location to another\n\nCan use file globbing here - ?, *, [], …\n\ncp to copy files instead of moving\n\nCan use file globbing here - ?, *, [], …\n\nmkdir to make a directory\nrm to remove files\nrmdir to remove directories\nrm -rf to blast everything! WARNING!!! DO NOT USE UNLESS YOU KNOW WHAT YOU ARE DOING"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-8",
    "href": "slides/01-slides.html#linux-command-line-8",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nUsing BASH for Data Exploration\nCommands:\n\nhead FILENAME / tail FILENAME - glimpsing the first / last few rows of data\nmore FILENAME / less FILENAME - viewing the data with basic up / (up & down) controls\ncat FILENAME - print entire file contents into terminal\nvim FILENAME - open (or edit!) the file in vim editor\ngrep FILENAME - search for lines within a file that match a regex expression\nwc FILENAME - count the number of lines (-l flag) or number of words (-w flag)\n\nReference link Reference material: Text Lesson 8,9,15,16"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-9",
    "href": "slides/01-slides.html#linux-command-line-9",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nPipes and Arrows\n\n| sends the stdout to another command (is the most powerful symbol in BASH!)\n> sends stdout to a file and overwrites anything that was there before\n>> appends the stdout to the end of a file (or starts a new file from scratch if one does not exist yet)\n< sends stdin into the command on the left\n\nTo-dos:\n\necho Hello World\nCounting rows of data with certain attributes\n\nReference material: Text Lesson 1,2,3,4,5"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-10",
    "href": "slides/01-slides.html#linux-command-line-10",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nAlias and User Files\n.bashrc is where your shell settings are located\nIf we wanted a shortcut to find out the number of our running processes, we would write a commmand like whoami | xargs ps -u | wc -l.\nWe don’t want to write out this full command every time! Let’s make an alias.\nalias alias_name=\"command_to_run\"\nalias nproc=\"whoami | xargs ps -u | wc -l\"\nNow we need to put this alias into the .bashrc\nalias nproc=\"whoami | xargs ps -u | wc -l\" >> ~/.bashrc\nWhat happened??\necho alias nproc=\"whoami | xargs ps -u | wc -l\" >> ~/.bashrc\nYour commands get saved in ~/.bash_history"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-11",
    "href": "slides/01-slides.html#linux-command-line-11",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nProcess Managment\nUse the command ps to see your running processes.\nUse the command top or even better htop to see all the running processes on the machine.\nInstall the program htop using the command sudo yum install htop -y\nFind the process ID (PID) so you can kill a broken process.\nUse the command kill [PID NUM] to signal the process to terminate. If things get really bad, then use the command kill -9 [PID NUM]\nTo kill a command in the terminal window it is running in, try using Ctrl + C or Ctrl + /\nRun the cat command on its own to let it stay open. Now open a new terminal to examine the processes and find the cat process.\nReference material: Text Lesson 1,2,3,7,9,10\nTry playing a Linux game!\nhttps://gitlab.com/slackermedia/bashcrawl is a game to help you practice your navigation and file access skills. Click on the binder link in this repo to launch a jupyter lab session and explore!"
  },
  {
    "objectID": "slides/02-slides.html#look-back",
    "href": "slides/02-slides.html#look-back",
    "title": "Lecture 2",
    "section": "Look back",
    "text": "Look back\n\nGreat use of Slack\nBig data definition\nUsed the shell in Linux on a virtual machine through Codespaces"
  },
  {
    "objectID": "slides/02-slides.html#agenda-and-goals-for-today",
    "href": "slides/02-slides.html#agenda-and-goals-for-today",
    "title": "Lecture 2",
    "section": "Agenda and Goals for Today",
    "text": "Agenda and Goals for Today\n\nQuick tour of the cloud services that are used in the course\nExtended Lab:\n\nSetting up AWS accounts\nStarting VMs in the cloud and connecting to them"
  },
  {
    "objectID": "slides/02-slides.html#glossary",
    "href": "slides/02-slides.html#glossary",
    "title": "Lecture 2",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\n \n  \n    Term \n    Definition \n  \n \n\n  \n    Local \n    Your current workstation (laptop, desktop, etc.), wherever you start the terminal/console application. \n  \n  \n    Remote \n    Any machine you connect to via ssh or other means."
  },
  {
    "objectID": "slides/02-slides.html#working-on-a-single-machine",
    "href": "slides/02-slides.html#working-on-a-single-machine",
    "title": "Lecture 2",
    "section": "Working on a single machine",
    "text": "Working on a single machine\nYou are most likely using traditional data analysis tools, which are single threaded and run on a single machine."
  },
  {
    "objectID": "slides/02-slides.html#the-big-data-problem",
    "href": "slides/02-slides.html#the-big-data-problem",
    "title": "Lecture 2",
    "section": "The BIG DATA problem",
    "text": "The BIG DATA problem"
  },
  {
    "objectID": "slides/02-slides.html#is-moores-law-dead",
    "href": "slides/02-slides.html#is-moores-law-dead",
    "title": "Lecture 2",
    "section": "Is Moore’s Law Dead?",
    "text": "Is Moore’s Law Dead?"
  },
  {
    "objectID": "slides/02-slides.html#new-hardware",
    "href": "slides/02-slides.html#new-hardware",
    "title": "Lecture 2",
    "section": "New Hardware",
    "text": "New Hardware\nNeed\n\nThe demand for data processing will not be met by relying on the same technology.\nThe key to modern data processing is new semiconductors\n\nNot just squeezing more transistors per area\nNeed new compute architectures that are built and optimized for specialized functions\n\nSpecialized edge hardware for Edge Computing\nWhile many declare Moore’s Law to be broken or no longer valid, in reality it’s not the law that is broken but rather a heat problem.\n\nWhat\n\nGraphic Processing Units (GPUs)\nField Programmable Gate Arrays (FPGAs)\nData Processing Units (DPUs)\nPhotonic computing"
  },
  {
    "objectID": "slides/02-slides.html#so-we-cant-store-or-process-data-on-a-single-machine-what-do-we-do",
    "href": "slides/02-slides.html#so-we-cant-store-or-process-data-on-a-single-machine-what-do-we-do",
    "title": "Lecture 2",
    "section": "So, we can’t store or process data on a single machine, what do we do?",
    "text": "So, we can’t store or process data on a single machine, what do we do?"
  },
  {
    "objectID": "slides/02-slides.html#we-distribute",
    "href": "slides/02-slides.html#we-distribute",
    "title": "Lecture 2",
    "section": "We distribute",
    "text": "We distribute\nMore CPUs, more memory, more storage!"
  },
  {
    "objectID": "slides/02-slides.html#simple-we-use-the-cloud",
    "href": "slides/02-slides.html#simple-we-use-the-cloud",
    "title": "Lecture 2",
    "section": "Simple, we use the cloud",
    "text": "Simple, we use the cloud"
  },
  {
    "objectID": "slides/02-slides.html#cloud-computing-is-a-big-deal",
    "href": "slides/02-slides.html#cloud-computing-is-a-big-deal",
    "title": "Lecture 2",
    "section": "Cloud computing is a big deal!",
    "text": "Cloud computing is a big deal!\nBenefits\n\nProvides access to low-cost computing\nCosts are decreasing every year\nElastic\nPAAS works!\nMany other benefits…"
  },
  {
    "objectID": "slides/02-slides.html#what-is-the-claaaaaaawd-the-cloud",
    "href": "slides/02-slides.html#what-is-the-claaaaaaawd-the-cloud",
    "title": "Lecture 2",
    "section": "What is the claaaaaaawd (the cloud)",
    "text": "What is the claaaaaaawd (the cloud)"
  },
  {
    "objectID": "slides/02-slides.html#what-is-the-cloud",
    "href": "slides/02-slides.html#what-is-the-cloud",
    "title": "Lecture 2",
    "section": "What is the cloud?",
    "text": "What is the cloud?\n\\kloud\\ noun\nthe practice of storing regularly used computer data on multiple servers that can be accessed through the Internet\nUsing someone else’s computer(s)"
  },
  {
    "objectID": "slides/02-slides.html#nist-definition",
    "href": "slides/02-slides.html#nist-definition",
    "title": "Lecture 2",
    "section": "NIST Definition",
    "text": "NIST Definition"
  },
  {
    "objectID": "slides/02-slides.html#service-models",
    "href": "slides/02-slides.html#service-models",
    "title": "Lecture 2",
    "section": "Service Models",
    "text": "Service Models"
  },
  {
    "objectID": "slides/02-slides.html#the-evolution-of-the-cloud",
    "href": "slides/02-slides.html#the-evolution-of-the-cloud",
    "title": "Lecture 2",
    "section": "The evolution of the Cloud",
    "text": "The evolution of the Cloud\n\n\n\n\n \n  \n    Yesterday \n    Today \n    Tomorrow \n  \n \n\n  \n    Limited number of tools and vendors \n    Many tools and vendors to work with \n    Integrated tools and vendors \n  \n  \n    One platform - few devices \n    Multiple platforms - many devices \n    Connected platforms and devices \n  \n  \n    Data is scarce but manageable \n    Overabundance of data \n    Data is used for important business decisions \n  \n  \n    IT has major influence and control \n    IT has limited influence and control \n    IT is strategic to the business \n  \n  \n    People only work when they are at work \n    People work wherever they want \n    People have access to what they need, wherever they are"
  },
  {
    "objectID": "slides/02-slides.html#what-does-the-cloud-look-like",
    "href": "slides/02-slides.html#what-does-the-cloud-look-like",
    "title": "Lecture 2",
    "section": "What does the cloud look like?",
    "text": "What does the cloud look like?"
  },
  {
    "objectID": "slides/02-slides.html#virtual-visit-to-a-microsoft-azure-data-center",
    "href": "slides/02-slides.html#virtual-visit-to-a-microsoft-azure-data-center",
    "title": "Lecture 2",
    "section": "Virtual Visit to a Microsoft Azure Data Center",
    "text": "Virtual Visit to a Microsoft Azure Data Center"
  },
  {
    "objectID": "slides/02-slides.html#microsoft-azure-data-center-in-boydton-va",
    "href": "slides/02-slides.html#microsoft-azure-data-center-in-boydton-va",
    "title": "Lecture 2",
    "section": "Microsoft Azure Data Center in Boydton, VA",
    "text": "Microsoft Azure Data Center in Boydton, VA"
  },
  {
    "objectID": "slides/02-slides.html#loudon-county-va-is-called-cloudon",
    "href": "slides/02-slides.html#loudon-county-va-is-called-cloudon",
    "title": "Lecture 2",
    "section": "Loudon County, VA is called “CLoudon”",
    "text": "Loudon County, VA is called “CLoudon”\n\nHow data centers power VA’s Loudon County: https://gcn.com/articles/2018/10/12/loudoun-county-data-centers.aspx\nThe heart of “The Cloud” is in Virginia: https://www.cbsnews.com/news/cloud-computing-loudoun-county-virginia/\nCBS Sunday Morning Visits the Home of the Internet in Loudoun County: https://biz.loudoun.gov/2017/10/30/cbs-sunday-morning-visits-loudoun/"
  },
  {
    "objectID": "slides/02-slides.html#of-the-worlds-internet-traffic-passes-through-loudon-county-va",
    "href": "slides/02-slides.html#of-the-worlds-internet-traffic-passes-through-loudon-county-va",
    "title": "Lecture 2",
    "section": "70% of the world’s internet traffic passes through Loudon County, VA",
    "text": "70% of the world’s internet traffic passes through Loudon County, VA"
  },
  {
    "objectID": "slides/03-slides.html#agenda-and-goals-for-today",
    "href": "slides/03-slides.html#agenda-and-goals-for-today",
    "title": "Lecture 3",
    "section": "Agenda and Goals for Today",
    "text": "Agenda and Goals for Today\n\nScaling up and scaling out\nParallelization\nMap and Reduce functions\nLab Preview: Parallelization with Python\n\nUse the multiprocessing module\nImplement synchronous and asynchronous processing\n\nHomework Preview: Parallelization with Python\n\nParallel data processing"
  },
  {
    "objectID": "slides/03-slides.html#looking-back",
    "href": "slides/03-slides.html#looking-back",
    "title": "Lecture 3",
    "section": "Looking back",
    "text": "Looking back\n\nContinued great use of Slack \n\nNice interactions\n\nDue date reminders:\n\nAssignment 2: Wednesday, 31st January\nLab 3: Wednesday, 31st January\nAssignment 3: Wednesday, 7th February"
  },
  {
    "objectID": "slides/03-slides.html#glossary",
    "href": "slides/03-slides.html#glossary",
    "title": "Lecture 3",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\n \n  \n    Term \n    Definition \n  \n \n\n  \n    Local \n    Your current workstation (laptop, desktop, etc.), wherever you start the terminal/console application. \n  \n  \n    Remote \n    Any machine you connect to via ssh or other means. \n  \n  \n    EC2 \n    Single virtual machine in the cloud where you can run computation (ephemeral) \n  \n  \n    SageMaker \n    Integrated Developer Environment where you can conduct data science on single machines \n  \n  \n    Ephemeral \n    Lasting for a short time - any machine that will get turned off or place you will lose data \n  \n  \n    Persistent \n    Lasting for a long time - any environment where your work is NOT lost when the timer goes off"
  },
  {
    "objectID": "slides/03-slides.html#typical-real-world-scenarios",
    "href": "slides/03-slides.html#typical-real-world-scenarios",
    "title": "Lecture 3",
    "section": "Typical real world scenarios",
    "text": "Typical real world scenarios\n\n\nYou are a Data Scientist and you want to cross-validate your models. This involves running the model 1000 times but each run takes over an hour.\nYou are a Data Scientist and you want to run multiple models on your data, where each run can take up to 1 hour.\nYou are a genomics researcher and have been using small datasets of sequence data but soon you will receive a new type of sequencing data that is 10 times as large. This means 10x more transcripts to process, but the processing for each transcript is similar.\nYou are an engineer using a fluid dynamics package that has an option to run in parallel. So far, you haven’t used this option on your workstation. When moving from 2D to 3D simulations, the simulation time has more than tripled so it may make sense to take advantage of the parallel feature\nYou are a Data Scientist at the Federal Reserve and you have millions of text to process. So far, you have only executed NLP on thousands of articles and have not implemented machine learning models on them."
  },
  {
    "objectID": "slides/03-slides.html#linear-vs.-parallel",
    "href": "slides/03-slides.html#linear-vs.-parallel",
    "title": "Lecture 3",
    "section": "Linear vs. Parallel",
    "text": "Linear vs. Parallel\n\n\nLinear/Sequential\n\nA program starts to run\nThe program issues an instruction\nThe instruction is executed\nSteps 2 and 3 are repeated\nThe program finishes running\n\n\nParallel\n\nA program starts to run\nThe program divides up the work into chunks of instructions and data\nEach chunk of work is executed independently\nThe chunks of work are reassembled\nThe program finishes running"
  },
  {
    "objectID": "slides/03-slides.html#linear-vs.-parallel-1",
    "href": "slides/03-slides.html#linear-vs.-parallel-1",
    "title": "Lecture 3",
    "section": "Linear vs. Parallel",
    "text": "Linear vs. Parallel"
  },
  {
    "objectID": "slides/03-slides.html#linear-vs.-parallel-2",
    "href": "slides/03-slides.html#linear-vs.-parallel-2",
    "title": "Lecture 3",
    "section": "Linear vs. Parallel",
    "text": "Linear vs. Parallel\nFrom a data science perspective\n\n\nLinear\n\nThe data remains monolithic\nProcedures act on the data sequentially\n\nEach procedure has to complete before the next procedure can start\n\nYou can think of this as a single pipeline\n\n\nParallel\n\nThe data can be split up into chunks\nThe same procedures can be run on each chunk at the same time\nOr, independent procedures can run on different chunks at the same time\nNeed to bring things back together at the end\n\n\n\n\n\nWhat are some examples of linear and parallel data science workflows?"
  },
  {
    "objectID": "slides/03-slides.html#embarrasingly-parallel",
    "href": "slides/03-slides.html#embarrasingly-parallel",
    "title": "Lecture 3",
    "section": "Embarrasingly Parallel",
    "text": "Embarrasingly Parallel\nIt’s easy to speed things up when:\n\nYou need to calculate the same thing many times\nCalculations are independent of each other\nEach calculation takes a decent amount of time\n\nJust run multiple calculations at the same time"
  },
  {
    "objectID": "slides/03-slides.html#embarrasingly-parallel-1",
    "href": "slides/03-slides.html#embarrasingly-parallel-1",
    "title": "Lecture 3",
    "section": "Embarrasingly Parallel",
    "text": "Embarrasingly Parallel\nThe concept is based on the old middle/high school math problem:\n\nIf 5 people can shovel a parking lot in 6 hours, how long will it take 100 people to shovel the same parking lot?\n\nBasic idea is that many hands (cores/instances) make lighter (faster/more efficient) work of the same problem, as long as the effort can be split up appropriately into nearly equal parcels\n\n\nThe classical answer to the problem is 18 minutes"
  },
  {
    "objectID": "slides/03-slides.html#embarassingly-parallel",
    "href": "slides/03-slides.html#embarassingly-parallel",
    "title": "Lecture 3",
    "section": "Embarassingly parallel",
    "text": "Embarassingly parallel\n\n\nIf you can truly split up your problem into multiple independent parts, then you can often get linear speedups with the number of parallel components (to a limit)\n\nThe more cores you use and the more you parallelize, the more you incur communication overhead and decrease available RAM, so the speedup is almost certainly sub-linear, i.e. for a 4-core machine you’ll probably get a 3-4x speedup, but rarely a full 4x speedup1\n\nThe question often is, which part of your problem is embarassingly parallel?\nAmdahl’s law (which we’ll see in a few slides) shows how parallelization can benefit overall if a large proportion of the problem is parallelizable\nIt’s not all milk and honey. Setting up, programming, evaluating, debugging parallel computations requires better infrastructure and more expertise.\n\n\nGorelick & Ozsvald, 2020. High Performance Python, O’Reilly"
  },
  {
    "objectID": "slides/03-slides.html#some-limitations",
    "href": "slides/03-slides.html#some-limitations",
    "title": "Lecture 3",
    "section": "Some limitations",
    "text": "Some limitations\nYou can get speedups by parallelizing computations, but\n\nHaving to transport data between parallel processes (memory bottlenecks) and communication between processes (I/O bottlenecks) can make things more expensive and can exceed the benefits of parallelization\nIf you’re moving a lot of data but not doing a lot of parallel computing, it’s often not worth the effort\n\n\n\nSetting up and debugging parallel programs can be difficult\nBut this has become easier with better software, like the multiprocessing module in Python\n\n\n\nMaking sure that we can get back all the pieces needs monitoring\n\nFailure tolerance and protections (Hadoop, e.g.)\nProper collection and aggregation of the processed data"
  },
  {
    "objectID": "slides/03-slides.html#amdahls-law",
    "href": "slides/03-slides.html#amdahls-law",
    "title": "Lecture 3",
    "section": "Amdahl’s Law",
    "text": "Amdahl’s Law\n\n\n\n\n\n\n\\[\n\\lim_{s\\rightarrow\\infty} S_{latency} = \\frac{1}{1-p}\n\\]\nwhere \\(s\\) is the speedup of that part of the task (which is \\(p\\) proportion of the overall task) benefitting from improved resources.\n\nIf 50% of the task is embarassingly parallel, you can get a maximum speedup of 2-fold, while if 90% is embarassingly parallel, you can get a maximum speedup of \\(1/(1-0.9) = 10\\) fold.\n\n\nBy Daniels220 at English Wikipedia, CC BY-SA 3.0, Link"
  },
  {
    "objectID": "slides/03-slides.html#pros-and-cons-of-parallelization",
    "href": "slides/03-slides.html#pros-and-cons-of-parallelization",
    "title": "Lecture 3",
    "section": "Pros and cons of parallelization",
    "text": "Pros and cons of parallelization\n\nYes\n\nGroup by analysis\nSimulations\nResampling / Bootstrapping\nOptimization\nCross-validation\nTraining bagged models (like Random Forests)\nMultiple chains in a Bayesian MCMC\nScoring (predicting) using trained models\n\n\n\n\n\nNo\n\nSQL Operations\nInverting a matrix\nTraining linear regression\nTraining logistic regression\nTraining trees\nTraining neural nets\nTraining boosted models (like gradient boosted trees)\nEach chain in a Bayesian MCMC\nMost things time series\n\n\n\n\n\n\n\n\n\n\nFor processes in the “No” column, each step depends on a previous step, and so they cannot be parallelized. However, there are approximate numerical methods applicable to big data which are parallelizable and get you to the right answer, based on parallely taking random subsets of the data. We’ll see some of these when we look at Spark ML"
  },
  {
    "objectID": "slides/03-slides.html#pros-and-cons-of-parallelization-1",
    "href": "slides/03-slides.html#pros-and-cons-of-parallelization-1",
    "title": "Lecture 3",
    "section": "Pros and cons of parallelization",
    "text": "Pros and cons of parallelization\n\n\nPros\n\nHigher efficiency\nUsing modern infrastructure\nScalable to larger data, more complex procedures\n\nproviso procedures are embarassingly parallel\n\n\n\n\n\nCons\n\nHigher programming complexity\nNeed proper software infrastructure (MPI, Hadoop, etc)\nNeed to ensure right packages/modules are distributed across processors\nNeed to account for a proportion of jobs failing, and recovering from them\nHence, Hadoop/Spark and other technologies\nHigher setup cost in terms of time/expertise/money\n\n\n\n\n\nThere are good solutions today for most of the cons, so the pros have it and so this paradigm is widely accepted and implemented"
  },
  {
    "objectID": "slides/03-slides.html#distributed-memory-message-passing-model",
    "href": "slides/03-slides.html#distributed-memory-message-passing-model",
    "title": "Lecture 3",
    "section": "Distributed memory / Message Passing Model",
    "text": "Distributed memory / Message Passing Model"
  },
  {
    "objectID": "slides/03-slides.html#data-parallel-model",
    "href": "slides/03-slides.html#data-parallel-model",
    "title": "Lecture 3",
    "section": "Data parallel model",
    "text": "Data parallel model"
  },
  {
    "objectID": "slides/03-slides.html#hybrid-model",
    "href": "slides/03-slides.html#hybrid-model",
    "title": "Lecture 3",
    "section": "Hybrid model",
    "text": "Hybrid model"
  },
  {
    "objectID": "slides/03-slides.html#partitioning-data",
    "href": "slides/03-slides.html#partitioning-data",
    "title": "Lecture 3",
    "section": "Partitioning data",
    "text": "Partitioning data"
  },
  {
    "objectID": "slides/03-slides.html#designing-parallel-programs",
    "href": "slides/03-slides.html#designing-parallel-programs",
    "title": "Lecture 3",
    "section": "Designing parallel programs",
    "text": "Designing parallel programs\n\nData partitioning\nCommunication\nSynchronization / Orchestration\nData dependencies\nLoad balancing\nInput and Output (I/O)\nDebugging\n\n\nA lot of these components are data engineering and DevOps issues\n\n\nInfrastructures have standardized many of these and have helped data scientists implement parallel programming much more easily\n\n\nWe’ll see in the lab how the multiprocessing module in Python makes parallel processing on a machine quite easy to implement"
  },
  {
    "objectID": "slides/03-slides.html#map-and-reduce",
    "href": "slides/03-slides.html#map-and-reduce",
    "title": "Lecture 3",
    "section": "Map and Reduce",
    "text": "Map and Reduce"
  },
  {
    "objectID": "slides/03-slides.html#components-of-a-parallel-programming-workflow",
    "href": "slides/03-slides.html#components-of-a-parallel-programming-workflow",
    "title": "Lecture 3",
    "section": "Components of a parallel programming workflow",
    "text": "Components of a parallel programming workflow\n\nDivide the work into chunks\nWork on each chunk separately\nReassemble the work\n\nThis paradigm is often referred to as a map-reduce framework, or, more descriptively, the split-apply-combine paradigm"
  },
  {
    "objectID": "slides/03-slides.html#map-1",
    "href": "slides/03-slides.html#map-1",
    "title": "Lecture 3",
    "section": "Map",
    "text": "Map\nThe map operation is a 1-1 operation that takes each split and processes it\nThe map operation keeps the same number of objects in its output that were present in its input"
  },
  {
    "objectID": "slides/03-slides.html#map-2",
    "href": "slides/03-slides.html#map-2",
    "title": "Lecture 3",
    "section": "Map",
    "text": "Map\n\n\n\nThe operations included in a particular map can be quite complex, involving multiple steps. In fact, you can implement a pipeline of procedures within the map step to process each data object.\nThe main point is that the same operations will be run on each data object in the map implementation"
  },
  {
    "objectID": "slides/03-slides.html#map-3",
    "href": "slides/03-slides.html#map-3",
    "title": "Lecture 3",
    "section": "Map",
    "text": "Map\nSome examples of a map operations are\n\nExtracting a standard table from online reports from multiple years\nExtracting particular records from multiple JSON objects\nTransforming data (as opposed to summarizing it)\nRun a normalization script on each transcript in a GWAS dataset\nStandardizing demographic data for each of the last 20 years against the 2000 US population"
  },
  {
    "objectID": "slides/03-slides.html#reduce-1",
    "href": "slides/03-slides.html#reduce-1",
    "title": "Lecture 3",
    "section": "Reduce",
    "text": "Reduce\nThe reduce operation takes multiple objects and reduces them to a (perhaps) smaller number of objects using transformations that aren’t amenable to the map paradigm.\nThese transformations are often serial/linear in nature\nThe reduce transformation is usually the last, not-so-elegant transformation needed after most of the other transformations have been efficiently handled in a parallel fashion by map"
  },
  {
    "objectID": "slides/03-slides.html#reduce-2",
    "href": "slides/03-slides.html#reduce-2",
    "title": "Lecture 3",
    "section": "Reduce",
    "text": "Reduce\nThe reduce operation requires\n\n\nAn accumulator function, that will update serially as new data is fed into it\n\nA sequence of objects to run through the accumulator function\n\nA starting value from which the accumulator function starts\n\nProgrammatically, this can be written as"
  },
  {
    "objectID": "slides/03-slides.html#reduce-3",
    "href": "slides/03-slides.html#reduce-3",
    "title": "Lecture 3",
    "section": "Reduce",
    "text": "Reduce\nThe reduce operation works serially from “left” to “right”, passing each object successively through the accumulator function.\nFor example, if we were to add successive numbers with a function called add…"
  },
  {
    "objectID": "slides/03-slides.html#reduce-4",
    "href": "slides/03-slides.html#reduce-4",
    "title": "Lecture 3",
    "section": "Reduce",
    "text": "Reduce\nSome examples:\n\nFinding the common elements (intersection) of a large number of sets\nComputing a table of group-wise summaries\nFiltering\nTabulating"
  },
  {
    "objectID": "slides/03-slides.html#map-reduce-1",
    "href": "slides/03-slides.html#map-reduce-1",
    "title": "Lecture 3",
    "section": "map-reduce",
    "text": "map-reduce\nCombining the map and reduce operations creates a powerful pipeline that can handle a diverse range of problems in the Big Data context"
  },
  {
    "objectID": "slides/03-slides.html#parallelization-and-map-reduce-are-bed-mates",
    "href": "slides/03-slides.html#parallelization-and-map-reduce-are-bed-mates",
    "title": "Lecture 3",
    "section": "Parallelization and map-reduce are bed-mates",
    "text": "Parallelization and map-reduce are bed-mates\n\n\n\n\nOne of the issues here is, how to split the data in a “good” manner so that the map-reduce framework works well"
  },
  {
    "objectID": "slides/03-slides.html#the-multiprocessing-module",
    "href": "slides/03-slides.html#the-multiprocessing-module",
    "title": "Lecture 3",
    "section": "The multiprocessing module",
    "text": "The multiprocessing module\n\nFocused on single-machine multicore parallelism\nFacilitates:\n\nprocess- and thread-based parallel processing\nsharing work over queues\nsharing data among processes"
  },
  {
    "objectID": "slides/03-slides.html#processes-and-threads",
    "href": "slides/03-slides.html#processes-and-threads",
    "title": "Lecture 3",
    "section": "Processes and threads",
    "text": "Processes and threads\n\n\nA process is an executing program, that is self-contained and has dedicated runtime and memory\nA thread is the basic unit to which the operating system allocates processor time. It is an entity within a process. A thread can execute any part of the process code, including parts currently being executed by another thread.\nA thread will often be faster to spin up and terminate than a full process\nThreads can share memory and data with each other\n\n\n\n\n\n\n\n\n\nPython has the Global Interpretor Lock (GIL) which only allows only one thread to interact with Python objects at a time. So the way to parallel process in Python is to do multi-processor parallelization, where we run multiple Python interpretors across multiple processes, each with its own private memory space and GIL."
  },
  {
    "objectID": "slides/03-slides.html#some-concepts-in-multiprocessing",
    "href": "slides/03-slides.html#some-concepts-in-multiprocessing",
    "title": "Lecture 3",
    "section": "Some concepts in multiprocessing1",
    "text": "Some concepts in multiprocessing1\nProcess\nA forked copy of the current process; this creates a new process identifier, and the task runs as an independent child process in the operating system\nPool\nWraps the Process into a convenient pool of workers that share a chunk of work and return an aggregated result\nGorelick & Ozsvald, 2020. High Performance Python, O’Reilly"
  },
  {
    "objectID": "slides/03-slides.html#other-methods-of-parallel-processing-in-python",
    "href": "slides/03-slides.html#other-methods-of-parallel-processing-in-python",
    "title": "Lecture 3",
    "section": "Other methods of parallel processing in Python",
    "text": "Other methods of parallel processing in Python\n\nThe joblib module\nMost scikit-learn functions have implicit parallelization baked in through the n_jobs parameter\n\nFor example\nfrom sklearn.ensembles import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators = 100, random_state = 124, n_jobs=-1)\nuses the joblib module to use all available processors (n_jobs=-1) to do the bootstrapping\n\n\n\n\n\nSee here for a description of parallel processing in the scikit-learn module"
  },
  {
    "objectID": "slides/03-slides.html#the-need-for-async-io",
    "href": "slides/03-slides.html#the-need-for-async-io",
    "title": "Lecture 3",
    "section": "The need for Async I/O",
    "text": "The need for Async I/O\n\n\n\n\nWhen talking to external systems (databases, APIs) the bottleneck is not local CPU/memory but rather the time it takes to receive a response from the external system.\nThe Async I/O model addresses this by allowing to send multiple request in parallel without having to wait for a response.\nReferences: asyncio — Asynchronous I/O, Async IO in Python: A Complete Walkthrough\n\n\n\n\n\n\nAsync I/O"
  },
  {
    "objectID": "slides/03-slides.html#concurrency-and-parallelism-in-python3",
    "href": "slides/03-slides.html#concurrency-and-parallelism-in-python3",
    "title": "Lecture 3",
    "section": "Concurrency and parallelism in Python3",
    "text": "Concurrency and parallelism in Python3\n\n\n\n\nParallelism: multiple tasks are running in parallel, each on a different processors. This is done through the multiprocessing module.\nConcurrency: multiple tasks are taking turns to run on the same processor. Another task can be scheduled while the current one is blocked on I/O.\nThreading: multiple threads take turns executing tasks. One process can contain multiple threads. Similar to concurrency but within the context of a single process.\n\n\n\n\n\n\nConcurrency in Python"
  },
  {
    "objectID": "slides/03-slides.html#how-to-implement-concurrency-with-asyncio",
    "href": "slides/03-slides.html#how-to-implement-concurrency-with-asyncio",
    "title": "Lecture 3",
    "section": "How to implement concurrency with asyncio",
    "text": "How to implement concurrency with asyncio\n\n\n\n\nasyncio is a library to write concurrent code using the async/await syntax.\nUse of async and await keywords. You can call an async function multiple times while you await the result of a previous invocation.\nawait the result of multiple async tasks using gather.\nThe main function in this example is called a coroutine. Multiple coroutines can be run concurrnetly as awaitable tasks.\n\n\n\nimport asyncio\n\nasync def main():\n    print('Hello ...')\n    await asyncio.sleep(1)\n    print('... World!')\n\nasyncio.run(main())"
  },
  {
    "objectID": "slides/03-slides.html#anatomy-of-an-asyncio-python-program",
    "href": "slides/03-slides.html#anatomy-of-an-asyncio-python-program",
    "title": "Lecture 3",
    "section": "Anatomy of an asyncio Python program",
    "text": "Anatomy of an asyncio Python program\n\n\n\n\nWrite a regular Python function that makes a call to a database, an API or any other blocking functionality as you normally would.\nCreate a coroutine i.e. an async wrapper function to the blocking function using async and await, with the call to blocking function made using the asyncio.to_thread function. This enables the coroutine execution in a separate thread.\nCreate another coroutine that makes multiple calls (in a loop, list comprehension) to the async wrapper created in the previous step and awaits completion of all of the invocations using the asyncio.gather function.\nCall the coroutine created in the previous step from another function using the asyncio.run function.\n\n\n\nimport time\nimport asyncio\n\ndef my_blocking_func(i):\n    # some blocking code such as an api call\n    print(f\"{i}, entry\")\n    time.sleep(1)\n    print(f\"{i}, exiting\")\n    return None\n  \nasync def async_my_blocking_func(i: int):\n    return await asyncio.to_thread(my_blocking_func, i)\n\nasync def async_my_blocking_func_for_multiple(n: int):\n    return await asyncio.gather(*[async_my_blocking_func(i) for i in range(n)])\n\nif __name__ == \"__main__\":\n    # async version\n    s = time.perf_counter()\n    n = 20\n    brewery_counts = asyncio.run(async_my_blocking_func_for_multiple(n))\n    elapsed_async = time.perf_counter() - s\n    print(f\"{__file__}, async_my_blocking_func_for_multiple finished in {elapsed_async:0.2f} seconds\")"
  },
  {
<<<<<<< HEAD
    "objectID": "slides/04-slides.html#agenda-and-goals-for-today",
    "href": "slides/04-slides.html#agenda-and-goals-for-today",
    "title": "Lecture 4",
    "section": "Agenda and Goals for Today",
    "text": "Agenda and Goals for Today\n\n\nLecture\n\nDocker containers\n\nBackground and concepts\nSyntax and commands\nExamples\n\nLambda/microservices\n\nBackground and concepts\nSyntax and commands\nExamples\n\n\n\nLab\n\nLambda service exploration\nCloud9 Setup\nDocker Container exploration in Cloud9"
  },
  {
    "objectID": "slides/04-slides.html#logistics-and-review",
    "href": "slides/04-slides.html#logistics-and-review",
    "title": "Lecture 4",
    "section": "Logistics and Review",
    "text": "Logistics and Review\n\n\nDeadlines\n\nAssignment 1: Python Skills Due Sept 5 11:59pm\nLab 2: Cloud Tooling Due Sept 5 6pm\nAssignment 2: Shell & Linux Due Sept 11 11:59pm\nLab 3: Parallel Computing Due Sept 12 6pm\nAssignment 3: Parallelization Due Sept 18 11:59pm\nLab 4: Docker and Lambda Due Sept 19 6pm\nAssignment 4: Containers Due Sept 25 11:59pm\nLab 5: DuckDB & Polars Due Sept 26 6pm\n\n\nLook back and ahead\n\nContinue to use Slack for questions!\nParallelization in Python\nConcepts of map and reduce\nChoosing when to re-factor/optimize code for scalability\nQuestions on Assignment 3: Parallelization?"
  },
  {
    "objectID": "slides/04-slides.html#process-isolation-and-deployment",
    "href": "slides/04-slides.html#process-isolation-and-deployment",
    "title": "Lecture 4",
    "section": "Process Isolation and Deployment",
    "text": "Process Isolation and Deployment\n\nEncapsulate all the dependencies of your code\nDeploy your code into production in the same environment as your development space\nRun your code without other processes affecting your work\nYou need virtualization!"
  },
  {
    "objectID": "slides/04-slides.html#motivation",
    "href": "slides/04-slides.html#motivation",
    "title": "Lecture 4",
    "section": "Motivation",
    "text": "Motivation\nWhy should you care?"
  },
  {
    "objectID": "slides/04-slides.html#history-of-virtualization",
    "href": "slides/04-slides.html#history-of-virtualization",
    "title": "Lecture 4",
    "section": "History of Virtualization",
    "text": "History of Virtualization\n\nTwo types of virtualization - hardware and software\nHardware time sharing started in the late 1950s\nVirtualization first introduced in IBM in the 1960s\nVMWare started in\n\n\nlink"
  },
  {
    "objectID": "slides/04-slides.html#virtual-machines",
    "href": "slides/04-slides.html#virtual-machines",
    "title": "Lecture 4",
    "section": "Virtual Machines",
    "text": "Virtual Machines\n\n\n\nHardware server is the host\nMultiple virtual machines can sit on the hypervisor as “guests”\nPros: decades of development in VMs, multiple emulated full computers on single hardware, allows for infrastructure “lift and shift” to the cloud\nCons: VM images very large (10-100s GBs), “guest” OS takes up resources, large start up cost\n\nlink"
  },
  {
    "objectID": "slides/04-slides.html#containers-1",
    "href": "slides/04-slides.html#containers-1",
    "title": "Lecture 4",
    "section": "Containers",
    "text": "Containers\n\n\n\nPackage and run applications in isolated environment\nGreat for production work, not as much for developmental work\nDocker application manages “guest” applications with a single operating system\nPros: Lightweight (10-100s MBs), faster spin up time, one host can manage many more containers\nCons: containers must all work with the same OS, somewhat less secure due to shared OS resources, only 10 years old\n\nlink"
  },
  {
    "objectID": "slides/04-slides.html#docker-vs.-virtual-machine",
    "href": "slides/04-slides.html#docker-vs.-virtual-machine",
    "title": "Lecture 4",
    "section": "Docker vs. Virtual Machine",
    "text": "Docker vs. Virtual Machine"
  },
  {
    "objectID": "slides/04-slides.html#docker-container-architecture",
    "href": "slides/04-slides.html#docker-container-architecture",
    "title": "Lecture 4",
    "section": "Docker Container Architecture",
    "text": "Docker Container Architecture\n\nImages are a snapshot of instructions for setup of an application environment\nContainers are executing applications deployed based on an image\nContainer images become containers at runtime (Docker Engine!)"
  },
  {
    "objectID": "slides/04-slides.html#containers-are-not-enough",
    "href": "slides/04-slides.html#containers-are-not-enough",
    "title": "Lecture 4",
    "section": "Containers are not enough",
    "text": "Containers are not enough"
  },
  {
    "objectID": "slides/04-slides.html#container-lifecycle",
    "href": "slides/04-slides.html#container-lifecycle",
    "title": "Lecture 4",
    "section": "Container lifecycle",
    "text": "Container lifecycle\n\nContainers (like applications) need a lifecycle"
  },
  {
    "objectID": "slides/04-slides.html#container-orchestration",
    "href": "slides/04-slides.html#container-orchestration",
    "title": "Lecture 4",
    "section": "Container orchestration",
    "text": "Container orchestration"
  },
  {
    "objectID": "slides/04-slides.html#docker-layers",
    "href": "slides/04-slides.html#docker-layers",
    "title": "Lecture 4",
    "section": "Docker layers",
    "text": "Docker layers\nEach command to modify the environment is a layer in your build process"
  },
  {
    "objectID": "slides/04-slides.html#docker-base-images",
    "href": "slides/04-slides.html#docker-base-images",
    "title": "Lecture 4",
    "section": "Docker base images",
    "text": "Docker base images\n\nThe first command in a Docker build is to call FROM XXXXX. This is the base image that has the core OS-level software.\nsmaller size == faster execution"
  },
  {
    "objectID": "slides/04-slides.html#the-storage-problem-of-containers",
    "href": "slides/04-slides.html#the-storage-problem-of-containers",
    "title": "Lecture 4",
    "section": "The storage problem of containers",
    "text": "The storage problem of containers\n\nWhen containers are killed, all temporary is lost! What to do…\nContainers need persistent storage sometimes\nDocker volumes live outside the container and allow for permanent storage"
  },
  {
    "objectID": "slides/04-slides.html#dockerfile-syntax",
    "href": "slides/04-slides.html#dockerfile-syntax",
    "title": "Lecture 4",
    "section": "Dockerfile Syntax",
    "text": "Dockerfile Syntax\nStudents explain what each command is doing\n\nFormat is always INSTRUCTION arguments\nFROM [--platform=<platform>] <image>[:<tag>] [AS <name>] - Pulling the base image to start with\nRUN <command> or RUN [\"executable\", \"param1\", \"param2\"] - Run commands of any type\nVOLUME - Creates mountpoint for Docker volume\nENV <key>=<value> ... - Set environment variables for your container\nWORKDIR - Sets working directory for other commands\nEXPOSE <port> [<port>/<protocol>...] - Open a port for the container\nCOPY [--chown=<user>:<group>] [--chmod=<perms>] <src>... <dest> - Copies files from host file system to container file system. Note working directory matters in the container too!\nADD - Copy with a link to allow for reusing of already built layers. Helps with build speed!"
  },
  {
    "objectID": "slides/04-slides.html#dockerfile-cmd-and-entrypoint",
    "href": "slides/04-slides.html#dockerfile-cmd-and-entrypoint",
    "title": "Lecture 4",
    "section": "Dockerfile CMD and ENTRYPOINT",
    "text": "Dockerfile CMD and ENTRYPOINT\n\nShould have a purpose for your container and have it run something!\nCMD command param1 param2 - Final command to prepare the launching container. Typically occurs before an ENTRYPOINT instruction or as its own standalone command. CMD can be overwritten by new docker run arguments.\nENTRYPOINT command param1 param2 - Command that will run the Docker container as an executable, such as a web server, Jupter environment, or data pipeline."
  },
  {
    "objectID": "slides/04-slides.html#dockerfile-syntax-examples",
    "href": "slides/04-slides.html#dockerfile-syntax-examples",
    "title": "Lecture 4",
    "section": "Dockerfile Syntax Examples",
    "text": "Dockerfile Syntax Examples\n\nRUN [\"/bin/bash\", \"-c\", \"echo hello\"]\n\nFROM busybox\nENV FOO=/bar\nWORKDIR ${FOO}   # WORKDIR /bar\nADD . $FOO       # ADD . /bar\nCOPY \\$FOO /quux # COPY $FOO /quu\n# syntax=docker/dockerfile:1\nFROM python:3.6\nADD mypackage.tgz wheels/\nRUN --network=none pip install --find-links wheels mypackage\nDockerfile reference link"
  },
  {
    "objectID": "slides/04-slides.html#s3-example",
    "href": "slides/04-slides.html#s3-example",
    "title": "Lecture 4",
    "section": "S3 example",
    "text": "S3 example\n# syntax=docker/dockerfile:1\nFROM python:3\nRUN pip install awscli\nRUN --mount=type=secret,id=aws,target=/root/.aws/credentials aws s3 cp s3://... ..."
  },
  {
    "objectID": "slides/04-slides.html#web-server-example",
    "href": "slides/04-slides.html#web-server-example",
    "title": "Lecture 4",
    "section": "Web server example",
    "text": "Web server example\nFROM debian:stable\nRUN apt-get update && apt-get install -y --force-yes apache2\nEXPOSE 80 443\nVOLUME [\"/var/www\", \"/var/log/apache2\", \"/etc/apache2\"]\nENTRYPOINT [\"/usr/sbin/apache2ctl\", \"-D\", \"FOREGROUND\"]"
  },
  {
    "objectID": "slides/04-slides.html#docker-for-python-jobs",
    "href": "slides/04-slides.html#docker-for-python-jobs",
    "title": "Lecture 4",
    "section": "Docker for Python jobs",
    "text": "Docker for Python jobs\nBasic needs:\n\nDockerfile that sets up environment\nApplication code to execute\nList of packages to install\n\nWalkthrough"
  },
  {
    "objectID": "slides/04-slides.html#docker-python-example",
    "href": "slides/04-slides.html#docker-python-example",
    "title": "Lecture 4",
    "section": "Docker Python example",
    "text": "Docker Python example\n\nWrite your Dockerfile\n\nFROM python:3.8-slim-buster\nWORKDIR /app\nCOPY requirements.txt requirements.txt\nRUN pip3 install -r requirements.txt\nCOPY . .\nCMD [ \"python3\", \"-m\" , \"flask\", \"run\", \"--host=0.0.0.0\"]\n\nBuild the docker image using a tag and version number docker build ./ -t docker-tag-name:1.0. Read about docker building options here.\nCheck your docker images in your local file system using docker images\nTest your docker image using docker run -it docker-tag-name. Read about the flags here. The i is to keep STDIN open, the t is for pseudo-TTY mode (shell).\nDeploy your docker image somewhere!"
  },
  {
    "objectID": "slides/04-slides.html#docker-links",
    "href": "slides/04-slides.html#docker-links",
    "title": "Lecture 4",
    "section": "Docker links",
    "text": "Docker links\nAWS examples\nArticle discussing performance considerations\nWhat are containers?"
  },
  {
    "objectID": "slides/04-slides.html#overview",
    "href": "slides/04-slides.html#overview",
    "title": "Lecture 4",
    "section": "Overview",
    "text": "Overview\n\nLambda (or \\(\\lambda\\)) is the name given to anonymous functions in some languages like Python.\nLambda is a powerful tool to build serverless microfunctions in a variety of programming languages that can be triggered in a variety of ways.\nThe execution of a Lambda can scale to meet the burst needs of users going to a website, or the number of rows of incoming data.\nThere can be thousands of executions happening simultaneously.\nLambda is billed by the millisecond (ms).\n\n\n\nThe name Lambda is derived from Lambda calculus, a mathematical system to express computation introduced by Dr. Alonzo Church in the 1930s. See wikipedia entry on Lambda calculus for why the symbol \\(\\lambda\\) was choosen."
  },
  {
    "objectID": "slides/04-slides.html#lambda-details",
    "href": "slides/04-slides.html#lambda-details",
    "title": "Lecture 4",
    "section": "Lambda Details",
    "text": "Lambda Details\n\n\nMicro executions without dealing with any hardware, “serverless” computation.\nCan run many copies of code depending on number of requests or files.\nSupported languages: Java, Go, PowerShell, Node.js, C#, Python, and Ruby.\n\nProvides a Runtime API which allows you to use any additional programming languages to author your functions\n\nPricing based on RAM and time used - Lambda pricing\n\nMax execution time - 15 minutes\nMax RAM - 10 GB"
  },
  {
    "objectID": "slides/04-slides.html#lambda-triggers",
    "href": "slides/04-slides.html#lambda-triggers",
    "title": "Lecture 4",
    "section": "Lambda triggers",
    "text": "Lambda triggers\n\n\n\nLambdas only start executing in response to an event, such as an API being invoked, a schedule or more complex things.\nA Lambda function is used primarily in the following two ways:\n\nHosting an API, this is done in conjunction with an Amazon API Gateway.\nRunning custom code to respond to events such as a new file showing up in an S3 bucket, a row being deleted from a database etc.\n\n\n\n\n\n\nLambda triggers"
  },
  {
    "objectID": "slides/04-slides.html#different-ways-to-invoke-lambda",
    "href": "slides/04-slides.html#different-ways-to-invoke-lambda",
    "title": "Lecture 4",
    "section": "Different ways to invoke Lambda",
    "text": "Different ways to invoke Lambda\n Source: Understanding the different ways to invoke Lambda function"
  },
  {
    "objectID": "slides/04-slides.html#some-real-world-use-cases-for-a-lambda",
    "href": "slides/04-slides.html#some-real-world-use-cases-for-a-lambda",
    "title": "Lecture 4",
    "section": "Some real world use-cases for a Lambda",
    "text": "Some real world use-cases for a Lambda\n\nTransforming a picture taken by a delivery agent into a desired format and storing it in S3 in real-time.\nAs a front-end (along with an API Gateway) to a ML model to provide real-time prediction to a customer clicking a link on a website .\nA backend API that gets invoked when a QR code is scanned.\nA simple single page website with a form.\nSending a scheduled newsletter to email addresses on a mailing list.\n\nAWS Lambda customer case studies"
  },
  {
    "objectID": "slides/04-slides.html#lambda-can-be-set-on-a-schedule",
    "href": "slides/04-slides.html#lambda-can-be-set-on-a-schedule",
    "title": "Lecture 4",
    "section": "Lambda can be set on a schedule",
    "text": "Lambda can be set on a schedule\n\nSimilar to Linux Crontab or Windows Task Scheduler\n\nexample of schedule trigger\n\nSite for translating crontab symbols to next execution: https://crontab.guru/\nSite for generating crontab syntax from dropdown options: https://crontab-generator.org/"
  },
  {
    "objectID": "slides/04-slides.html#lambda-coding-choices",
    "href": "slides/04-slides.html#lambda-coding-choices",
    "title": "Lecture 4",
    "section": "Lambda coding choices",
    "text": "Lambda coding choices\nLevels of Lambda complexity:\n\nCode written directly on the AWS Lambda console\nZip archive\nDocker Containers"
  },
  {
    "objectID": "slides/04-slides.html#lambda-basic-code-demo",
    "href": "slides/04-slides.html#lambda-basic-code-demo",
    "title": "Lecture 4",
    "section": "Lambda basic code demo",
    "text": "Lambda basic code demo\n\nBeyond any simple prototyping, you would probably never use this method.\nThe “hello world” example and then numbers with an endpoint\n\nhttps://frankcorso.dev/aws-lambda-function-endpoint-api-gateway.html"
  },
  {
    "objectID": "slides/04-slides.html#using-a-zip-archive-for-lambda",
    "href": "slides/04-slides.html#using-a-zip-archive-for-lambda",
    "title": "Lecture 4",
    "section": "Using a Zip archive for Lambda",
    "text": "Using a Zip archive for Lambda\n\nThe Lambda code (say the index.py file) and all the dependencies i.e. Python packages used by the code are packaged together in a Zip file, typically called function.zip.\n\nBe careful to always install these packages in a special directory specifically created for the Lambda function so to not override anything currently installed in your existing Python installation.\n\nThe function.zip file is put in an S3 bucket and the Lambda is configured to pull it from there.\nExample script for building a Lambda function.zip and uploading it to S3.\n\nAWS example of zip archive"
  },
  {
    "objectID": "slides/04-slides.html#lambda-with-docker-containers",
    "href": "slides/04-slides.html#lambda-with-docker-containers",
    "title": "Lecture 4",
    "section": "Lambda with Docker containers",
    "text": "Lambda with Docker containers\nAWS example Walkthrough post"
  },
  {
    "objectID": "slides/04-slides.html#dockerfile",
    "href": "slides/04-slides.html#dockerfile",
    "title": "Lecture 4",
    "section": "Dockerfile",
    "text": "Dockerfile\n# syntax=docker/dockerfile:1\n\n# adapted from https://www.philschmid.de/aws-lambda-with-custom-docker-image\n# https://docs.aws.amazon.com/lambda/latest/dg/python-image.html\nFROM public.ecr.aws/lambda/python:3.9\n\n# copy requirements file and install necessary packages\nADD requirements.txt ${LAMBDA_TASK_ROOT}\nRUN pip3 install -r ${LAMBDA_TASK_ROOT}/requirements.txt --target \"${LAMBDA_TASK_ROOT}\"\n\n# Copy function code to docker container\nCOPY app.py ${LAMBDA_TASK_ROOT}\n\n# app (name of py file)\n# handler (name of function to execute for lambda job)\nCMD [ \"app.lambda_handler\" ]"
  },
  {
    "objectID": "slides/04-slides.html#interactivity-with-lambda",
    "href": "slides/04-slides.html#interactivity-with-lambda",
    "title": "Lecture 4",
    "section": "Interactivity with Lambda?",
    "text": "Interactivity with Lambda?\nIf you “hack” it!"
  },
  {
    "objectID": "slides/04-slides.html#connecting-multiple-lambdas-together",
    "href": "slides/04-slides.html#connecting-multiple-lambdas-together",
    "title": "Lecture 4",
    "section": "Connecting multiple Lambdas together",
    "text": "Connecting multiple Lambdas together\nConnecting multiple lambda using AWS Step Functions"
  },
  {
    "objectID": "slides/04-slides.html#performance-optimization",
    "href": "slides/04-slides.html#performance-optimization",
    "title": "Lecture 4",
    "section": "Performance optimization",
    "text": "Performance optimization\nwarm start vs cold start\nDiscussion post on AWS"
  },
  {
    "objectID": "slides/04-slides.html#extra-links",
    "href": "slides/04-slides.html#extra-links",
    "title": "Lecture 4",
    "section": "Extra links",
    "text": "Extra links\nhttps://www.tatvasoft.com/blog/aws-lambda-vs-azure-functions/\nserverless web scraper with python and lambda - up to date simple example https://towardsdatascience.com/serverless-covid-19-data-scraper-with-python-and-aws-lambda-d6789a551b78"
=======
    "objectID": "slides/04-slides.html#section",
    "href": "slides/04-slides.html#section",
    "title": "Lecture 4",
    "section": "",
    "text": "Looking back\n\nLinux and version control\nPython and parallelization\nIntro to the Cloud\nSetup of AWS Services\n\n\n\nToday\n\nIntro to Hadoop and MapReduce\nHadoop Streaming\nLab/Demo (depending on time):\n\nStart a cluster on AWS\nRun a sample Hadoop job\nRun a simluated mapreduce job using the command line\nRun a Hadoop Streaming job"
  },
  {
    "objectID": "slides/04-slides.html#yesterdays-hardware-for-big-data-processing",
    "href": "slides/04-slides.html#yesterdays-hardware-for-big-data-processing",
    "title": "Lecture 4",
    "section": "Yesterday’s hardware for big data processing",
    "text": "Yesterday’s hardware for big data processing\n\nThe 1990’s solution\nOne big box, all processors share memory\nThis was:\n\nVery expensive\nLow volume\n\nIt was all premium hardware. And yet is still was not big enough!"
  },
  {
    "objectID": "slides/04-slides.html#enter-commodity-hardware",
    "href": "slides/04-slides.html#enter-commodity-hardware",
    "title": "Lecture 4",
    "section": "Enter commodity hardware!",
    "text": "Enter commodity hardware!\n\nConsumer-grade hardware\nNot expensive, premium nor fancy in any way\nDesktop-like servers are cheap, so buy a lot!\n\nEasy to add capacity\nCheaper per CPU/disk\n\nBut\nNeed more complex software to be able to run on lots of smaller/cheaper machines."
  },
  {
    "objectID": "slides/04-slides.html#problems-with-cheap-hardware",
    "href": "slides/04-slides.html#problems-with-cheap-hardware",
    "title": "Lecture 4",
    "section": "Problems with cheap hardware",
    "text": "Problems with cheap hardware\n\nFailures\n\n1-5% hard drives/year\n0.2% DIMMs/year\n\nNetwork speed vs. shared memory\n\nMuch more latency\nNetwork slower than storage\n\nUneven performance"
  },
  {
    "objectID": "slides/04-slides.html#hang-on-to-this-thought",
    "href": "slides/04-slides.html#hang-on-to-this-thought",
    "title": "Lecture 4",
    "section": "Hang on to this thought!",
    "text": "Hang on to this thought!"
  },
  {
    "objectID": "slides/04-slides.html#meet-doug-cutting",
    "href": "slides/04-slides.html#meet-doug-cutting",
    "title": "Lecture 4",
    "section": "Meet Doug Cutting",
    "text": "Meet Doug Cutting\n\n\n\n\n\nIn 1997, Doug Cutting started writing the first version of Lucene (a full text search library).\nIn 2001, Lucene moves to the Apache Software Foundation, and Mike Cafarella joins Doug and create a Lucene subproject called Nutch, a web-crawler. Nutch uses Lucene to index the contents of a web page as it crawls it.\nNutch and Lucene were deployed on a single machine (single core processor, 1GB RAM, 8 HDDs ~ 1TB), achieved decent performance, but they needed something that would be scalable enough to be able to index the web."
  },
  {
    "objectID": "slides/04-slides.html#doug-and-mike-set-out-to-to-improve-nutch",
    "href": "slides/04-slides.html#doug-and-mike-set-out-to-to-improve-nutch",
    "title": "Lecture 4",
    "section": "Doug and Mike set out to to improve Nutch",
    "text": "Doug and Mike set out to to improve Nutch\nThey needed a to build some kind of distributed storage layer to be the foundation of a scalable system. The came up with these requirements:\n\nSchemaless\nDurable\nCapable of handling component failure\nAutomatically rebalanced\n\n\nDoes this sound familiar?"
  },
  {
    "objectID": "slides/04-slides.html#section-1",
    "href": "slides/04-slides.html#section-1",
    "title": "Lecture 4",
    "section": "",
    "text": "The Google File System (GFS) Paper\nIt describes how Google stored its information, at scale, using a reliable and high-available storage system can be built on commodity machines considering that failures are the norm rather than the exception.\nGFS is: * optimized for special application environment * fault tolerance is built in * centralized metadata management * simplify the operation semantics * decouple I/O and metadata operations"
  },
  {
    "objectID": "slides/04-slides.html#section-2",
    "href": "slides/04-slides.html#section-2",
    "title": "Lecture 4",
    "section": "",
    "text": "The Google MapReduce Paper\nDescribes how Google processes data, at scale using MapReduce, a paradigm based on functional programming. MapReduce is an approach and infrastructure for doing things at scale. MapReduce is two things:\n\nA data processing model named MapReduce\nA distributed, large scale data processing paradigm.\n\nWhich provides the following benefits:\n\nMoves the computation to the data\nAutomatic parallelization and distribution\nFault tolerance\nI/O scheduling\nIntegrated status and monitoring"
  },
  {
    "objectID": "slides/04-slides.html#section-3",
    "href": "slides/04-slides.html#section-3",
    "title": "Lecture 4",
    "section": "",
    "text": "The MapReduce model\n\nMap function takes an input pair and produces a set of intermediate key/value pairs. The MapReduce library groups together all intermediate values associated with the same intermediate key and passes them to the Reduce function.\nReduce function accepts an intermediate key and a set of values for that key. It merges together these values to form a possibly smaller set of values. Typically just zero or one output value is produced per Reduce invocation."
  },
  {
    "objectID": "slides/04-slides.html#section-4",
    "href": "slides/04-slides.html#section-4",
    "title": "Lecture 4",
    "section": "",
    "text": "The advantages\n\nThe model is relatively easy to use, even for programmers without experience with parallel and distributed systems since it handles parallelization, fault-tolerance, locality optimization, and load balancing.\nA large variety of problems are easily expressible as MapReduce computations.\nThis paper developed an implementation of MapReduce that scales to large clusters of machines comprising thousands of machines."
  },
  {
    "objectID": "slides/04-slides.html#the-genesis-of-hadoop-an-implementation-of-googles-ideas",
    "href": "slides/04-slides.html#the-genesis-of-hadoop-an-implementation-of-googles-ideas",
    "title": "Lecture 4",
    "section": "The genesis of Hadoop: an implementation of Google’s ideas",
    "text": "The genesis of Hadoop: an implementation of Google’s ideas\n\nUsing the Google papers as a specification, they started implementing the ideas in Java in 2004 and created the Nutch Distributed File System (NDFS).\nThe main purpose of this file system was to abstract the cluster’s storage so that it presents itself as a single, reliable, file system.\nAnother first class featuer of the system was its ability to handle failures without operator interventions and it can run on inexpensive, commodity hardware components\nIn 2006, Hadoop was created by moving NDFS (which becamie HDFS) and the MapReduce implementation out of Lucene. Hadoop 1.0 was released. 1.8TB of data sorts in 188 nodes in 48hours. Doug Cutting goes to work at Yahoo!\nIn 2007, LinkedIn, Twitter and Facebook started adopting this new tool and contributing back to the project. Yahoo! is running their first production Hadoop cluster with 1,000 machines.\nIn 2008, Yahoo! is running a 10,000 core cluster. World record created for fastest sorting of 1Tb in 209 seconds using a 910 node cluster. Cloudera is formed.\nIn 2009, Yahoo!’s cluster is 24,000 cores and claims to sort 1TB in 62 seconds. Amazon introduces Elastic MapReduce. HDFS and MapReduce become their own separate sub-projects.\nIn 2010, Yahoo!’s cluster is 4,000 and ~70PB, Facebook’s is 2,300 nodes and ~40PB.\nIn 2011, Yahoo!’s cluster is 42,000 nodes\nIn 2017, Twitter Hadoop cluster has over 500 Petabytes of data. Biggest cluster over 10k nodes. Read more here"
  },
  {
    "objectID": "slides/04-slides.html#trivia-question-why-is-it-named-hadoop",
    "href": "slides/04-slides.html#trivia-question-why-is-it-named-hadoop",
    "title": "Lecture 4",
    "section": "Trivia question: why is it named Hadoop?",
    "text": "Trivia question: why is it named Hadoop?"
  },
  {
    "objectID": "slides/04-slides.html#tangent-why-open-source-matters",
    "href": "slides/04-slides.html#tangent-why-open-source-matters",
    "title": "Lecture 4",
    "section": "Tangent: Why Open Source Matters!",
    "text": "Tangent: Why Open Source Matters!\nThe Google File System is not popular as a program among the business world… why???\nHadoop has become the standard for cluster storage because it is open source!!\nWho has heard of Colossus? (Google’s next version of GFS)\nReview of GFS vs. Hadoop"
  },
  {
    "objectID": "slides/04-slides.html#what-is-hadoop-1.0-mapreduce-engine-and-hdfs",
    "href": "slides/04-slides.html#what-is-hadoop-1.0-mapreduce-engine-and-hdfs",
    "title": "Lecture 4",
    "section": "What is Hadoop 1.0: MapReduce Engine and HDFS",
    "text": "What is Hadoop 1.0: MapReduce Engine and HDFS\n\nMapReduce performs the computations and manages cluster\n\nThe Job Tracker is the master planner\nThe Task Tracker runs each task\n\nHDFS stores the data"
  },
  {
    "objectID": "slides/04-slides.html#section-5",
    "href": "slides/04-slides.html#section-5",
    "title": "Lecture 4",
    "section": "",
    "text": "Hadoop 1.0 Problems\n\nMonolithic\nMapReduce had too many responsibilites\nAssigning cluster resources\nManaging job execution\nDoing data processing\nInterfacing with client applications\nOnly supported MapReduce\nHad a single NameNode to manage the cluster (single point of failure)\nBatch oriented and extremely inefficient with iterative queries\n\n\n\n\n\n\nYARN & Hadoop 2.0\n\nCluster management capability gets pulled out of MapReduce and becomes YARN (Yet Anothe Resource Negotiator), decoupling cluster operations from data pipeline\nAllowed for other applications to run on a cluster\nHadoop 2.0 was released in 2013"
  },
  {
    "objectID": "slides/04-slides.html#section-6",
    "href": "slides/04-slides.html#section-6",
    "title": "Lecture 4",
    "section": "",
    "text": "A MapReduce Job (1.0)\n\n\n\nA MapReduce Job (2.0+)"
  },
  {
    "objectID": "slides/04-slides.html#how-yarn-manages-the-cluster",
    "href": "slides/04-slides.html#how-yarn-manages-the-cluster",
    "title": "Lecture 4",
    "section": "How YARN manages the Cluster",
    "text": "How YARN manages the Cluster"
  },
  {
    "objectID": "slides/04-slides.html#the-hadoop-architecture-is-scalable",
    "href": "slides/04-slides.html#the-hadoop-architecture-is-scalable",
    "title": "Lecture 4",
    "section": "The Hadoop Architecture is Scalable",
    "text": "The Hadoop Architecture is Scalable"
  },
  {
    "objectID": "slides/04-slides.html#hadoop-3.0-released-2017",
    "href": "slides/04-slides.html#hadoop-3.0-released-2017",
    "title": "Lecture 4",
    "section": "Hadoop 3.0 (Released 2017)",
    "text": "Hadoop 3.0 (Released 2017)\n\nSupports multiple standby NameNodes.\nSupports multiple NameNodes for multiple namespaces.\nStorage overhead reduced from 200% to 50%\n\nSupports GPUs\n\nSupports for Microsoft Azure Data Lake and Aliyun Object Storage System file-system connectors\nRewrite of Hadoop Shell\nMapReduce task Level Native Optimization.\nIntroduces more powerful YARN"
  },
  {
    "objectID": "slides/04-slides.html#section-7",
    "href": "slides/04-slides.html#section-7",
    "title": "Lecture 4",
    "section": "",
    "text": "The Current Hadoop Ecosystem: https://hadoopecosystemtable.github.io/"
  },
  {
    "objectID": "slides/04-slides.html#map",
    "href": "slides/04-slides.html#map",
    "title": "Lecture 4",
    "section": "Map",
    "text": "Map\n\n\n\n\nMap tasks are broken into the following phases:\n\nrecord reader: translates the input split generated by the input format into records or key/value pairs\nmap: function that executes on every input record\ncombiner: acts as a localized reducer to pre-aggregate within the map phase\npartitioner: takes intermediate key/value pairs from the mapper (or combiner if used) and splits them up into shards, one shard per reducer"
  },
  {
    "objectID": "slides/04-slides.html#reduce",
    "href": "slides/04-slides.html#reduce",
    "title": "Lecture 4",
    "section": "Reduce",
    "text": "Reduce\n\n\n\n\nReduce tasks are broken into the following phases:\n\nshuffle and sort: this phase takes the output files written by all of the partitioners and downloads the files to the local node where the reducer code will run\nreducer: function that executes on every group of records for the same key\noutput format: translates the final key/value pair and writes out to a file by a record writer"
  },
  {
    "objectID": "slides/04-slides.html#combining-everything-into-mapreduce",
    "href": "slides/04-slides.html#combining-everything-into-mapreduce",
    "title": "Lecture 4",
    "section": "Combining Everything into MapReduce",
    "text": "Combining Everything into MapReduce"
  },
  {
    "objectID": "slides/04-slides.html#map-only",
    "href": "slides/04-slides.html#map-only",
    "title": "Lecture 4",
    "section": "Map Only",
    "text": "Map Only\n\n\n\n\nExamples of map-only jobs:\n\nFiltering\nData reorganization\nRunning a task in an embarrassingly parallel way"
  },
  {
    "objectID": "slides/04-slides.html#map-and-reduce",
    "href": "slides/04-slides.html#map-and-reduce",
    "title": "Lecture 4",
    "section": "Map and Reduce",
    "text": "Map and Reduce\n\nSingle reducer\n\n\n\nMultiple reducers"
  },
  {
    "objectID": "slides/04-slides.html#section-8",
    "href": "slides/04-slides.html#section-8",
    "title": "Lecture 4",
    "section": "",
    "text": "Benefits of Hadoop/MapReduce\n\nFault tolerance\nMap and Reduce functions are simple to understand and easy to program\n\n\n\nLimitations of Hadoop/MapReduce\n\nEverything has to be expressed in a map or reduce, and sometimes you can’t\nThere is no control over the order in which map or reduce run\nData is not indexed\nHigh overhead\nYou cannot leverage parallelism if you have an extremely large number of very small files (smaller than a single block)\nOnly suited for batch-processing, not real-time"
  },
  {
    "objectID": "slides/04-slides.html#everything-begins-and-ends-in-a-distributed-filesystem",
    "href": "slides/04-slides.html#everything-begins-and-ends-in-a-distributed-filesystem",
    "title": "Lecture 4",
    "section": "Everything begins and ends in a distributed filesystem",
    "text": "Everything begins and ends in a distributed filesystem"
  },
  {
    "objectID": "slides/04-slides.html#distributed-filesystems",
    "href": "slides/04-slides.html#distributed-filesystems",
    "title": "Lecture 4",
    "section": "Distributed filesystems",
    "text": "Distributed filesystems"
  },
  {
    "objectID": "slides/04-slides.html#cloud-object-storage-vs-hdfs",
    "href": "slides/04-slides.html#cloud-object-storage-vs-hdfs",
    "title": "Lecture 4",
    "section": "Cloud Object Storage vs HDFS",
    "text": "Cloud Object Storage vs HDFS\n\nIndependent scalability\nIntegration via simple REST API calls\nLower cost\nNo single point of failure"
  },
  {
    "objectID": "slides/04-slides.html#cloud-based-hadoop-clusters",
    "href": "slides/04-slides.html#cloud-based-hadoop-clusters",
    "title": "Lecture 4",
    "section": "Cloud based Hadoop Clusters",
    "text": "Cloud based Hadoop Clusters\nAmazon Elastic MapReduce (EMR) and Azure HDInsight\n\nOn Premises\n\n\n\nCloud Based (note the storage layer)"
  },
  {
    "objectID": "slides/04-slides.html#accessing-data-in-distributed-objectfile-system",
    "href": "slides/04-slides.html#accessing-data-in-distributed-objectfile-system",
    "title": "Lecture 4",
    "section": "Accessing Data in Distributed Object/File System",
    "text": "Accessing Data in Distributed Object/File System\nFor files in the Cluster HDFS\n\ndirectory/data/ which is a shortcut to\n/user/hadoop/directory/data/\n\nFor files in Amazon S3\nThe service name is S3 and the data container is called a “bucket (bucket_name)” * s3://bucket_name/directory/data/\nFor Files in Azure Blob\nThe service name is Azure Storage Account (your_account) and the data container is called a “container (container)”. There can be many containers in a storage account.\n\nwasb://container@your_account.blob.core.windows.net/directory/data/"
  },
  {
    "objectID": "slides/04-slides.html#hdfs-essential-commands",
    "href": "slides/04-slides.html#hdfs-essential-commands",
    "title": "Lecture 4",
    "section": "HDFS essential commands",
    "text": "HDFS essential commands\nWhat commands have we discussed about the Linux file system that we want for HDFS?\n#| code-line-numbers: false\n# list files\nhdfs dfs -ls <hdfs path>\n\n# make directories\nhdfs dfs -mkdir <folder location>\n\n# touch an empty file\nhdfs dfs -touchz <hdfs path>\n\n# \"put\" file to hdfs\nhdfs dfs -put <other file path> <dest hdfs path>\n\n# \"get\" file from hdfs\nhdfs dfs -get <other file path> <dest hdfs path>"
  },
  {
    "objectID": "slides/04-slides.html#hdfs-essential-commands-1",
    "href": "slides/04-slides.html#hdfs-essential-commands-1",
    "title": "Lecture 4",
    "section": "HDFS essential commands",
    "text": "HDFS essential commands\n# other flag commands, after command and subcommand\nhdfs dfs -cat <hdfs path> #print contents of file\nhdfs dfs -mv <old hdfs path> <new hdfs path> # move file/folder\nhdfs dfs -rm <hdfs path> #only one file\nhdfs dfs -rmr <hdfs path> # recursive flag!\nhdfs dfs -chmod 777 <hdfs path> # set permissions\nhdfs dfs -setfacl -m default:user:sqoop:rwx <hdfs path> # access control list\nReview common commands here\nMore common commands"
  },
  {
    "objectID": "slides/04-slides.html#how-to-programmatically-interact-with-hdfs-from-python",
    "href": "slides/04-slides.html#how-to-programmatically-interact-with-hdfs-from-python",
    "title": "Lecture 4",
    "section": "How to programmatically interact with HDFS from python?",
    "text": "How to programmatically interact with HDFS from python?\n\nUse a python library (hdfs, snakebite, pydoop)\nWhat I do…\nsubprocess to call the real functions!\n\n\nimport subprocess\n\n# local path of the file\nlocal_path = '/home/hadoop/test_dir/myfile.txt'\n\n# can be a new file name or a just folder destination\nhdfs_path = '/user/hadoop/data_folder/'\n\n# construct string command to execute in linux\nlinux_command = f'hdfs dfs -put {local_path} {hdfs_path}'\n\n# run in linux, grab result code and message\nresult_code, result_message = subprocess.getstatusoutput(linux_command)\n\n# raise an error if command did not exit properly\nif result_code != 0:\n  raise TypeError(f'exit code: {result_code}, {result_message}')"
  },
  {
    "objectID": "slides/04-slides.html#writing-mapreduce-applications",
    "href": "slides/04-slides.html#writing-mapreduce-applications",
    "title": "Lecture 4",
    "section": "Writing MapReduce applications",
    "text": "Writing MapReduce applications\nHadoop is written in Java and provides a Java API that allows you to specify the following:\n\nThe input and output data locations in your distributed object/file system\nThe Map and Reduce functions which are providede in the form of Java classes\nMany kinds of job parameters and configurations\n\nOh no! What if I want to use other programming languages?"
  },
  {
    "objectID": "slides/04-slides.html#the-name-hadoop-streaming-comes-from-the-unix-streams",
    "href": "slides/04-slides.html#the-name-hadoop-streaming-comes-from-the-unix-streams",
    "title": "Lecture 4",
    "section": "The name Hadoop Streaming comes from the Unix Streams",
    "text": "The name Hadoop Streaming comes from the Unix Streams"
  },
  {
    "objectID": "slides/04-slides.html#hadoop-streaming-allows-you-to-run-any-executable-script-on-the-hadoop-framework",
    "href": "slides/04-slides.html#hadoop-streaming-allows-you-to-run-any-executable-script-on-the-hadoop-framework",
    "title": "Lecture 4",
    "section": "Hadoop Streaming allows you to run any executable script on the Hadoop Framework",
    "text": "Hadoop Streaming allows you to run any executable script on the Hadoop Framework\nIt uses an existing Java program to run non-java programs!\n\nKeys and values can be single or compound"
  },
  {
    "objectID": "slides/04-slides.html#section-10",
    "href": "slides/04-slides.html#section-10",
    "title": "Lecture 4",
    "section": "",
    "text": "Advantages of Hadoop Streaming\n\nUsing existing code-base (non Java)\nLeveraging Hadoop framework to scale out\n\n\n\nLimitations of Hadoop Streaming\n\nNo combiner or partitioner, just map -> shuffle-sort -> reduce\nEverything is text. Your programs have to parse everything appropriately and convert strings to other data types as needed. You also have manually create the key/value pair for output.\nIf using R or Python, you must limit your programs to the base libraries."
  },
  {
    "objectID": "slides/04-slides.html#basic-python-mapper",
    "href": "slides/04-slides.html#basic-python-mapper",
    "title": "Lecture 4",
    "section": "Basic Python Mapper",
    "text": "Basic Python Mapper\n#!/usr/bin/env python\n\nimport sys\n\nif __name__ == \"__main__\":\n for line in sys.stdin:\n     for word in line.split():\n         sys.stdout.write(\"{}\\t1\\n\".format(word))\n\nThe first line makes this an executable Python script\nRead in line\nSplit line by space into single words\nFor every line: produce an output in the form of word\\t1 where \\t is a tab\nOutput is always a key and value pair!"
  },
  {
    "objectID": "slides/04-slides.html#basic-python-reducer",
    "href": "slides/04-slides.html#basic-python-reducer",
    "title": "Lecture 4",
    "section": "Basic Python Reducer",
    "text": "Basic Python Reducer\n\n\n#!/usr/bin/env python\n\nimport sys\n\ndef stdout(key, val):\n  sys.stdout.write(f\"{key}\\t{val}\\n\")\n\nif __name__ == '__main__':\n curkey = None\n total = 0\n for line in sys.stdin:\n     key, val = line.split(\"\\t\")\n     val = int(val)\n\n     if key == curkey:\n         total += val\n     else:\n         if curkey is not None:\n              stdout(curkey, total)\n\n         curkey = key\n         total = val\n\n stdout(curkey, total)\n\n\n\n\nThe first line makes this an executable Python script\nMake stdout function\nRead in line and stores in memory\nLook at the key from the key/value\nRead next line and compare key\nKeep reading lines and storing lines in memory until the key is different\nWhen new key is received, take all the lines from the previous key and do something and write out result\nRepeat until all lines are processed"
  },
  {
    "objectID": "slides/04-slides.html#section-11",
    "href": "slides/04-slides.html#section-11",
    "title": "Lecture 4",
    "section": "",
    "text": "hadoop jar /usr/lib/hadoop/hadoop-streaming.jar \\\n-files basic-mapper.py,basic-reducer.py \\\n-input [[input-location]] \\\n-output [[output-location]] \\\n-mapper basic-mapper.py \\\n-reducer basic-reducer.py\n\nhadoop jar /usr/lib/hadoop/hadoop-streaming.jar launches Hadoop with the hadoop-streaming.jar\n-files /home/hadoop/hadoop-streaming/basic-mapper.py,/home/hadoop/hadoop-streaming/basic-reducer.py tells the job to “ship” the executable mapper and reducer scripts (the actual filename and absolute path) to every node on the cluster. This line must always be second when you need to ship files with your job.\n-input [[input-location]] tells the Hadoop the location of your source data in a distributed object/filesystem. If you specify a directory, all files in the directory will be used as inputs\n-output [[output-location]] tells the Hadoop the location of your output data in a distributed object/filesystem. This parameter is just a name of a location, and it must not exist before running the job otherwise the job will fail.\n-mapper basic-mapper.py and reducer basic-reducer.py specify the commands to run as a mapper and reducer, respectively. These must be shell scripts or native Linux commands. If you are using a script, you must “ship” it to the nodes with the -files parameter above."
  },
  {
    "objectID": "slides/04-slides.html#gotchas",
    "href": "slides/04-slides.html#gotchas",
    "title": "Lecture 4",
    "section": "Gotcha’s",
    "text": "Gotcha’s\n\nThe output location of the job (HFDF, S3, Blob) must not exist before running a job. You will get an error if you try to write out to an existing/non-empty location\nYou have to ship or package your executables with the job.\nHadoop/Java error messages can be cryptic and only give you errors regarding the framework. If you need to debug your script you will need to look at the stdout/stderr logs\nYou need to specify the full path name of your mapper and reducer scripts"
  },
  {
    "objectID": "slides/04-slides.html#example-code",
    "href": "slides/04-slides.html#example-code",
    "title": "Lecture 4",
    "section": "Example code",
    "text": "Example code\nhadoop jar /usr/lib/hadoop/hadoop-streaming.jar \\\n-files basic-mapper.py,basic-reducer.py \\\n-input /user/hadoop/in_data \\\n-output /user/hadoop/in_data \\\n-mapper basic-mapper.py \\\n-reducer basic-reducer.py"
  },
  {
    "objectID": "slides/04-slides.html#hadoop-references",
    "href": "slides/04-slides.html#hadoop-references",
    "title": "Lecture 4",
    "section": "Hadoop References",
    "text": "Hadoop References\n\nHadoop 3.3.1: https://hadoop.apache.org/docs/r3.3.1/\n\nHDFS Filesystem Commands: https://hadoop.apache.org/docs/r3.3.1/hadoop-project-dist/hadoop-common/FileSystemShell.html\n\nHadoop Streaming https://hadoop.apache.org/docs/r3.3.1/hadoop-streaming/HadoopStreaming.html"
>>>>>>> 5cfd7cc16e257c0a05ca9589ee545fcbfa2d7d0d
  },
  {
    "objectID": "slides/05-slides.html#takeaway",
    "href": "slides/05-slides.html#takeaway",
    "title": "Lecture 5",
    "section": "Takeaway",
    "text": "Takeaway\n\n\nDask provides an easy to use Python interface for data science and machine learning workloads. If you are familiar with Pandas and Numpy you can become productive with Dask very quickly. Dask is used by individual researchers as well as institutions across a broad set of domains.\n\n\n\nMatthew Rocklin (2015). Dask: Parallel Computation with Blocked algorithms and Task Scheduling. https://conference.scipy.org/proceedings/scipy2015/pdfs/matthew_rocklin.pdf"
  },
  {
    "objectID": "slides/05-slides.html#what-is-dask",
    "href": "slides/05-slides.html#what-is-dask",
    "title": "Lecture 5",
    "section": "What is Dask?",
    "text": "What is Dask?\n \n\nDask is an open-source flexible library for parallel computing in Python."
  },
  {
    "objectID": "slides/05-slides.html#ok-but-what-is-it-really",
    "href": "slides/05-slides.html#ok-but-what-is-it-really",
    "title": "Lecture 5",
    "section": "Ok, but what is it really…",
    "text": "Ok, but what is it really…\n\nBlocked Algorithms1: Take a very large dataset that cannot fit into memory and break it into many many smaller parts each of which can be computed upon independantly, combine the results.\nDirected Acyclic Graphs (DAGs): Create a graph to represent this sequence of operations.\nDistributed Compute: Compute the results by executing this graph on a compute platform (multi-core CPU, multiple multi-core CPUs).\n\nWhat is a blocked algorithm"
  },
  {
    "objectID": "slides/05-slides.html#as-a-programmer-what-do-i-need-to-know",
    "href": "slides/05-slides.html#as-a-programmer-what-do-i-need-to-know",
    "title": "Lecture 5",
    "section": "As a programmer what do I need to know?",
    "text": "As a programmer what do I need to know?\nDask is composed of two parts: Dynamic Task Scheduling and Big Data Collections (such as parallel arrays, dataframes and lists). More details on the Dask Website."
  },
  {
    "objectID": "slides/05-slides.html#talk-is-cheap.-show-me-the-code.",
    "href": "slides/05-slides.html#talk-is-cheap.-show-me-the-code.",
    "title": "Lecture 5",
    "section": "Talk is cheap. Show me the code.",
    "text": "Talk is cheap. Show me the code.\nData: All of the NYC yellow taxi trip data files from 2021. \n\n\nimport time\nfrom dask import dataframe as dd\n\n# Dask is able to read multiple files without us having to write a loop\ndf = dd.read_parquet(\"s3://bigdatateaching/nyctaxi-yellow-tripdata/2021/yellow_tripdata_2021-*.parquet\")\n\n# Lets take dask for a spin, how about finding the average of\n# a few columns of interest: trip_distance, passenger_count, total_amount\nstart = time.perf_counter()\nresults = df[['trip_distance', 'total_amount']].mean().compute().to_frame().reset_index()\nend = time.perf_counter()\n\n# set column names for the resulting dataframe\nresults.columns = [\"feature\", \"mean\"]"
  },
  {
    "objectID": "slides/05-slides.html#millions-of-rows-processed-in-a-few-seconds",
    "href": "slides/05-slides.html#millions-of-rows-processed-in-a-few-seconds",
    "title": "Lecture 5",
    "section": "Millions of rows processed in a few seconds",
    "text": "Millions of rows processed in a few seconds\nData that would not fit into the 16GB RAM of my laptop all at one time is easily analyzed and in quick time too!"
  },
  {
    "objectID": "slides/05-slides.html#dask-dataframes",
    "href": "slides/05-slides.html#dask-dataframes",
    "title": "Lecture 5",
    "section": "Dask DataFrames",
    "text": "Dask DataFrames\n\n\n\nThink of Dask DataFrames as collection of multiple Pandas dataframes.\nEach Pandas dataframe contains rows that are grouped together based on an index.\nOperations on a Dask DataFrame become faster because they happen in parallel across multiple partitions.\nThe individual Pandas dataframes may live on disk or on other machines.\n\n\n\n\n\nDask DataFrame"
  },
  {
    "objectID": "slides/05-slides.html#dask-dataframes-contd.",
    "href": "slides/05-slides.html#dask-dataframes-contd.",
    "title": "Lecture 5",
    "section": "Dask DataFrames (contd.)",
    "text": "Dask DataFrames (contd.)\n\nProvide a subset of the Pandas API.\nFollowing operations are fast\n\nElement-wise operations: df.x + df.y, df * df\nRow-wise selections: df[df.x > 0]\ngroupby-aggregate: df.groupby('x').min()\nJoin with pandas DataFrames: dd.merge(df1, df2, on='id')\nRolling averages: df.rolling(…)\nPearson’s correlation: df[[‘col1’, ‘col2’]].corr()\n\nOfficial Reference, Code Samples"
  },
  {
    "objectID": "slides/05-slides.html#dask-dataframe-contd.",
    "href": "slides/05-slides.html#dask-dataframe-contd.",
    "title": "Lecture 5",
    "section": "Dask DataFrame (contd.)",
    "text": "Dask DataFrame (contd.)\n\n\nQuestion: Since Dask DataFrame operations are parallel, does it mean Dask is always faster than Pandas?\nAnswer: Actually, no. Dask is used when we operate on data that does not fit into memory and needs to be distributed (across cores, on the same machine, better still across multiple machines).\n\n\nPandas is highly optimized for performance, with critical code paths written in Cython or C. More here.\n\n\nOther Dask collections: Array, Bag and also Delayed and Futures."
  },
  {
    "objectID": "slides/05-slides.html#dask-dataframes-when-not-to-use",
    "href": "slides/05-slides.html#dask-dataframes-when-not-to-use",
    "title": "Lecture 5",
    "section": "Dask DataFrames, when not to use",
    "text": "Dask DataFrames, when not to use\n\nSetting a new index from an unsorted column.\nOperations like groupby-apply and join on unsorted columns.\nOperations that are slow in Pandas, such as iterating row-by-row, remain slow on a Dask DataFrame."
  },
  {
    "objectID": "slides/05-slides.html#moving-to-a-dask-cluster",
    "href": "slides/05-slides.html#moving-to-a-dask-cluster",
    "title": "Lecture 5",
    "section": "Moving to a Dask Cluster",
    "text": "Moving to a Dask Cluster\n\nTo truly see the power of Dask and horizontal scaling in action, we need to move out of our local machine and onto a cluster.\nCreating a Dask cluster is a lot of work involving networking, security and compute. Unlike an EMR cluster, there is no GUI for creating a Dask cluster.\nSeveral options exist, including something called Deployment as a service i.e. creating a cluster by calling an API, not a cloud provider API but an APIU provided by a SaaS which in turn does all the behind the scenes work of creating the cluster by calling (many) cloud provider APIs. See Coiled."
  },
  {
    "objectID": "slides/05-slides.html#moving-to-a-dask-cluster-contd.",
    "href": "slides/05-slides.html#moving-to-a-dask-cluster-contd.",
    "title": "Lecture 5",
    "section": "Moving to a Dask Cluster (contd.)",
    "text": "Moving to a Dask Cluster (contd.)\n\nThere are open source solutions also which provide a cloud agnostic way of deploying Dask clusters. No matter whether you are on AWS, Azure or GCP, you call the same API. See Dask Cloud Provider.\nCloud providers such as AWS provide Infrastructure as Code templates to create Dask Clusters. These template (yaml or json files) are declarative i.e. they describe what infrastructure you want to create and the Infrastructure as Code service takes care of translating these templates into API calls that actually create these resources."
  },
  {
    "objectID": "slides/05-slides.html#moving-to-a-dask-cluster-contd.-1",
    "href": "slides/05-slides.html#moving-to-a-dask-cluster-contd.-1",
    "title": "Lecture 5",
    "section": "Moving to a Dask Cluster (contd.)",
    "text": "Moving to a Dask Cluster (contd.)\n\nSimilar to EMR, Dask clusters have the concept of a Scheduler and a collection of Workers.\nThe compute for a Dask cluster can be provided by a collection of EC2 VMs like we saw with EMR.\nIt could also be provided by a serverless compute engine such as AWS Fargate where we do not create or manage the server (hence serverless) but only specify the container image to run and the memory and CPU requirements."
  },
  {
    "objectID": "slides/05-slides.html#machine-learning-using-dask",
    "href": "slides/05-slides.html#machine-learning-using-dask",
    "title": "Lecture 5",
    "section": "Machine Learning using Dask",
    "text": "Machine Learning using Dask\n\nDask.org page says Scale the Python tools you love. How about Machine Learning libraries?\nDask-ML provides scalable machine learning in Python using Dask alongside popular machine learning libraries like Scikit-Learn, XGBoost, and others.\nDask-ML helps with both compute bound (such as grid search) and memory bound (dataset too large) ML tasks."
  },
  {
    "objectID": "slides/05-slides.html#dask-and-apache-spark",
    "href": "slides/05-slides.html#dask-and-apache-spark",
    "title": "Lecture 5",
    "section": "Dask and Apache Spark",
    "text": "Dask and Apache Spark\n\nThere is hardly ever (probably never) a single solution that works for all big data analytics use-cases.\nDask or Spark is a question that often comes up because they do have overlapping functionality. From the Dask website: Comparison to Spark.\nA rule of thumb: If you are designing an Extract Transform Load (ETL) pipeline and want to use SQL then stick with Spark ecosystem (Hadoop, Hudi, Iceberg etc.), if you want to remain within the Python ecosystem (Pandas, Numpy, scikit-learn, joblib) then Dask could be a good choice."
  },
  {
    "objectID": "slides/05-slides.html#further-reading",
    "href": "slides/05-slides.html#further-reading",
    "title": "Lecture 5",
    "section": "Further Reading",
    "text": "Further Reading\n\nDask and Spark\n\nhttps://www.databricks.com/session/dask-and-apache-spark\nhttps://coiled.io/blog/dask-as-a-spark-replacement/\nhttps://dzone.com/articles/example-of-etl-application-using-apache-spark-and\nhttps://medium.com/geekculture/dask-or-spark-a-comparison-for-data-scientists-d4cba8ba9ef7\n\nCluster creation\n\nhttps://docs.coiled.io/user_guide/cluster_creation.html\nhttps://cloudprovider.dask.org/en/latest/aws.html\nhttps://docs.dask.org/en/stable/deploying-python.html"
  },
  {
    "objectID": "slides/05-slides.html#further-reading-contd.",
    "href": "slides/05-slides.html#further-reading-contd.",
    "title": "Lecture 5",
    "section": "Further Reading (contd.)",
    "text": "Further Reading (contd.)\n\nDask-ML\n\nhttps://examples.dask.org/machine-learning/xgboost.html\nhttps://ml.dask.org/\n\nBlogs on Dask\n\nhttps://aws.amazon.com/blogs/publicsector/analyze-terabyte-scale-geospatial-datasets-with-dask-and-jupyter-on-aws/\nhttps://medium.com/capital-one-tech/dask-and-rapids-the-next-big-things-for-data-science-and-machine-learning-at-capital-one-d4bba136cc70\nhttps://spell.ml/blog/large-scale-etl-jobs-using-dask-Xyl8GhEAACQAjK6h"
  },
  {
    "objectID": "slides/05-slides.html#further-reading-contd.-1",
    "href": "slides/05-slides.html#further-reading-contd.-1",
    "title": "Lecture 5",
    "section": "Further Reading (contd.)",
    "text": "Further Reading (contd.)\n\nMisc.\n\nhttps://www.analyticsvidhya.com/blog/2018/08/dask-big-datasets-machine_learning-python/\nhttps://hub.docker.com/r/daskdev/dask/tags\nhttps://blog.dask.org/2020/07/30/beginners-config\nDask JupyterLab extension\nhttps://docs.dask.org/en/stable/deploying-cli.html\n\nDask Issues.\n\nhttps://github.com/dask/distributed/issues/1015\nhttps://stackoverflow.com/questions/65982439/dask-aws-cluster-error-when-initializing-user-data-is-limited-to-16384-bytes\nhttps://github.com/dask/dask/issues/4843"
  },
  {
    "objectID": "slides/06-slides.html#starting-with-our-big-dataset",
    "href": "slides/06-slides.html#starting-with-our-big-dataset",
    "title": "Lecture 6",
    "section": "Starting with our BIG dataset",
    "text": "Starting with our BIG dataset"
  },
  {
    "objectID": "slides/06-slides.html#the-data-is-split",
    "href": "slides/06-slides.html#the-data-is-split",
    "title": "Lecture 6",
    "section": "The data is split",
    "text": "The data is split"
  },
  {
    "objectID": "slides/06-slides.html#the-data-is-distributed-across-a-cluster-of-machines",
    "href": "slides/06-slides.html#the-data-is-distributed-across-a-cluster-of-machines",
    "title": "Lecture 6",
    "section": "The data is distributed across a cluster of machines",
    "text": "The data is distributed across a cluster of machines\n\n\n\n\n\n\nYou can think of your split/distributed data as a single collection]"
  },
  {
    "objectID": "slides/06-slides.html#important-latency-numbers",
    "href": "slides/06-slides.html#important-latency-numbers",
    "title": "Lecture 6",
    "section": "Important Latency Numbers",
    "text": "Important Latency Numbers"
  },
  {
    "objectID": "slides/06-slides.html#memory-vs.-disk",
    "href": "slides/06-slides.html#memory-vs.-disk",
    "title": "Lecture 6",
    "section": "Memory vs. Disk",
    "text": "Memory vs. Disk"
  },
  {
    "objectID": "slides/06-slides.html#memory-vs.-network",
    "href": "slides/06-slides.html#memory-vs.-network",
    "title": "Lecture 6",
    "section": "Memory vs. Network",
    "text": "Memory vs. Network"
  },
  {
    "objectID": "slides/06-slides.html#memory-disk-and-network",
    "href": "slides/06-slides.html#memory-disk-and-network",
    "title": "Lecture 6",
    "section": "Memory, Disk and Network",
    "text": "Memory, Disk and Network"
  },
  {
    "objectID": "slides/06-slides.html#mapreducehadoop-was-groundbreaking",
    "href": "slides/06-slides.html#mapreducehadoop-was-groundbreaking",
    "title": "Lecture 6",
    "section": "MapReduce/Hadoop was groundbreaking",
    "text": "MapReduce/Hadoop was groundbreaking\n\nIt provided a simple API (map and reduce steps)\nIt provided fault tolerance, which made it possible to scale to 100s/1000s of nodes of commodity machines where the likelihood of a node failing midway through a job was very high\n\nComputations on very large datasets failed and recovered and jobs completed"
  },
  {
    "objectID": "slides/06-slides.html#fault-tolerance-came-at-a-cost",
    "href": "slides/06-slides.html#fault-tolerance-came-at-a-cost",
    "title": "Lecture 6",
    "section": "Fault tolerance came at a cost!",
    "text": "Fault tolerance came at a cost!\n\nBetween each map and reduce step, MapReduce shuffles its data and writes intermediate data to disk\n\nReading/writing to disk is 100x slower than in-memory\nNetwork communication is 1,000,000x slower than in-memory"
  },
  {
    "objectID": "slides/06-slides.html#what-is-spark",
    "href": "slides/06-slides.html#what-is-spark",
    "title": "Lecture 6",
    "section": "What is Spark",
    "text": "What is Spark\n\n\n\n\n\nA simple programming model that can capture streaming, batch, and interactive workloads\nRetains fault-tolerance\nUses a different strategy for handling latency: it keeps all data immutable and in memory\nProvides speed and flexibility"
  },
  {
    "objectID": "slides/06-slides.html#spark-stack",
    "href": "slides/06-slides.html#spark-stack",
    "title": "Lecture 6",
    "section": "Spark Stack",
    "text": "Spark Stack"
  },
  {
    "objectID": "slides/06-slides.html#connected-and-extensible",
    "href": "slides/06-slides.html#connected-and-extensible",
    "title": "Lecture 6",
    "section": "Connected and extensible",
    "text": "Connected and extensible"
  },
  {
    "objectID": "slides/06-slides.html#three-data-structure-apis",
    "href": "slides/06-slides.html#three-data-structure-apis",
    "title": "Lecture 6",
    "section": "Three data structure APIs",
    "text": "Three data structure APIs\n\nRDDs (Resilient Distributed Datasets)\nDataFrames SQL-like structured datasets with query operations\nDatasets A mixture of RDDs and DataFrames\n\nWe’ll only use RDDs and DataFrames in this course."
  },
  {
    "objectID": "slides/06-slides.html#spark-vs.-hadoop",
    "href": "slides/06-slides.html#spark-vs.-hadoop",
    "title": "Lecture 6",
    "section": "Spark vs. Hadoop",
    "text": "Spark vs. Hadoop\n\n\n\n\n\n\n  \n  \n    \n      Hadoop Limitation\n      Spark Approach\n    \n  \n  \n    For iterative processes and interactive use, Hadoop and MapReduce's mandatory dumping of output to disk proved to be a huge bottleneck. In ML, for example, users rely on iterative processes to train-test-retrain.\nSpark uses an in-memory processing paradigm, which lowers the disk IO substantially. Spark uses DAGs to store details of each transformation done on a parallelized dataset and does not process them to get results until required (lazy).\n    Traditional Hadoop applications needed the data first to be copied to HDFS (or other distributed filesystem) and then did the processing.\nSpark works equally well with HDFS or any POSIX style filesystem. However, parallel Spark needs the data to be distributed.\n    Mappers needed a data-localization phase in which the data was written to the local filesystem to bring resilience.\nResilience in Spark is brough about by the DAGs, in which a missing RDD is re-calculated by following the path from which the RDD was created.\n    Hadoop is built on Java and you must use Java to take advantage of all of it's capabilities. Although you can run non-Java scripts with Hadoop Streaming, it is still running a Java Framework.\nSpark is developed in Scala, and it has a unified API with so you can use Spark with Scala, Java, R and Python."
  },
  {
    "objectID": "slides/06-slides.html#example-word-count-yes-again",
    "href": "slides/06-slides.html#example-word-count-yes-again",
    "title": "Lecture 6",
    "section": "Example: word count (yes, again!)",
    "text": "Example: word count (yes, again!)\nThe “Hello, World!” of programming with large scale data.\n\n# read data from text file and split each line into words\nrdd = sc.textFile(\"...\") \n\ncount = rdd.flatMap(lambda line: line.split(\" \")) \\ # separate lines into words\n                     .map(lambda word: (word, 1)) \\ # Add 1 to each word\n                     .reduceByKey(lambda a,b:a +b) # sum all the 1's for each key\n\nThat’s it!"
  },
  {
    "objectID": "slides/06-slides.html#transformations-and-actions-key-spark-concept",
    "href": "slides/06-slides.html#transformations-and-actions-key-spark-concept",
    "title": "Lecture 6",
    "section": "Transformations and Actions (key Spark concept)",
    "text": "Transformations and Actions (key Spark concept)"
  },
  {
    "objectID": "slides/06-slides.html#how-to-create-and-rdd",
    "href": "slides/06-slides.html#how-to-create-and-rdd",
    "title": "Lecture 6",
    "section": "How to create and RDD?",
    "text": "How to create and RDD?\nRDDs can be created in two ways:\n\n\nTransforming an existing RDD: just like a call to map on a list returns a new list, many higher order functions defined on RDDs return a new RDD\nFrom a SparkContext or SparkSession object: the SparkContext object (renamed SparkSession) can be though of as your handle to the Spark cluster. It represents a connection between the Spark Cluster and your application/client. It defines a handful of methods which can be used to create and populate a new RDD:\n\nparallelize: converts a local object into an RDD\ntextFile: reads a text file from your filesystem and returns an RDD of strings"
  },
  {
    "objectID": "slides/06-slides.html#transformations-and-actions",
    "href": "slides/06-slides.html#transformations-and-actions",
    "title": "Lecture 6",
    "section": "Transformations and Actions",
    "text": "Transformations and Actions\nSpark defines transformations and actions on RDDs:\nTransformations return new RDDs as results.    Actions compute a result based on an RDD which is either returned or saved to an external filesystem."
  },
  {
    "objectID": "slides/06-slides.html#transformations-and-actions-1",
    "href": "slides/06-slides.html#transformations-and-actions-1",
    "title": "Lecture 6",
    "section": "Transformations and Actions",
    "text": "Transformations and Actions\nSpark defines transformations and actions on RDDs:\nTransformations return new RDDs as results.  Transfomations are lazy, their result RDD is not immediately computed.   Actions compute a result based on an RDD which is either returned or saved to an external filesystem.  Actions are eager, their result is immediately computed."
  },
  {
    "objectID": "slides/06-slides.html#common-rdd-transformations",
    "href": "slides/06-slides.html#common-rdd-transformations",
    "title": "Lecture 6",
    "section": "Common RDD Transformations",
    "text": "Common RDD Transformations\n\n\n\n\n\n\n  \n  \n    \n      Method\n      Description\n    \n  \n  \n    map\n\nExpresses a one-to-one transformation and transforms each element of a collection into one element of the resulting collection\n\n    flatMap\n\nExpresses a one-to-many transformation and transforms each element to 0 or more elements\n\n    filter\n\nApplies filter function that returns a boolean and returs an RDD of elements that have passed the filter condition\n\n    distinct\n\nReturns RDD with duplicates removed"
  },
  {
    "objectID": "slides/06-slides.html#common-rdd-actions",
    "href": "slides/06-slides.html#common-rdd-actions",
    "title": "Lecture 6",
    "section": "Common RDD Actions",
    "text": "Common RDD Actions\n\n\n\n\n\n\n  \n  \n    \n      Method\n      Description\n    \n  \n  \n    collect\n\nReturns all distributed elements of the RDD to the driver\n\n    count\n\nReturns the number of elements in an RDD\n\n    take\n\nReturns the first n elements of the RDD\n\n    reduce\n\nCombines elements of the RDD together using some function and returns result"
  },
  {
    "objectID": "slides/06-slides.html#collect-caution",
    "href": "slides/06-slides.html#collect-caution",
    "title": "Lecture 6",
    "section": "collect CAUTION",
    "text": "collect CAUTION"
  },
  {
    "objectID": "slides/06-slides.html#another-example",
    "href": "slides/06-slides.html#another-example",
    "title": "Lecture 6",
    "section": "Another example",
    "text": "Another example\nLet’s assume that we have an RDD of strings which contains gigabytes of logs collected over the previous year. Each element of the RDD represents one line of logs.\nAssuming the dates come in the form YYYY-MM-DD:HH:MM:SS and errors are logged with a prefix that includes the word “error”…\nHow would you determine the number of errors that were logged in December 2019?\n\n\n# read data from text file and split each line into words\nlogs = sc.textFile(\"...\") \n\n# this is a transformation\nerrors = logs.filter(lambda x: \"error\" in x and \"2019-12\" in x)\n\n# this is an action\nerrors.count()\n\nSpark computes RDDs the first time they are used in an action!"
  },
  {
    "objectID": "slides/06-slides.html#caching-and-persistence",
    "href": "slides/06-slides.html#caching-and-persistence",
    "title": "Lecture 6",
    "section": "Caching and Persistence",
    "text": "Caching and Persistence\nBy default, RDDs are recomputed every time you run an action on them. This can be expensive (in time) if you need to use a dataset more than once.\nSpark allows you to control what is cached in memory.\nTo tell spark to cache an object in memory, use persist() or cache():\n\ncache(): is a shortcut for using default storage level, which is memory only\npersist(): can be customized to other ways to persist data (including both memory and/or disk)\n\n\n# caches error RDD in memory, but only after an action is run\nerrors = logs.filter(lambda x: \"error\" in x and \"2019-12\" in x).cache()"
  },
  {
    "objectID": "slides/06-slides.html#using-memory-is-great-for-iterative-workloads",
    "href": "slides/06-slides.html#using-memory-is-great-for-iterative-workloads",
    "title": "Lecture 6",
    "section": "Using memory is great for iterative workloads",
    "text": "Using memory is great for iterative workloads"
  },
  {
    "objectID": "slides/06-slides.html#dataframes-in-a-nutshell",
    "href": "slides/06-slides.html#dataframes-in-a-nutshell",
    "title": "Lecture 6",
    "section": "DataFrames in a nutshell",
    "text": "DataFrames in a nutshell\nDataFrames are…\nDatasets organized into named columns\nConceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood.\nA relational API over Spark’s RDDs\nBecause sometimes it’s more convenient to use declarative relational APIs than functional APIs for analysis jobs:\n\n\nselect\nwhere\nlimit\n\n\n\n\norderBy\ngroupBy\njoin\n\n\nAble to be automatically aggresively optimized\nSparkSQL applies years of research on relational optimizations in the database community to Spark"
  },
  {
    "objectID": "slides/06-slides.html#dataframe-data-types",
    "href": "slides/06-slides.html#dataframe-data-types",
    "title": "Lecture 6",
    "section": "DataFrame Data Types",
    "text": "DataFrame Data Types\nSparkSQL’s DataFrames operate on a restricted (yet broad) set of data types. These are the most common:\n\nInteger types (at different lengths): ByteType, ShortType, IntegerType, LongType\nDecimal types: Float, Double\nBooleanType\nStringType\nDate/Time: TimestampType, DateType"
  },
  {
    "objectID": "slides/06-slides.html#a-dataframe",
    "href": "slides/06-slides.html#a-dataframe",
    "title": "Lecture 6",
    "section": "A DataFrame",
    "text": "A DataFrame"
  },
  {
    "objectID": "slides/06-slides.html#getting-a-look-at-your-data",
    "href": "slides/06-slides.html#getting-a-look-at-your-data",
    "title": "Lecture 6",
    "section": "Getting a look at your data",
    "text": "Getting a look at your data\nThere are a few ways you can have a look at your data in DataFrames:\n\nshow() pretty-prints DataFrame in tabular form. Shows first 20 elements\nprintSchema() prints the schema of your DataFrame in a tree format."
  },
  {
    "objectID": "slides/06-slides.html#common-dataframe-transformations",
    "href": "slides/06-slides.html#common-dataframe-transformations",
    "title": "Lecture 6",
    "section": "Common DataFrame Transformations",
    "text": "Common DataFrame Transformations\nLike on RDD’s, transformations on DataFrames are:\n\nOperations that return another DataFrame as a results\nAre lazily evaluated"
  },
  {
    "objectID": "slides/06-slides.html#some-common-transformations-include",
    "href": "slides/06-slides.html#some-common-transformations-include",
    "title": "Lecture 6",
    "section": "Some common transformations include:",
    "text": "Some common transformations include:\n\n\n\n\n\n\n  \n  \n    \n      Method\n      Description\n    \n  \n  \n    select\n\nSelects a set of named columns and returns a new DataFrame with those columns as a result\n\n    agg\n\nPerforms aggregations on a series of columns and returns a new DataFrame with the calculated output\n\n    groupBy\n\nGroups the DataFrame using the specified columns, usually used before some kind of aggregation\n\n    join\n\nInner join with another DataFrame\n\n  \n  \n  \n\n\n\n\nOther transformations include: filter, limit, orderBy, where."
  },
  {
    "objectID": "slides/06-slides.html#specifying-columns",
    "href": "slides/06-slides.html#specifying-columns",
    "title": "Lecture 6",
    "section": "Specifying columns",
    "text": "Specifying columns\nMost methods take a parameter of type Column or String, always referring to some attribute/column in the the DataFrame.\nYou can select and work with columns in ways using the DataFrame API:\n\nUsing $ notation: df.filter($\"age\" > 18)\nReferring to the DataFrame: df.filter(df(\"age\") > 18)\nUsing SQL query string: df.filter(\"age > 18\")"
  },
  {
    "objectID": "slides/06-slides.html#filtering-in-sparksql",
    "href": "slides/06-slides.html#filtering-in-sparksql",
    "title": "Lecture 6",
    "section": "Filtering in SparkSQL",
    "text": "Filtering in SparkSQL\nThe DataFrame API makes two methods available for filtering: filter and where. They are equivalent!\n\nemployee_df.filter(\"age > 30\").show()\n\nis equivalent to\n\nemployee_df.where(\"age > 30\").show()"
  },
  {
    "objectID": "slides/06-slides.html#use-either-dataframe-api-and-sparksql",
    "href": "slides/06-slides.html#use-either-dataframe-api-and-sparksql",
    "title": "Lecture 6",
    "section": "Use either DataFrame API and SparkSQL",
    "text": "Use either DataFrame API and SparkSQL\nThe DataFrame API and SparkSQL syntax can be used interchangeably!\nExample: return the firstname and lastname of all the employees over the age over 25 that reside in Washington D.C."
  },
  {
    "objectID": "slides/06-slides.html#dataframe-api",
    "href": "slides/06-slides.html#dataframe-api",
    "title": "Lecture 6",
    "section": "DataFrame API",
    "text": "DataFrame API\n\nresults = df.select(\"firstname\", \"lastname\") \\\n            .where(\"city == 'Washington D.C.' && age >= 25\")"
  },
  {
    "objectID": "slides/06-slides.html#sparksql",
    "href": "slides/06-slides.html#sparksql",
    "title": "Lecture 6",
    "section": "SparkSQL",
    "text": "SparkSQL\n\nspark.sql(\"select firstname, lastname from df_view where city == 'Washington D.C.' and age >= 25\")\n          \n# * Note: you have to register `df` using `df.createOrReplaceTempView(\"df_view\")`"
  },
  {
    "objectID": "slides/06-slides.html#grouping-and-aggregating-on-dataframes",
    "href": "slides/06-slides.html#grouping-and-aggregating-on-dataframes",
    "title": "Lecture 6",
    "section": "Grouping and aggregating on DataFrames",
    "text": "Grouping and aggregating on DataFrames\nSome of the most common tasks on structured data tables include:\n\nGrouping by a certain attributed\nDoing some kind of aggregation on the grouping, like a count\n\nFor grouping and aggregating, SparkSQL provides a groupBy function which returns a RelationalGroupedDataset which has several standard aggregation functions like count, sum, max, min, and avg."
  },
  {
    "objectID": "slides/06-slides.html#how-to-group",
    "href": "slides/06-slides.html#how-to-group",
    "title": "Lecture 6",
    "section": "How to group",
    "text": "How to group\n\nCall a groupBy on a specific attribute/column of a DataFrame\nfollowed by a call to agg\n\n\nresults = df.groupBy(\"state\") \\\n            .agg(sum(\"sales\"))"
  },
  {
    "objectID": "slides/06-slides.html#actions-on-dataframes",
    "href": "slides/06-slides.html#actions-on-dataframes",
    "title": "Lecture 6",
    "section": "Actions on DataFrames",
    "text": "Actions on DataFrames\nLike RDDs, DataFrames also have their own set of actions:\n\n\n\n\n\n\n  \n  \n    \n      Method\n      Description\n    \n  \n  \n    collect\n\nReturns an array that contains all the rows in the DataFrame to the driver\n\n    count\n\nReturns the number of rows in a DataFrame\n\n    first\n\nReturns the first row in the DataFrame\n\n    show\n\nDisplays the top 20 rows in the DataFrame\n\n    take\n\nReturns the first n rows of the RDD"
  },
  {
    "objectID": "slides/06-slides.html#collect-caution-1",
    "href": "slides/06-slides.html#collect-caution-1",
    "title": "Lecture 6",
    "section": "collect CAUTION",
    "text": "collect CAUTION"
  },
  {
    "objectID": "slides/06-slides.html#limitations-on-dataframe",
    "href": "slides/06-slides.html#limitations-on-dataframe",
    "title": "Lecture 6",
    "section": "Limitations on DataFrame",
    "text": "Limitations on DataFrame\n\nCan only use DataFrame data types\nIf your unstructured data cannot be reformulated to adhere to some kind of schema, it would be better to use RDDs."
  },
  {
    "objectID": "slides/07-slides.html#aws-academy",
    "href": "slides/07-slides.html#aws-academy",
    "title": "Lecture 7",
    "section": "AWS Academy",
    "text": "AWS Academy\n\nCredit limit - $100\nCourse numbers:\n\nCourse #1 - 24178\nCourse #2 - 27354\nCourse #3 - 22802\nCourse #4 - 26418\n\n\nSTAY WITH COURSE 24178 UNLESS YOU HAVE RUN OUT OF CREDITS OR >$90 USED!\nNote that you will have to repeat several setup steps:\n\nsecurity group\nEC2 keypair uploading (the AWS part only)\nsagemaker setup\nany S3 uploading or copying as well as bucket creation as necessary\nEMR configuration"
  },
  {
    "objectID": "slides/07-slides.html#memory-disk-and-network",
    "href": "slides/07-slides.html#memory-disk-and-network",
    "title": "Lecture 7",
    "section": "Memory, Disk and Network",
    "text": "Memory, Disk and Network"
  },
  {
    "objectID": "slides/07-slides.html#mapreducehadoop-was-groundbreaking",
    "href": "slides/07-slides.html#mapreducehadoop-was-groundbreaking",
    "title": "Lecture 7",
    "section": "MapReduce/Hadoop was groundbreaking",
    "text": "MapReduce/Hadoop was groundbreaking\n\nIt provided a simple API (map and reduce steps)\nIt provided fault tolerance, which made it possible to scale to 100s/1000s of nodes of commodity machines where the likelihood of a node failing midway through a job was very high\n\nComputations on very large datasets failed and recovered and jobs completed"
  },
  {
    "objectID": "slides/07-slides.html#fault-tolerance-came-at-a-cost",
    "href": "slides/07-slides.html#fault-tolerance-came-at-a-cost",
    "title": "Lecture 7",
    "section": "Fault tolerance came at a cost!",
    "text": "Fault tolerance came at a cost!\n\nBetween each map and reduce step, MapReduce shuffles its data and writes intermediate data to disk\n\nReading/writing to disk is 100x slower than in-memory\nNetwork communication is 1,000,000x slower than in-memory"
  },
  {
    "objectID": "slides/07-slides.html#review-of-file-systems",
    "href": "slides/07-slides.html#review-of-file-systems",
    "title": "Lecture 7",
    "section": "Review of File Systems",
    "text": "Review of File Systems\nWhat are the possible file system options for each item: S3, HDFS, Local file system\n\nhadoop jar /usr/lib/hadoop/hadoop-streaming.jar #1\n\n\n\n-files basic-mapper.py,basic-reducer.py #2\n\n\n\n-input /user/hadoop/in_data #3\n-output /user/hadoop/in_data #3\n\n\n\n-mapper basic-mapper.py #4\n-reducer basic-reducer.py #4\n\n\n\nLocal file system\n\n\n\n\nLocal file system or S3\n\n\n\n\nHDFS or S3\n\n\n\n\nHDFS - why??"
  },
  {
    "objectID": "slides/07-slides.html#cluster-setup---1-install-miniconda",
    "href": "slides/07-slides.html#cluster-setup---1-install-miniconda",
    "title": "Lecture 7",
    "section": "Cluster Setup - 1 (install miniconda)",
    "text": "Cluster Setup - 1 (install miniconda)\necho \"Installing Miniconda\"\ncurl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o /tmp/miniconda.sh\n\necho $(date \"+%B %d %T\")\n\nbash /tmp/miniconda.sh -b -p /mnt/miniconda\nrm /tmp/miniconda.sh\nln -s /mnt/miniconda $HOME/miniconda\necho -e '\\nexport PATH=$HOME/miniconda/bin:$PATH' >> $HOME/.bashrc\nsource $HOME/.bashrc\nconda update conda -y\n\necho $(date \"+%B %d %T\")"
  },
  {
    "objectID": "slides/07-slides.html#cluster-setup---2-install-cluster-packages",
    "href": "slides/07-slides.html#cluster-setup---2-install-cluster-packages",
    "title": "Lecture 7",
    "section": "Cluster Setup - 2 (install cluster packages)",
    "text": "Cluster Setup - 2 (install cluster packages)\necho \"Installing common Python pacakges on master and workers\"\n#conda install \\\n#-c defaults \\\n#-c conda-forge \\\n#-y \\\n#python=3.7 \\\n#dask-yarn \\\n#pyarrow \\\n#s3fs \\\n#bokeh \\\n#conda-pack \\\n#tornado\n\naws s3 cp s3://bigdatateaching/bootstrap/worker_environment.yaml /tmp/worker_environment.yaml\n\nconda env update -n base --file /tmp/worker_environment.yaml\n\necho $(date \"+%B %d %T\")"
  },
  {
    "objectID": "slides/07-slides.html#cluster-setup---3-print-python-packages",
    "href": "slides/07-slides.html#cluster-setup---3-print-python-packages",
    "title": "Lecture 7",
    "section": "Cluster Setup - 3 (Print python packages)",
    "text": "Cluster Setup - 3 (Print python packages)\nif [ \"$IS_MASTER\" = false ]; then\n    echo \"Python packages installed in worker nodes:\"\n    conda list\nfi"
  },
  {
    "objectID": "slides/07-slides.html#cluster-setup---4-setup-master-node",
    "href": "slides/07-slides.html#cluster-setup---4-setup-master-node",
    "title": "Lecture 7",
    "section": "Cluster Setup - 4 (Setup master node)",
    "text": "Cluster Setup - 4 (Setup master node)\nsudo yum install -y git libcurl-devel htop\necho \"Installing Master node Python libraries\"\nconda install \\\n-c defaults \\\n-c conda-forge \\\n-y \\\nnotebook \\\nipywidgets \\\njupyter-server-proxy \\\nfindspark \\\nmatplotlib \\\njupyterlab \\\nscikit-learn \\\nnltk \\\nscipy \\\nbeautifulsoup4 \\\nnose \\\nlxml \\\nhdf5 \\\nseaborn \\\nboto3\n\npip install -U nbclassic>=0.2.8 \n# issue with older nbclassic - https://github.com/jupyterlab/jupyterlab/issues/10228\n\necho $(date \"+%B %d %T\")\n\npip install spark-nlp\n\necho $(date \"+%B %d %T\")"
  },
  {
    "objectID": "slides/07-slides.html#cluster-setup---5-jupyter-lab-setup",
    "href": "slides/07-slides.html#cluster-setup---5-jupyter-lab-setup",
    "title": "Lecture 7",
    "section": "Cluster Setup - 5 (Jupyter Lab Setup)",
    "text": "Cluster Setup - 5 (Jupyter Lab Setup)\nmkdir -p $HOME/.jupyter\n\ncat <<EOF >> $HOME/.jupyter/jupyter_notebook_config.py\nc.NotebookApp.open_browser = False\nc.NotebookApp.ip = '0.0.0.0'\nc.NotebookApp.port = 8765\nc.NotebookApp.token = ''\nEOF"
  },
  {
    "objectID": "slides/07-slides.html#cluster-setup---6-other-sys-admin",
    "href": "slides/07-slides.html#cluster-setup---6-other-sys-admin",
    "title": "Lecture 7",
    "section": "Cluster Setup - 6 (Other Sys Admin)",
    "text": "Cluster Setup - 6 (Other Sys Admin)\ncat <<EOF > /tmp/jupyter-lab.service\n[Unit]\nDescription=\"Jupyter Lab Server\"\n\n[Service]\nType=simple\nExecStart=/usr/bin/su -s /bin/bash hadoop -c \"cd $HOME; $HOME/miniconda/bin/jupyter lab\" >> /mnt/tmp/jupyter-lab.log 2>&1\n[Install]\nWantedBy=multi-user.target\nEOF\nsudo mv /tmp/jupyter-lab.service /etc/systemd/system\nsudo chmod 644 /etc/systemd/system/jupyter-lab.service\nsudo systemctl start jupyter-lab\n\necho \"Starting Jupyter Notebook Server\"\nsudo systemctl daemon-reload\nsudo systemctl start jupyter-lab\n\nconda init\necho -e \"\\nalias pip='$HOME/miniconda/bin/pip'\" >> $HOME/.bashrc\necho -e \"\\nalias python='$HOME/miniconda/bin/python'\" >> $HOME/.bashrc"
  },
  {
    "objectID": "slides/07-slides.html#review-of-logs-in-emr",
    "href": "slides/07-slides.html#review-of-logs-in-emr",
    "title": "Lecture 7",
    "section": "Review of Logs in EMR",
    "text": "Review of Logs in EMR\n\n\n\n\nNode -> first node option -> bootstrap-actions -> 1 -> stdout.gz\nClick to View/Download\nExample log file"
  },
  {
    "objectID": "slides/07-slides.html#check-the-bootup-routine-for-your-ec2",
    "href": "slides/07-slides.html#check-the-bootup-routine-for-your-ec2",
    "title": "Lecture 7",
    "section": "Check the Bootup Routine for your EC2",
    "text": "Check the Bootup Routine for your EC2\n\nHardware tab\nMaster Instance Group\nInstance ID of EC2\nEC2 Hardware Details\nActions -> “Monitor and troubleshoot” -> Get system log\nActions -> “Monitor and troubleshoot” -> Get instance screenshot"
  },
  {
    "objectID": "slides/07-slides.html#connected-and-extensible",
    "href": "slides/07-slides.html#connected-and-extensible",
    "title": "Lecture 7",
    "section": "Connected and extensible",
    "text": "Connected and extensible"
  },
  {
    "objectID": "slides/07-slides.html#three-data-structure-apis",
    "href": "slides/07-slides.html#three-data-structure-apis",
    "title": "Lecture 7",
    "section": "Three data structure APIs",
    "text": "Three data structure APIs\n\nRDDs (Resilient Distributed Datasets)\nDataFrames SQL-like structured datasets with query operations\nDatasets A mixture of RDDs and DataFrames"
  },
  {
    "objectID": "slides/07-slides.html#spark-vs.-hadoop",
    "href": "slides/07-slides.html#spark-vs.-hadoop",
    "title": "Lecture 7",
    "section": "Spark vs. Hadoop",
    "text": "Spark vs. Hadoop\n\n\n\n\n\n\n  \n  \n    \n      Hadoop Limitation\n      Spark Approach\n    \n  \n  \n    For iterative processes and interactive use, Hadoop and MapReduce's mandatory dumping of output to disk proved to be a huge bottleneck. In ML, for example, users rely on iterative processes to train-test-retrain.\nSpark uses an in-memory processing paradigm, which lowers the disk IO substantially. Spark uses DAGs to store details of each transformation done on a parallelized dataset and does not process them to get results until required (lazy).\n    Traditional Hadoop applications needed the data first to be copied to HDFS (or other distributed filesystem) and then did the processing.\nSpark works equally well with HDFS or any POSIX style filesystem. However, parallel Spark needs the data to be distributed.\n    Mappers needed a data-localization phase in which the data was written to the local filesystem to bring resilience.\nResilience in Spark is brough about by the DAGs, in which a missing RDD is re-calculated by following the path from which the RDD was created.\n    Hadoop is built on Java and you must use Java to take advantage of all of it's capabilities. Although you can run non-Java scripts with Hadoop Streaming, it is still running a Java Framework.\nSpark is developed in Scala, and it has a unified API with so you can use Spark with Scala, Java, R and Python."
  },
  {
    "objectID": "slides/07-slides.html#transformations-and-actions-key-spark-concept",
    "href": "slides/07-slides.html#transformations-and-actions-key-spark-concept",
    "title": "Lecture 7",
    "section": "Transformations and Actions (key Spark concept)",
    "text": "Transformations and Actions (key Spark concept)"
  },
  {
    "objectID": "slides/07-slides.html#how-to-create-and-rdd",
    "href": "slides/07-slides.html#how-to-create-and-rdd",
    "title": "Lecture 7",
    "section": "How to create and RDD?",
    "text": "How to create and RDD?\nRDDs can be created in two ways:\n\n\nTransforming an existing RDD: just like a call to map on a list returns a new list, many higher order functions defined on RDDs return a new RDD\nFrom a SparkContext or SparkSession object: the SparkContext object (renamed SparkSession) can be though of as your handle to the Spark cluster. It represents a connection between the Spark Cluster and your application/client. It defines a handful of methods which can be used to create and populate a new RDD:\n\nparallelize: converts a local object into an RDD\ntextFile: reads a text file from your filesystem and returns an RDD of strings"
  },
  {
    "objectID": "slides/07-slides.html#transformations-and-actions",
    "href": "slides/07-slides.html#transformations-and-actions",
    "title": "Lecture 7",
    "section": "Transformations and Actions",
    "text": "Transformations and Actions\nSpark defines transformations and actions on RDDs:\nTransformations return new RDDs as results.\n\nTransformations are lazy, their result RDD is not immediately computed.\n\n \nActions compute a result based on an RDD which is either returned or saved to an external filesystem.\n\nActions are eager, their result is immediately computed."
  },
  {
    "objectID": "slides/07-slides.html#common-rdd-transformations",
    "href": "slides/07-slides.html#common-rdd-transformations",
    "title": "Lecture 7",
    "section": "Common RDD Transformations",
    "text": "Common RDD Transformations\n\n\n\n\n\n\n  \n  \n    \n      Method\n      Description\n    \n  \n  \n    map\n\nExpresses a one-to-one transformation and transforms each element of a collection into one element of the resulting collection\n\n    flatMap\n\nExpresses a one-to-many transformation and transforms each element to 0 or more elements\n\n    filter\n\nApplies filter function that returns a boolean and returs an RDD of elements that have passed the filter condition\n\n    distinct\n\nReturns RDD with duplicates removed"
  },
  {
    "objectID": "slides/07-slides.html#common-rdd-actions",
    "href": "slides/07-slides.html#common-rdd-actions",
    "title": "Lecture 7",
    "section": "Common RDD Actions",
    "text": "Common RDD Actions\n\n\n\n\n\n\n  \n  \n    \n      Method\n      Description\n    \n  \n  \n    collect\n\nReturns all distributed elements of the RDD to the driver\n\n    count\n\nReturns the number of elements in an RDD\n\n    take\n\nReturns the first n elements of the RDD\n\n    reduce\n\nCombines elements of the RDD together using some function and returns result"
  },
  {
    "objectID": "slides/07-slides.html#collect-caution",
    "href": "slides/07-slides.html#collect-caution",
    "title": "Lecture 7",
    "section": "collect CAUTION",
    "text": "collect CAUTION"
  },
  {
    "objectID": "slides/07-slides.html#caching-and-persistence",
    "href": "slides/07-slides.html#caching-and-persistence",
    "title": "Lecture 7",
    "section": "Caching and Persistence",
    "text": "Caching and Persistence\nBy default, RDDs are recomputed every time you run an action on them. This can be expensive (in time) if you need to use a dataset more than once.\nSpark allows you to control what is cached in memory.\nTo tell spark to cache an object in memory, use persist() or cache():\n\ncache(): is a shortcut for using default storage level, which is memory only\npersist(): can be customized to other ways to persist data (including both memory and/or disk)\n\n\n# caches error RDD in memory, but only after an action is run\nerrors = logs.filter(lambda x: \"error\" in x and \"2019-12\" in x).cache()"
  },
  {
    "objectID": "slides/07-slides.html#review-of-pysparksql-cheatsheet",
    "href": "slides/07-slides.html#review-of-pysparksql-cheatsheet",
    "title": "Lecture 7",
    "section": "Review of PySparkSQL Cheatsheet",
    "text": "Review of PySparkSQL Cheatsheet\nhttps://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf"
  },
  {
    "objectID": "slides/07-slides.html#dataframes-in-a-nutshell",
    "href": "slides/07-slides.html#dataframes-in-a-nutshell",
    "title": "Lecture 7",
    "section": "DataFrames in a nutshell",
    "text": "DataFrames in a nutshell\nDataFrames are…\nDatasets organized into named columns\nConceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood.\nA relational API over Spark’s RDDs\nBecause sometimes it’s more convenient to use declarative relational APIs than functional APIs for analysis jobs:\n\n\nselect\nwhere\nlimit\n\n\n\n\norderBy\ngroupBy\njoin\n\n\nAble to be automatically aggresively optimized\nSparkSQL applies years of research on relational optimizations in the database community to Spark"
  },
  {
    "objectID": "slides/07-slides.html#dataframe-data-types",
    "href": "slides/07-slides.html#dataframe-data-types",
    "title": "Lecture 7",
    "section": "DataFrame Data Types",
    "text": "DataFrame Data Types\nSparkSQL’s DataFrames operate on a restricted (yet broad) set of data types. These are the most common:\n\nInteger types (at different lengths): ByteType, ShortType, IntegerType, LongType\nDecimal types: Float, Double\nBooleanType\nStringType\nDate/Time: TimestampType, DateType"
  },
  {
    "objectID": "slides/07-slides.html#a-dataframe",
    "href": "slides/07-slides.html#a-dataframe",
    "title": "Lecture 7",
    "section": "A DataFrame",
    "text": "A DataFrame"
  },
  {
    "objectID": "slides/07-slides.html#getting-a-look-at-your-data",
    "href": "slides/07-slides.html#getting-a-look-at-your-data",
    "title": "Lecture 7",
    "section": "Getting a look at your data",
    "text": "Getting a look at your data\nThere are a few ways you can have a look at your data in DataFrames:\n\nshow() pretty-prints DataFrame in tabular form. Shows first 20 elements\nprintSchema() prints the schema of your DataFrame in a tree format."
  },
  {
    "objectID": "slides/07-slides.html#common-dataframe-transformations",
    "href": "slides/07-slides.html#common-dataframe-transformations",
    "title": "Lecture 7",
    "section": "Common DataFrame Transformations",
    "text": "Common DataFrame Transformations\nLike on RDD’s, transformations on DataFrames are:\n\nOperations that return another DataFrame as a results\nAre lazily evaluated"
  },
  {
    "objectID": "slides/07-slides.html#some-common-transformations-include",
    "href": "slides/07-slides.html#some-common-transformations-include",
    "title": "Lecture 7",
    "section": "Some common transformations include:",
    "text": "Some common transformations include:\n\n\n\n\n\n\n  \n  \n    \n      Method\n      Description\n    \n  \n  \n    select\n\nSelects a set of named columns and returns a new DataFrame with those columns as a result\n\n    agg\n\nPerforms aggregations on a series of columns and returns a new DataFrame with the calculated output\n\n    groupBy\n\nGroups the DataFrame using the specified columns, usually used before some kind of aggregation\n\n    join\n\nInner join with another DataFrame\n\n  \n  \n  \n\n\n\n\nOther transformations include: filter, limit, orderBy, where."
  },
  {
    "objectID": "slides/07-slides.html#specifying-columns",
    "href": "slides/07-slides.html#specifying-columns",
    "title": "Lecture 7",
    "section": "Specifying columns",
    "text": "Specifying columns\nMost methods take a parameter of type Column or String, always referring to some attribute/column in the the DataFrame.\nYou can select and work with columns in ways using the DataFrame API:\n\nUsing $ notation: df.filter($\"age\" > 18)\nReferring to the DataFrame: df.filter(df(\"age\") > 18)\nUsing SQL query string: df.filter(\"age > 18\")"
  },
  {
    "objectID": "slides/07-slides.html#filtering-in-sparksql",
    "href": "slides/07-slides.html#filtering-in-sparksql",
    "title": "Lecture 7",
    "section": "Filtering in SparkSQL",
    "text": "Filtering in SparkSQL\nThe DataFrame API makes two methods available for filtering: filter and where. They are equivalent!\n\nemployee_df.filter(\"age > 30\").show()\n\nis equivalent to\n\nemployee_df.where(\"age > 30\").show()"
  },
  {
    "objectID": "slides/07-slides.html#use-either-dataframe-api-and-sparksql",
    "href": "slides/07-slides.html#use-either-dataframe-api-and-sparksql",
    "title": "Lecture 7",
    "section": "Use either DataFrame API and SparkSQL",
    "text": "Use either DataFrame API and SparkSQL\nThe DataFrame API and SparkSQL syntax can be used interchangeably!\nExample: return the firstname and lastname of all the employees over the age over 25 that reside in Washington D.C."
  },
  {
    "objectID": "slides/07-slides.html#dataframe-api",
    "href": "slides/07-slides.html#dataframe-api",
    "title": "Lecture 7",
    "section": "DataFrame API",
    "text": "DataFrame API\n\nresults = df.select(\"firstname\", \"lastname\") \\\n            .where(\"city == 'Washington D.C.' && age >= 25\")"
  },
  {
    "objectID": "slides/07-slides.html#sparksql",
    "href": "slides/07-slides.html#sparksql",
    "title": "Lecture 7",
    "section": "SparkSQL",
    "text": "SparkSQL\n\nspark.sql(\"select firstname, lastname from df_view where city == 'Washington D.C.' and age >= 25\")\n          \n# * Note: you have to register `df` using `df.createOrReplaceTempView(\"df_view\")`"
  },
  {
    "objectID": "slides/07-slides.html#grouping-and-aggregating-on-dataframes",
    "href": "slides/07-slides.html#grouping-and-aggregating-on-dataframes",
    "title": "Lecture 7",
    "section": "Grouping and aggregating on DataFrames",
    "text": "Grouping and aggregating on DataFrames\nSome of the most common tasks on structured data tables include:\n\nGrouping by a certain attributed\nDoing some kind of aggregation on the grouping, like a count\n\nFor grouping and aggregating, SparkSQL provides a groupBy function which returns a RelationalGroupedDataset which has several standard aggregation functions like count, sum, max, min, and avg."
  },
  {
    "objectID": "slides/07-slides.html#how-to-group",
    "href": "slides/07-slides.html#how-to-group",
    "title": "Lecture 7",
    "section": "How to group",
    "text": "How to group\n\nCall a groupBy on a specific attribute/column of a DataFrame\nfollowed by a call to agg\n\n\nresults = df.groupBy(\"state\") \\\n            .agg(sum(\"sales\"))"
  },
  {
    "objectID": "slides/07-slides.html#actions-on-dataframes",
    "href": "slides/07-slides.html#actions-on-dataframes",
    "title": "Lecture 7",
    "section": "Actions on DataFrames",
    "text": "Actions on DataFrames\nLike RDDs, DataFrames also have their own set of actions:\n\n\n\n\n\n\n  \n  \n    \n      Method\n      Description\n    \n  \n  \n    collect\n\nReturns an array that contains all the rows in the DataFrame to the driver\n\n    count\n\nReturns the number of rows in a DataFrame\n\n    first\n\nReturns the first row in the DataFrame\n\n    show\n\nDisplays the top 20 rows in the DataFrame\n\n    take\n\nReturns the first n rows of the RDD"
  },
  {
    "objectID": "slides/07-slides.html#collect-caution-1",
    "href": "slides/07-slides.html#collect-caution-1",
    "title": "Lecture 7",
    "section": "collect CAUTION",
    "text": "collect CAUTION"
  },
  {
    "objectID": "slides/07-slides.html#limitations-on-dataframe",
    "href": "slides/07-slides.html#limitations-on-dataframe",
    "title": "Lecture 7",
    "section": "Limitations on DataFrame",
    "text": "Limitations on DataFrame\n\nCan only use DataFrame data types\nIf your unstructured data cannot be reformulated to adhere to some kind of schema, it would be better to use RDDs."
  },
  {
    "objectID": "slides/08-slides.html#aws-academy",
    "href": "slides/08-slides.html#aws-academy",
    "title": "Lecture 8",
    "section": "AWS Academy",
    "text": "AWS Academy\n\nCredit limit - $100\nCourse numbers:\n\nCourse #1 - 24178\nCourse #2 - 27354\nCourse #3 - 22802\nCourse #4 - 26418\n\n\nSTAY WITH COURSE 24178 UNLESS YOU HAVE RUN OUT OF CREDITS OR >$90 USED!\nNote that you will have to repeat several setup steps:\n\nsecurity group\nEC2 keypair uploading (the AWS part only)\nsagemaker setup\nany S3 uploading or copying as well as bucket creation as necessary\nEMR configuration"
  },
  {
    "objectID": "slides/08-slides.html#grades",
    "href": "slides/08-slides.html#grades",
    "title": "Lecture 8",
    "section": "Grades",
    "text": "Grades\n\nYou are responsible for your grades\nIf there is a problem, you need to bring it up\nAll assignments are mandatory\nState of assignments from syllabus\nGroup Project: 30%\nHW Assignments: 45%\nLab Deliverables: 15% - Halfway through\nQuizzes: 10% - One to two more"
  },
  {
    "objectID": "slides/08-slides.html#review-of-file-systems",
    "href": "slides/08-slides.html#review-of-file-systems",
    "title": "Lecture 8",
    "section": "Review of File Systems",
    "text": "Review of File Systems\nWhat are the possible file system options for each item: S3, HDFS, Local file system\n\nhadoop jar /usr/lib/hadoop/hadoop-streaming.jar #1\n\n\n\n-files basic-mapper.py,basic-reducer.py #2\n\n\n\n-input /user/hadoop/in_data #3\n-output /user/hadoop/in_data #3\n\n\n\n-mapper basic-mapper.py #4\n-reducer basic-reducer.py #4\n\n\n\nLocal file system\n\n\n\n\nLocal file system or S3\n\n\n\n\nHDFS or S3\n\n\n\n\nHDFS - why??"
  },
  {
    "objectID": "slides/08-slides.html#connected-and-extensible",
    "href": "slides/08-slides.html#connected-and-extensible",
    "title": "Lecture 8",
    "section": "Connected and extensible",
    "text": "Connected and extensible"
  },
  {
    "objectID": "slides/08-slides.html#caching-and-persistence",
    "href": "slides/08-slides.html#caching-and-persistence",
    "title": "Lecture 8",
    "section": "Caching and Persistence",
    "text": "Caching and Persistence\nBy default, RDDs are recomputed every time you run an action on them. This can be expensive (in time) if you need to use a dataset more than once.\nSpark allows you to control what is cached in memory.\nTo tell spark to cache an object in memory, use persist() or cache():\n\ncache(): is a shortcut for using default storage level, which is memory only\npersist(): can be customized to other ways to persist data (including both memory and/or disk)\n\n\n# caches error RDD in memory, but only after an action is run\nerrors = logs.filter(lambda x: \"error\" in x and \"2019-12\" in x).cache()"
  },
  {
    "objectID": "slides/08-slides.html#review-of-pysparksql-cheatsheet",
    "href": "slides/08-slides.html#review-of-pysparksql-cheatsheet",
    "title": "Lecture 8",
    "section": "Review of PySparkSQL Cheatsheet",
    "text": "Review of PySparkSQL Cheatsheet\nhttps://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf"
  },
  {
    "objectID": "slides/08-slides.html#collect-caution",
    "href": "slides/08-slides.html#collect-caution",
    "title": "Lecture 8",
    "section": "collect CAUTION",
    "text": "collect CAUTION"
  },
  {
    "objectID": "slides/08-slides.html#understanding-how-the-cluster-is-running-your-job",
    "href": "slides/08-slides.html#understanding-how-the-cluster-is-running-your-job",
    "title": "Lecture 8",
    "section": "Understanding how the cluster is running your job",
    "text": "Understanding how the cluster is running your job\nSpark Application UI shows important facts about you Spark job:\n\nEvent timeline for each stage of your work\nDirected acyclical graph (DAG) of your job\nSpark job history\nStatus of Spark executors\nPhysical / logical plans for any SQL queries\n\nTool to confirm you are getting the horizontal scaling that you need!\nAdapted from AWS Glue Spark UI docs and Spark UI docs"
  },
  {
    "objectID": "slides/08-slides.html#spark-ui---event-timeline",
    "href": "slides/08-slides.html#spark-ui---event-timeline",
    "title": "Lecture 8",
    "section": "Spark UI - Event timeline",
    "text": "Spark UI - Event timeline"
  },
  {
    "objectID": "slides/08-slides.html#spark-ui---dag",
    "href": "slides/08-slides.html#spark-ui---dag",
    "title": "Lecture 8",
    "section": "Spark UI - DAG",
    "text": "Spark UI - DAG"
  },
  {
    "objectID": "slides/08-slides.html#spark-ui---job-history",
    "href": "slides/08-slides.html#spark-ui---job-history",
    "title": "Lecture 8",
    "section": "Spark UI - Job History",
    "text": "Spark UI - Job History"
  },
  {
    "objectID": "slides/08-slides.html#spark-ui---executors",
    "href": "slides/08-slides.html#spark-ui---executors",
    "title": "Lecture 8",
    "section": "Spark UI - Executors",
    "text": "Spark UI - Executors"
  },
  {
    "objectID": "slides/08-slides.html#spark-ui---sql",
    "href": "slides/08-slides.html#spark-ui---sql",
    "title": "Lecture 8",
    "section": "Spark UI - SQL",
    "text": "Spark UI - SQL"
  },
  {
    "objectID": "slides/08-slides.html#udf-workflow",
    "href": "slides/08-slides.html#udf-workflow",
    "title": "Lecture 8",
    "section": "UDF Workflow",
    "text": "UDF Workflow\n\n\n\nAdapted from UDFs in Spark"
  },
  {
    "objectID": "slides/08-slides.html#udf-example",
    "href": "slides/08-slides.html#udf-example",
    "title": "Lecture 8",
    "section": "UDF Example",
    "text": "UDF Example\nProblem: make a new column with ages for adults-only\n+-------+--------------+\n|room_id|   guests_ages|\n+-------+--------------+\n|      1|  [18, 19, 17]|\n|      2|   [25, 27, 5]|\n|      3|[34, 38, 8, 7]|\n+-------+--------------+"
  },
  {
    "objectID": "slides/08-slides.html#udf-code-solution",
    "href": "slides/08-slides.html#udf-code-solution",
    "title": "Lecture 8",
    "section": "UDF Code Solution",
    "text": "UDF Code Solution\n\nfrom pyspark.sql.functions import udf, col\n\n@udf(\"array<integer>\")\n   def filter_adults(elements):\n   return list(filter(lambda x: x >= 18, elements))\n\n# alternatively\nfrom pyspark.sql.types IntegerType, ArrayType\n@udf(returnType=ArrayType(IntegerType()))\ndef filter_adults(elements):\n   return list(filter(lambda x: x >= 18, elements))\n\n+-------+----------------+------------+\n|room_id| guests_ages    | adults_ages|\n+-------+----------------+------------+\n| 1     | [18, 19, 17]   |    [18, 19]|\n| 2     | [25, 27, 5]    |    [25, 27]|\n| 3     | [34, 38, 8, 7] |    [34, 38]|\n| 4     |[56, 49, 18, 17]|[56, 49, 18]|\n+-------+----------------+------------+"
  },
  {
    "objectID": "slides/08-slides.html#alternative-to-spark-udf",
    "href": "slides/08-slides.html#alternative-to-spark-udf",
    "title": "Lecture 8",
    "section": "Alternative to Spark UDF",
    "text": "Alternative to Spark UDF\n\n# Spark 3.1\nfrom pyspark.sql.functions import col, filter, lit\n\ndf.withColumn('adults_ages',\n              filter(col('guests_ages'), lambda x: x >= lit(18))).show()"
  },
  {
    "objectID": "slides/08-slides.html#udf-speed-comparison",
    "href": "slides/08-slides.html#udf-speed-comparison",
    "title": "Lecture 8",
    "section": "UDF Speed Comparison",
    "text": "UDF Speed Comparison\n\n\n\n\nCosts:\n\nSerialization/deserialization (think pickle files)\nData movement between JVM and Python\nLess Spark optimization possible\n\nOther ways to make your Spark jobs faster source:\n\nCache/persist your data into memory\nUsing Spark DataFrames over Spark RDDs\nUsing Spark SQL functions before jumping into UDFs\nSave to serialized data formats like Parquet"
  },
  {
    "objectID": "slides/08-slides.html#pandas-udf",
    "href": "slides/08-slides.html#pandas-udf",
    "title": "Lecture 8",
    "section": "Pandas UDF",
    "text": "Pandas UDF\nFrom PySpark docs - Pandas UDFs are user defined functions that are executed by Spark using Arrow to transfer data and Pandas to work with the data, which allows vectorized operations. A Pandas UDF is defined using the pandas_udf as a decorator or to wrap the function, and no additional configuration is required. A Pandas UDF behaves as a regular PySpark function API in general.\n\n@pandas_udf(\"string\")\ndef to_upper(s: pd.Series) -> pd.Series:\n    return s.str.upper()\n\ndf = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\ndf.select(to_upper(\"name\")).show()\n+--------------+\n|to_upper(name)|\n+--------------+\n|      JOHN DOE|\n+--------------+\n\n\n@pandas_udf(\"first string, last string\")\ndef split_expand(s: pd.Series) -> pd.DataFrame:\n    return s.str.split(expand=True)\n\n\ndf = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\ndf.select(split_expand(\"name\")).show()\n+------------------+\n|split_expand(name)|\n+------------------+\n|       [John, Doe]|\n+------------------+\n\nhttps://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.pandas_udf.html"
  },
  {
    "objectID": "slides/08-slides.html#overview-of-ml",
    "href": "slides/08-slides.html#overview-of-ml",
    "title": "Lecture 8",
    "section": "Overview of ML",
    "text": "Overview of ML"
  },
  {
    "objectID": "slides/08-slides.html#the-advanced-analyticsml-process",
    "href": "slides/08-slides.html#the-advanced-analyticsml-process",
    "title": "Lecture 8",
    "section": "The advanced analytics/ML process",
    "text": "The advanced analytics/ML process\n\n\nGather and collect data\nClean and inspect data\nPerform feature engineering\nSplit data into train/test\nEvaluate and compare models"
  },
  {
    "objectID": "slides/08-slides.html#what-is-mllib",
    "href": "slides/08-slides.html#what-is-mllib",
    "title": "Lecture 8",
    "section": "What is MLlib",
    "text": "What is MLlib\n\nCapabilities\n\nGather and clean data\nPerform feature engineering\nPerform feature selection\nTrain and tune models\nPut models in production\n\nAPI is divided into two packages\norg.apache.spark.ml (High level API) - Provides higher level API built on top of DataFrames - Allows the construction of ML pipelines\norg.apache.spark.mllib (Predates DataFrames) - Original API built on top of RDDs\n\n\nInspired by scikit-learn\n\nDataFrame\nTransformer\nEstimator\nPipeline\nParameter"
  },
  {
    "objectID": "slides/08-slides.html#transformers",
    "href": "slides/08-slides.html#transformers",
    "title": "Lecture 8",
    "section": "Transformers",
    "text": "Transformers\n\nTransformers take DataFrames as input, and return a new DataFrame as output. Transformers do not learn any parameters from the data, they simply apply rule-based transformations to either prepare data for model training or generate predictions using a trained model.\nTransformers are run using the .transform() method"
  },
  {
    "objectID": "slides/08-slides.html#some-examples-of-transformers",
    "href": "slides/08-slides.html#some-examples-of-transformers",
    "title": "Lecture 8",
    "section": "Some Examples of Transformers",
    "text": "Some Examples of Transformers\n\nConverting categorical variables to numeric (must do this)\n\nStringIndexer\nOneHotEncoder can act on multiple columns at a time\n\nData Normalization\n\nNormalizer\nStandardScaler\n\nString Operations\n\nTokenizer\nStopWordsRemover\nPCA\n\nConverting continuous to discrete\n\nBucketizer\nQuantileDiscretizer\n\nMany more\n\nSpark 2.4.7: http://spark.apache.org/docs/2.4.7/ml-guide.html\nSpark 3.1.1: http://spark.apache.org/docs/3.1.1/ml-guide.html"
  },
  {
    "objectID": "slides/08-slides.html#mllib-algorithms-for-machine-learning-models-need-single-numeric-features-column-as-input",
    "href": "slides/08-slides.html#mllib-algorithms-for-machine-learning-models-need-single-numeric-features-column-as-input",
    "title": "Lecture 8",
    "section": "MLlib algorithms for machine learning models need single, numeric features column as input",
    "text": "MLlib algorithms for machine learning models need single, numeric features column as input\nEach row in this column contains a vector of data points corresponding to the set of features used for prediction.\n\nUse the VectorAssembler transformer to create a single vector column from a list of columns.\nAll categorical data needs to be numeric for machine learning\n\n\n# Example from Spark docs\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\ndataset = spark.createDataFrame(\n    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],\n    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\n\nassembler = VectorAssembler(\n    inputCols=[\"hour\", \"mobile\", \"userFeatures\"],\n    outputCol=\"features\")\n\noutput = assembler.transform(dataset)\nprint(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\noutput.select(\"features\", \"clicked\").show(truncate=False)"
  },
  {
    "objectID": "slides/08-slides.html#estimators",
    "href": "slides/08-slides.html#estimators",
    "title": "Lecture 8",
    "section": "Estimators",
    "text": "Estimators\n\nEstimators learn (or “fit”) parameters from your DataFrame via the .fit() method, and return a model which is a Transformer"
  },
  {
    "objectID": "slides/08-slides.html#pipelines",
    "href": "slides/08-slides.html#pipelines",
    "title": "Lecture 8",
    "section": "Pipelines",
    "text": "Pipelines"
  },
  {
    "objectID": "slides/08-slides.html#pipelines-1",
    "href": "slides/08-slides.html#pipelines-1",
    "title": "Lecture 8",
    "section": "Pipelines",
    "text": "Pipelines\n\nPipelines combine multiple steps into a single workflow that can be easily run. * Data cleaning and feature processing via transformers, using stages * Model definition * Run the pipeline to do all transformations and fit the model\nThe Pipeline constructor takes an array of pipeline stages"
  },
  {
    "objectID": "slides/08-slides.html#why-pipelines",
    "href": "slides/08-slides.html#why-pipelines",
    "title": "Lecture 8",
    "section": "Why Pipelines?",
    "text": "Why Pipelines?\n\nCleaner Code: Accounting for data at each step of preprocessing can get messy. With a Pipeline, you won’t need to manually keep track of your training and validation data at each step.\nFewer Bugs: There are fewer opportunities to misapply a step or forget a preprocessing step.\nEasier to Productionize: It can be surprisingly hard to transition a model from a prototype to something deployable at scale, but Pipelines can help.\nMore Options for Model Validation: We can easily apply cross-validation and other techniques to our Pipelines."
  },
  {
    "objectID": "slides/08-slides.html#example",
    "href": "slides/08-slides.html#example",
    "title": "Lecture 8",
    "section": "Example",
    "text": "Example\nUsing the HMP Dataset. The structure of the dataset looks like this:\n\n# read data from text file and split each line into words\ndf.printSchema()\n\nroot  \n|-- x: integer (nullable = true)  \n|-- y: integer (nullable = true)  \n|-- z: integer (nullable = true)  \n|-- class: string (nullable = false)  \n|-- source: string (nullable = false)"
  },
  {
    "objectID": "slides/08-slides.html#lets-transform-strings-to-numbers",
    "href": "slides/08-slides.html#lets-transform-strings-to-numbers",
    "title": "Lecture 8",
    "section": "Let’s transform strings to numbers",
    "text": "Let’s transform strings to numbers\n\nThe StringIndexer is an estimator having both fit and transform methods.\nTo create a StringIndexer object (indexer), pass the class column as inputCol, and classIndex as outputCol.\nThen we fit the DataFrame to the indexer (to run the estimator), and transform the DataFrame. This creates a brand new DataFrame (indexed), which we can see below, containing the classIndex additional column.\n\n\nfrom pyspark.ml.feature import StringIndexer\n\nindexer = StringIndexer(inputCol = 'class', outputCol = 'classIndex')\nindexed = indexer.fit(df).transform(df)  # This is a new data frame\n\n# Let's see it\nindexed.show(5)\n\n+---+---+---+-----------+--------------------+----------+\n|  x|  y|  z|      class|              source|classIndex|\n+---+---+---+-----------+--------------------+----------+\n| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|\n| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|\n| 28| 38| 52|Drink_glass|Accelerometer-201...|       2.0|\n| 29| 37| 51|Drink_glass|Accelerometer-201...|       2.0|\n| 30| 38| 52|Drink_glass|Accelerometer-201...|       2.0|\n+---+---+---+-----------+--------------------+----------+\nonly showing top 5 rows"
  },
  {
    "objectID": "slides/08-slides.html#one-hot-encode-categorical-variables",
    "href": "slides/08-slides.html#one-hot-encode-categorical-variables",
    "title": "Lecture 8",
    "section": "One-Hot Encode categorical variables",
    "text": "One-Hot Encode categorical variables\n\nUnlike the StringIndexer, OneHotEncoder is a pure transformer, having only the transform method. It uses the same syntax of inputCol and outputCol.\nOneHotEncoder creates a brand new DataFrame (encoded), with a category_vec column added to the previous DataFrame(indexed).\nOneHotEncoder doesn’t return several columns containing only zeros and ones; it returns a sparse-vector as seen in the categoryVec column. Thus, for the ‘Drink_glass’ class above, SparkML returns a sparse-vector that basically says there are 13 elements, and at position 2, the class value exists(1.0).\n\n\nfrom pyspark.ml.feature import OneHotEncoder\n\n# The OneHotEncoder is a pure transformer object. it does not use the fit()\nencoder = OneHotEncoder(inputCol = 'classIndex', outputCol = 'categoryVec')\nencoded = encoder.transform(indexed)  # This is a new data frame\nencoded.show(5, False)\n\n+---+---+---+-----------+----------------------------------------------------+----------+--------------+\n|x  |y  |z  |class      |source                                              |classIndex|categoryVec   |\n+---+---+---+-----------+----------------------------------------------------+----------+--------------+\n|29 |39 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n|29 |39 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n|28 |38 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n|29 |37 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n|30 |38 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n+---+---+---+-----------+----------------------------------------------------+----------+--------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "slides/08-slides.html#assembling-the-feature-vector",
    "href": "slides/08-slides.html#assembling-the-feature-vector",
    "title": "Lecture 8",
    "section": "Assembling the feature vector",
    "text": "Assembling the feature vector\n\nThe VectorAssembler object gets initialized with the same syntax as StringIndexer and OneHotEncoder.\nThe list ['x', 'y', 'z'] is paseed to inputCols, and we specify outputCol = ‘features’.\nThis is also a pure transformer.\n\n\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n# VectorAssembler creates vectors from ordinary data types for us\n\nvectorAssembler = VectorAssembler(inputCols = ['x','y','z'], outputCol = 'features')\n# Now we use the vectorAssembler object to transform our last updated dataframe\n\nfeatures_vectorized = vectorAssembler.transform(encoded)  # note this is a new df\n\nfeatures_vectorized.show(5, False)\n\n+---+---+---+-----------+----------------------------------------------------+----------+--------------+----------------+\n|x  |y  |z  |class      |source                                              |classIndex|categoryVec   |features        |\n+---+---+---+-----------+----------------------------------------------------+----------+--------------+----------------+\n|29 |39 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[29.0,39.0,51.0]|\n|29 |39 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[29.0,39.0,51.0]|\n|28 |38 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[28.0,38.0,52.0]|\n|29 |37 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[29.0,37.0,51.0]|\n|30 |38 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[30.0,38.0,52.0]|\n+---+---+---+-----------+----------------------------------------------------+----------+--------------+----------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "slides/08-slides.html#normalizing-the-dataset",
    "href": "slides/08-slides.html#normalizing-the-dataset",
    "title": "Lecture 8",
    "section": "Normalizing the dataset",
    "text": "Normalizing the dataset\n\nNormalizer like all transformers have consistent syntax. Looking at the Normalizer object, it contains the parameter p=1.0 (the default norm value for Pyspark Normalizer is p=2.0.)\np=1.0 uses Manhattan Distance\np=2.0 uses Euclidean Distance\n\n\nfrom pyspark.ml.feature import Normalizer\nnormalizer = Normalizer(inputCol = 'features', outputCol = 'features_norm', p=1.0)  # Manhattan Distance\nnormalized_data = normalizer.transform(features_vectorized) # New data frame too.\n\nnormalized_data.show(5)\n>>\n+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n|  x|  y|  z|      class|              source|classIndex|   categoryVec|        features|       features_norm|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,51.0]|[0.24369747899159...|\n| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,51.0]|[0.24369747899159...|\n| 28| 38| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[28.0,38.0,52.0]|[0.23728813559322...|\n| 29| 37| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,37.0,51.0]|[0.24786324786324...|\n| 30| 38| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[30.0,38.0,52.0]|[0.25,0.316666666...|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "slides/08-slides.html#running-the-transformation-pipeline",
    "href": "slides/08-slides.html#running-the-transformation-pipeline",
    "title": "Lecture 8",
    "section": "Running the transformation Pipeline",
    "text": "Running the transformation Pipeline\nThe Pipeline constructor below takes an array of Pipeline stages we pass to it. Here we pass the 4 stages defined earlier, in the right sequence, one after another.\n\nfrom pyspark.ml import Pipeline\npipeline = Pipeline(stages = [indexer,encoder,vectorAssembler,normalizer])\n\nNow the Pipeline object fit to our original DataFrame df\n\ndata_model = pipeline.fit(df)\n\nFinally, we transform our DataFrame df using the Pipeline Object.\n\npipelined_data = data_model.transform(df)\n\npipelined_data.show(5)\n\n+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n|  x|  y|  z|      class|              source|classIndex|   categoryVec|        features|       features_norm|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,51.0]|[0.24369747899159...|\n| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,51.0]|[0.24369747899159...|\n| 28| 38| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[28.0,38.0,52.0]|[0.23728813559322...|\n| 29| 37| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,37.0,51.0]|[0.24786324786324...|\n| 30| 38| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[30.0,38.0,52.0]|[0.25,0.316666666...|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "slides/08-slides.html#one-last-step-before-training",
    "href": "slides/08-slides.html#one-last-step-before-training",
    "title": "Lecture 8",
    "section": "One last step before training",
    "text": "One last step before training\n\n# first let's list out the columns we want to drop\ncols_to_drop = ['x','y','z','class','source','classIndex','features']\n\n# Next let's use a list comprehension with conditionals to select cols we need\nselected_cols = [col for col in pipelined_data.columns if col not in cols_to_drop]\n\n# Let's define a new train_df with only the categoryVec and features_norm cols\ndf_train = pipelined_data.select(selected_cols)\n\n# Let's see our training dataframe.\ndf_train.show(5)\n\n+--------------+--------------------+\n|   categoryVec|       features_norm|\n+--------------+--------------------+\n|(13,[2],[1.0])|[0.24369747899159...|\n|(13,[2],[1.0])|[0.24369747899159...|\n|(13,[2],[1.0])|[0.23728813559322...|\n|(13,[2],[1.0])|[0.24786324786324...|\n|(13,[2],[1.0])|[0.25,0.316666666...|\nonly showing top 5 rows\n\nYou now have a DataFrame optimized for SparkML!"
  },
  {
    "objectID": "slides/08-slides.html#machine-learning-models-in-spark",
    "href": "slides/08-slides.html#machine-learning-models-in-spark",
    "title": "Lecture 8",
    "section": "Machine Learning Models in Spark",
    "text": "Machine Learning Models in Spark\nThere are many Spark machine learning models\nRegression\n\nLinear regression - what is the optimization method?\nSurvival regression\nGenearlized linear model (GLM)\nRandom forest regression\n\nClassification\n\nLogistic regression\nGradient-boosted tree model (GBM)\nNaive Bayes\nMultilayer perception (neural network!)\n\nOther\n\nClustering (K-means, LDA, GMM)\nAssociation rule mining"
  },
  {
    "objectID": "slides/08-slides.html#building-your-model",
    "href": "slides/08-slides.html#building-your-model",
    "title": "Lecture 8",
    "section": "Building your Model",
    "text": "Building your Model\nFeature selection using easy R-like formulas\n\n# Example from Spark docs\nfrom pyspark.ml.feature import RFormula\n\ndataset = spark.createDataFrame(\n    [(7, \"US\", 18, 1.0),\n     (8, \"CA\", 12, 0.0),\n     (9, \"NZ\", 15, 0.0)],\n    [\"id\", \"country\", \"hour\", \"clicked\"])\n\nformula = RFormula(\n    formula=\"clicked ~ country + hour\",\n    featuresCol=\"features\",\n    labelCol=\"label\")\n\noutput = formula.fit(dataset).transform(dataset)\noutput.select(\"features\", \"label\").show()"
  },
  {
    "objectID": "slides/08-slides.html#what-if-you-have-too-many-columns-for-your-model",
    "href": "slides/08-slides.html#what-if-you-have-too-many-columns-for-your-model",
    "title": "Lecture 8",
    "section": "What if you have too many columns for your model?",
    "text": "What if you have too many columns for your model?\nEvaluating 1,000s or 10,000s of variables?"
  },
  {
    "objectID": "slides/08-slides.html#chi-squared-selector",
    "href": "slides/08-slides.html#chi-squared-selector",
    "title": "Lecture 8",
    "section": "Chi-Squared Selector",
    "text": "Chi-Squared Selector\nPick categorical variables that are most dependent on the response variable\nCan even check the p-values of specific variables!\n\n# Example from Spark docs\nfrom pyspark.ml.feature import ChiSqSelector\nfrom pyspark.ml.linalg import Vectors\n\ndf = spark.createDataFrame([\n    (7, Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0,),\n    (8, Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0,),\n    (9, Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0,)], [\"id\", \"features\", \"clicked\"])\n\nselector = ChiSqSelector(numTopFeatures=1, featuresCol=\"features\",\n                         outputCol=\"selectedFeatures\", labelCol=\"clicked\")\n\nresult = selector.fit(df).transform(df)\n\nprint(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\nresult.show()"
  },
  {
    "objectID": "slides/08-slides.html#tuning-your-model",
    "href": "slides/08-slides.html#tuning-your-model",
    "title": "Lecture 8",
    "section": "Tuning your model",
    "text": "Tuning your model\nPart 1\n\n# Example from Spark docs\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n\n# Prepare training and test data.\ndata = spark.read.format(\"libsvm\")\\\n    .load(\"data/mllib/sample_linear_regression_data.txt\")\ntrain, test = data.randomSplit([0.9, 0.1], seed=12345)\n\nlr = LinearRegression(maxIter=10)\n\n# We use a ParamGridBuilder to construct a grid of parameters to search over.\n# TrainValidationSplit will try all combinations of values and determine best model using\n# the evaluator.\nparamGrid = ParamGridBuilder()\\\n    .addGrid(lr.regParam, [0.1, 0.01]) \\\n    .addGrid(lr.fitIntercept, [False, True])\\\n    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n    .build()"
  },
  {
    "objectID": "slides/08-slides.html#tuning-your-model-1",
    "href": "slides/08-slides.html#tuning-your-model-1",
    "title": "Lecture 8",
    "section": "Tuning your model",
    "text": "Tuning your model\nPart 2\n\n# In this case the estimator is simply the linear regression.\n# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\ntvs = TrainValidationSplit(estimator=lr,\n                           estimatorParamMaps=paramGrid,\n                           evaluator=RegressionEvaluator(),\n                           # 80% of the data will be used for training, 20% for validation.\n                           trainRatio=0.8)\n\n# Run TrainValidationSplit, and choose the best set of parameters.\nmodel = tvs.fit(train)\n\n# Make predictions on test data. model is the model with combination of parameters\n# that performed best.\nmodel.transform(test)\\\n    .select(\"features\", \"label\", \"prediction\")\\\n    .show()"
  },
  {
    "objectID": "slides/09-slides.html#aws-academy",
    "href": "slides/09-slides.html#aws-academy",
    "title": "Lecture 9",
    "section": "AWS Academy",
    "text": "AWS Academy\n\nCredit limit - $100\nCourse numbers:\n\nCourse #1 - 24178\nCourse #2 - 27354\nCourse #3 - 22802\nCourse #4 - 26418\n\n\nSTAY WITH COURSE 24178 UNLESS YOU HAVE RUN OUT OF CREDITS OR >$90 USED!\nNote that you will have to repeat several setup steps:\n\nsecurity group\nEC2 keypair uploading (the AWS part only)\nany S3 uploading or copying as well as bucket creation as necessary\nEMR configuration"
  },
  {
    "objectID": "slides/09-slides.html#review-of-file-systems",
    "href": "slides/09-slides.html#review-of-file-systems",
    "title": "Lecture 9",
    "section": "Review of File Systems",
    "text": "Review of File Systems\nWhat are the possible file system options for each item: S3, HDFS, Local file system\n\nhadoop jar /usr/lib/hadoop/hadoop-streaming.jar #1\n\n\n\n-files basic-mapper.py,basic-reducer.py #2\n\n\n\n-input /user/hadoop/in_data #3\n-output /user/hadoop/in_data #3\n\n\n\n-mapper basic-mapper.py #4\n-reducer basic-reducer.py #4\n\n\n\nLocal file system\n\n\n\n\nLocal file system/S3/ OR HDFS – UPDATE\n\n\n\n\nHDFS or S3\n\n\n\n\nUPDATE – It’s complicated - “The -files option creates a symlink in the current working directory of the tasks that points to the local copy of the file.” Location of #4 can be:\n\n\nSymlinked file (one you passed with -files) in the pwd in distributed containers\nExecutable path (local to a worker) that is consistent across all workers on the cluster"
  },
  {
    "objectID": "slides/09-slides.html#connected-and-extensible",
    "href": "slides/09-slides.html#connected-and-extensible",
    "title": "Lecture 9",
    "section": "Connected and extensible",
    "text": "Connected and extensible"
  },
  {
    "objectID": "slides/09-slides.html#caching-and-persistence",
    "href": "slides/09-slides.html#caching-and-persistence",
    "title": "Lecture 9",
    "section": "Caching and Persistence",
    "text": "Caching and Persistence\nBy default, RDDs are recomputed every time you run an action on them. This can be expensive (in time) if you need to use a dataset more than once.\nSpark allows you to control what is cached in memory.\nTo tell spark to cache an object in memory, use persist() or cache():\n\ncache(): is a shortcut for using default storage level, which is memory only\npersist(): can be customized to other ways to persist data (including both memory and/or disk)\n\n\n# caches error RDD in memory, but only after an action is run\nerrors = logs.filter(lambda x: \"error\" in x and \"2019-12\" in x).cache()"
  },
  {
    "objectID": "slides/09-slides.html#collect-caution",
    "href": "slides/09-slides.html#collect-caution",
    "title": "Lecture 9",
    "section": "collect CAUTION",
    "text": "collect CAUTION"
  },
  {
    "objectID": "slides/09-slides.html#spark-ui---executors",
    "href": "slides/09-slides.html#spark-ui---executors",
    "title": "Lecture 9",
    "section": "Spark UI - Executors",
    "text": "Spark UI - Executors"
  },
  {
    "objectID": "slides/09-slides.html#udf-speed-comparison",
    "href": "slides/09-slides.html#udf-speed-comparison",
    "title": "Lecture 9",
    "section": "UDF Speed Comparison",
    "text": "UDF Speed Comparison\n\n\n\n\nCosts:\n\nSerialization/deserialization (think pickle files)\nData movement between JVM and Python\nLess Spark optimization possible\n\nOther ways to make your Spark jobs faster source:\n\nCache/persist your data into memory\nUsing Spark DataFrames over Spark RDDs\nUsing Spark SQL functions before jumping into UDFs\nSave to serialized data formats like Parquet"
  },
  {
    "objectID": "slides/09-slides.html#transformers",
    "href": "slides/09-slides.html#transformers",
    "title": "Lecture 9",
    "section": "Transformers",
    "text": "Transformers\n\nTransformers take DataFrames as input, and return a new DataFrame as output. Transformers do not learn any parameters from the data, they simply apply rule-based transformations to either prepare data for model training or generate predictions using a trained model.\nTransformers are run using the .transform() method"
  },
  {
    "objectID": "slides/09-slides.html#estimators",
    "href": "slides/09-slides.html#estimators",
    "title": "Lecture 9",
    "section": "Estimators",
    "text": "Estimators\n\nEstimators learn (or “fit”) parameters from your DataFrame via the .fit() method, and return a model which is a Transformer"
  },
  {
    "objectID": "slides/09-slides.html#pipelines",
    "href": "slides/09-slides.html#pipelines",
    "title": "Lecture 9",
    "section": "Pipelines",
    "text": "Pipelines"
  },
  {
    "objectID": "slides/09-slides.html#pipelines-1",
    "href": "slides/09-slides.html#pipelines-1",
    "title": "Lecture 9",
    "section": "Pipelines",
    "text": "Pipelines\n\nPipelines combine multiple steps into a single workflow that can be easily run. * Data cleaning and feature processing via transformers, using stages * Model definition * Run the pipeline to do all transformations and fit the model\nThe Pipeline constructor takes an array of pipeline stages"
  },
  {
    "objectID": "slides/09-slides.html#why-pipelines",
    "href": "slides/09-slides.html#why-pipelines",
    "title": "Lecture 9",
    "section": "Why Pipelines?",
    "text": "Why Pipelines?\n\nCleaner Code: Accounting for data at each step of preprocessing can get messy. With a Pipeline, you won’t need to manually keep track of your training and validation data at each step.\nFewer Bugs: There are fewer opportunities to misapply a step or forget a preprocessing step.\nEasier to Productionize: It can be surprisingly hard to transition a model from a prototype to something deployable at scale, but Pipelines can help.\nMore Options for Model Validation: We can easily apply cross-validation and other techniques to our Pipelines."
  },
  {
    "objectID": "slides/09-slides.html#reddit-data",
    "href": "slides/09-slides.html#reddit-data",
    "title": "Lecture 9",
    "section": "Reddit Data!",
    "text": "Reddit Data!"
  },
  {
    "objectID": "slides/09-slides.html#comparison-of-social-data-infrastructure",
    "href": "slides/09-slides.html#comparison-of-social-data-infrastructure",
    "title": "Lecture 9",
    "section": "Comparison of social data infrastructure",
    "text": "Comparison of social data infrastructure"
  },
  {
    "objectID": "slides/09-slides.html#data-dictionary",
    "href": "slides/09-slides.html#data-dictionary",
    "title": "Lecture 9",
    "section": "Data Dictionary",
    "text": "Data Dictionary"
  },
  {
    "objectID": "slides/09-slides.html#next-steps",
    "href": "slides/09-slides.html#next-steps",
    "title": "Lecture 9",
    "section": "Next Steps",
    "text": "Next Steps\n\nStart working with your group to conduct EDA on the dataset\nInitial data questions - understand the structure of the data!\nDefine your business questions - how do you plan to work with this data\nLook into external data to merge with your Reddit data\n\nBasic requirements will be published on Thursday\nDeadline for Project Milestone 1: EDA Monday November 7"
  },
  {
    "objectID": "slides/09-slides.html#spark-methods-for-text-analytics",
    "href": "slides/09-slides.html#spark-methods-for-text-analytics",
    "title": "Lecture 9",
    "section": "Spark methods for text analytics",
    "text": "Spark methods for text analytics\n\nString manipulation SQL functions: F.length(col), F.substring(str, pos, len), F.trim(col), F.upper(col), …\nML transformers: Tokenizer(), StopWordsRemover(), Word2Vec(), CountVectorizer()"
  },
  {
    "objectID": "slides/09-slides.html#tokenizer",
    "href": "slides/09-slides.html#tokenizer",
    "title": "Lecture 9",
    "section": "Tokenizer",
    "text": "Tokenizer\n\n>>> df = spark.createDataFrame([(\"a b c\",)], [\"text\"])\n\n>>> tokenizer = Tokenizer(outputCol=\"words\")\n\n>>> tokenizer.setInputCol(\"text\")\n\n>>> tokenizer.transform(df).head()\n\nRow(text='a b c', words=['a', 'b', 'c'])\n\n>>> # Change a parameter.\n>>> tokenizer.setParams(outputCol=\"tokens\").transform(df).head()\nRow(text='a b c', tokens=['a', 'b', 'c'])\n\n>>> # Temporarily modify a parameter.\n>>> tokenizer.transform(df, {tokenizer.outputCol: \"words\"}).head()\nRow(text='a b c', words=['a', 'b', 'c'])\n\n>>> tokenizer.transform(df).head()\nRow(text='a b c', tokens=['a', 'b', 'c'])\n\ndoc"
  },
  {
    "objectID": "slides/09-slides.html#stopwordsremover",
    "href": "slides/09-slides.html#stopwordsremover",
    "title": "Lecture 9",
    "section": "StopWordsRemover",
    "text": "StopWordsRemover\n\n>>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], [\"text\"])\n>>> remover = StopWordsRemover(stopWords=[\"b\"])\n>>> remover.setInputCol(\"text\")\n>>> remover.setOutputCol(\"words\")\n>>> remover.transform(df).head().words == ['a', 'c']\nTrue\n>>> stopWordsRemoverPath = temp_path + \"/stopwords-remover\"\n>>> remover.save(stopWordsRemoverPath)\n>>> loadedRemover = StopWordsRemover.load(stopWordsRemoverPath)\n>>> loadedRemover.getStopWords() == remover.getStopWords()\nTrue\n>>> loadedRemover.getCaseSensitive() == remover.getCaseSensitive()\nTrue\n>>> loadedRemover.transform(df).take(1) == remover.transform(df).take(1)\nTrue\n>>> df2 = spark.createDataFrame([([\"a\", \"b\", \"c\"], [\"a\", \"b\"])], [\"text1\", \"text2\"])\n>>> remover2 = StopWordsRemover(stopWords=[\"b\"])\n>>> remover2.setInputCols([\"text1\", \"text2\"]).setOutputCols([\"words1\", \"words2\"])\n>>> remover2.transform(df2).show()\n+---------+------+------+------+\n|    text1| text2|words1|words2|\n+---------+------+------+------+\n|[a, b, c]|[a, b]|[a, c]|   [a]|\n+---------+------+------+------+\n\ndoc"
  },
  {
    "objectID": "slides/09-slides.html#application-of-sentiment-analysis-in-pyspark",
    "href": "slides/09-slides.html#application-of-sentiment-analysis-in-pyspark",
    "title": "Lecture 9",
    "section": "Application of Sentiment Analysis in PySpark",
    "text": "Application of Sentiment Analysis in PySpark\nData: S&P company earnings calls - 10s of millions of text statements\nMethod: proximity-based sentiment analysis\nTech: PySpark, Python UDFs, lots of list comprehensions!\nOutcome: Time series trends of company concerns about supply chain related issues"
  },
  {
    "objectID": "slides/09-slides.html#application-continued",
    "href": "slides/09-slides.html#application-continued",
    "title": "Lecture 9",
    "section": "Application continued",
    "text": "Application continued\nExample: find the A’s within a certain distance of a Y\n# within2 -> 0\nX X X X Y X X X A\n# within2 -> 1\nX X A X Y X X X A\n# within2 -> 2\nX A X A Y A X X A\n# within4 -> 3\nA X A X Y X X X A\n\nCount the number of Y’s in the text that have an A near enough to them\nAggregate at scale!\nProject uses Tokenizer() and StopWordsRemover()\n\nProf. Anderson’s Paper\nCOVID paper that championed the method"
  },
  {
    "objectID": "slides/09-slides.html#jonsnowlabs-spark-nlp-package",
    "href": "slides/09-slides.html#jonsnowlabs-spark-nlp-package",
    "title": "Lecture 9",
    "section": "JonSnowLabs Spark NLP Package",
    "text": "JonSnowLabs Spark NLP Package\nWhy use UDFs, run proximity-based sentiment? Let’s use more advanced natural language processing packages!\nWhich libraries have the most features?"
  },
  {
    "objectID": "slides/09-slides.html#comparing-nlp-packages",
    "href": "slides/09-slides.html#comparing-nlp-packages",
    "title": "Lecture 9",
    "section": "Comparing NLP Packages",
    "text": "Comparing NLP Packages\nJust because it is scalable does not mean it lacks features!"
  },
  {
    "objectID": "slides/09-slides.html#most-popular-aiml-packages",
    "href": "slides/09-slides.html#most-popular-aiml-packages",
    "title": "Lecture 9",
    "section": "Most Popular AI/ML Packages",
    "text": "Most Popular AI/ML Packages"
  },
  {
    "objectID": "slides/09-slides.html#spark-nlp-is-faster-than-spacy",
    "href": "slides/09-slides.html#spark-nlp-is-faster-than-spacy",
    "title": "Lecture 9",
    "section": "Spark NLP is faster than SpaCy",
    "text": "Spark NLP is faster than SpaCy\nBenchmark for training a pipeline with sentence bounder, tokenizer, and POS tagger on a 4-core laptop\nWhy??\n\nWhole stage code generation, vectorized in-memory columnar data\nNo copying of text in memory\nExtensive profiling, config & code optimization of Spark and TensorFlow\nOptimized for training and inference\n\nAaaaannndddd…. it scales!"
  },
  {
    "objectID": "slides/09-slides.html#spark-nlp",
    "href": "slides/09-slides.html#spark-nlp",
    "title": "Lecture 9",
    "section": "Spark NLP",
    "text": "Spark NLP\n\nReusing the Spark ML Pipeline\n\nUnified NLP & ML pipelines\nEnd-to-end execution planning\nSerializable\nDistributable\n\nReusing NLP Functionality\n\nTF-IDF calculation\nString distance calculation\nStop word removal\nTopic modeling\nDistributed ML algorithms"
  },
  {
    "objectID": "slides/09-slides.html#spark-nlp-terminology",
    "href": "slides/09-slides.html#spark-nlp-terminology",
    "title": "Lecture 9",
    "section": "Spark NLP Terminology",
    "text": "Spark NLP Terminology\nAnnotators\n\nLike the ML tools we used in Spark\nAlways need input and output columns\nTwo flavors:\n\nApproach - like ML estimators that need a fit() method to make an Annotator Model or Transformer\nModel - like ML transformers and uses transform() method only\n\n\nAnnotator Models\n\nPretrained public versions of models available through .pretained() method\n\nQ: Do transformer ML methods ever replace or remove columns in a Spark DataFrame?\n\nNo!! They only add columns."
  },
  {
    "objectID": "slides/09-slides.html#spark-nlp-sentiment-example",
    "href": "slides/09-slides.html#spark-nlp-sentiment-example",
    "title": "Lecture 9",
    "section": "Spark NLP Sentiment Example",
    "text": "Spark NLP Sentiment Example"
  },
  {
    "objectID": "slides/09-slides.html#spark-nlp-pipeline-example",
    "href": "slides/09-slides.html#spark-nlp-pipeline-example",
    "title": "Lecture 9",
    "section": "Spark NLP Pipeline Example",
    "text": "Spark NLP Pipeline Example"
  },
  {
    "objectID": "slides/09-slides.html#spark-nlp-pipeline-types",
    "href": "slides/09-slides.html#spark-nlp-pipeline-types",
    "title": "Lecture 9",
    "section": "Spark NLP Pipeline Types",
    "text": "Spark NLP Pipeline Types\nSpark Pipeline\n\nEfficiently run on a whole Spark Dataframe\nDistributable on a cluster\nUses Spark tasks, optimizations & execution planning\nUsed by PretrainedPipeline.transform()\n\nLight Pipeline\n\nEfficiently run on a single sentence\nFaster than a Spark pipeline for up to 50,000 local documents\nEasiest way to publish a pipeline as an API\nUsed by PretrainedPipeline.annotate()\n\nRecursive Pipeline\n\nGive annotators access to other annotators in the same pipeline\nRequired when training your own models\n\nEssential Spark NLP reading"
  },
  {
    "objectID": "slides/09-slides.html#overall-code-example",
    "href": "slides/09-slides.html#overall-code-example",
    "title": "Lecture 9",
    "section": "Overall Code Example",
    "text": "Overall Code Example\n\n\nfrom pyspark.ml import Pipeline\n\ndocument_assembler = DocumentAssembler()\\\n .setInputCol(“text”)\\\n .setOutputCol(“document”)\n \nsentenceDetector = SentenceDetector()\\\n .setInputCols([“document”])\\\n .setOutputCol(“sentences”)\n \ntokenizer = Tokenizer() \\\n .setInputCols([“sentences”]) \\\n .setOutputCol(“token”)\n \nnormalizer = Normalizer()\\\n .setInputCols([“token”])\\\n .setOutputCol(“normal”)\n \nword_embeddings=WordEmbeddingsModel.pretrained()\\\n .setInputCols([“document”,”normal”])\\\n .setOutputCol(“embeddings”)\n\n\n\n\nnlpPipeline = Pipeline(stages=[\n document_assembler, \n sentenceDetector,\n tokenizer,\n normalizer,\n word_embeddings,\n ])\n\npipelineModel = nlpPipeline.fit(df)"
  },
  {
    "objectID": "slides/09-slides.html#code-example-for-documentassembler",
    "href": "slides/09-slides.html#code-example-for-documentassembler",
    "title": "Lecture 9",
    "section": "Code Example for DocumentAssembler",
    "text": "Code Example for DocumentAssembler\n\nimport sparknlp; from sparknlp.base import *\nfrom sparknlp.annotator import *; from pyspark.ml import Pipeline\n\ndata = spark.createDataFrame([[\"Spark NLP is an open-source text processing library.\"]]).toDF(\"text\")\ndocumentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n\nresult = documentAssembler.transform(data)\n\nresult.select(\"document\").show(truncate=False)\n+----------------------------------------------------------------------------------------------+\n|document                                                                                      |\n+----------------------------------------------------------------------------------------------+\n|[[document, 0, 51, Spark NLP is an open-source text processing library., [sentence -> 0], []]]|\n+----------------------------------------------------------------------------------------------+"
  },
  {
    "objectID": "slides/09-slides.html#continued",
    "href": "slides/09-slides.html#continued",
    "title": "Lecture 9",
    "section": "Continued",
    "text": "Continued\n\nresult.select(\"document\").printSchema\nroot\n |-- document: array (nullable = True)\n |    |-- element: struct (containsNull = True)\n |    |    |-- annotatorType: string (nullable = True)\n |    |    |-- begin: integer (nullable = False)\n |    |    |-- end: integer (nullable = False)\n |    |    |-- result: string (nullable = True)\n |    |    |-- metadata: map (nullable = True)\n |    |    |    |-- key: string\n |    |    |    |-- value: string (valueContainsNull = True)\n |    |    |-- embeddings: array (nullable = True)\n |    |    |    |-- element: float (containsNull = False)"
  },
  {
    "objectID": "slides/09-slides.html#using-huggingface-transformers-models-with-sparknlp",
    "href": "slides/09-slides.html#using-huggingface-transformers-models-with-sparknlp",
    "title": "Lecture 9",
    "section": "Using HuggingFace Transformers Models with SparkNLP",
    "text": "Using HuggingFace Transformers Models with SparkNLP\nhttps://github.com/JohnSnowLabs/spark-nlp/discussions/5669\n\nfrom transformers import TFDistilBertForSequenceClassification, DistilBertTokenizer \nfrom sparknlp.annotator import *\nfrom sparknlp.base import *\n\nMODEL_NAME = 'distilbert-base-uncased-finetuned-sst-2-english'\n\ntokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.save_pretrained('./{}_tokenizer/'.format(MODEL_NAME))\n\nmodel = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\nmodel.save_pretrained(\"./{}\".format(MODEL_NAME), saved_model=True)\n\nasset_path = '{}/saved_model/1/assets'.format(MODEL_NAME)\n!cp {MODEL_NAME}_tokenizer/vocab.txt {asset_path}\n\nsequenceClassifier = DistilBertForSequenceClassification.loadSavedModel(\n     '{}/saved_model/1'.format(MODEL_NAME),spark)\\\n     .setInputCols([\"document\",'token'])\\\n  .setOutputCol(\"class\").setCaseSensitive(True).setMaxSentenceLength(128)\n  \n#### WHERE IS THIS SAVING TO???\nsequenceClassifier.write().overwrite().save(\"./{}_spark_nlp\".format(MODEL_NAME))"
  },
  {
    "objectID": "slides/09-slides.html#readings",
    "href": "slides/09-slides.html#readings",
    "title": "Lecture 9",
    "section": "Readings",
    "text": "Readings\nRequired:\n\nSpark NLP Code Concepts\n\nEncouraged:\n\nSpark NLP Annotators\nIntro to Spark NLP"
  },
  {
    "objectID": "slides/10-slides.html#aws-academy",
    "href": "slides/10-slides.html#aws-academy",
    "title": "Lecture 10",
    "section": "AWS Academy",
    "text": "AWS Academy\n\nCredit limit - $100\nCourse numbers:\n\nCourse #1 - 24178\nCourse #2 - 27354\nCourse #3 - 22802\nCourse #4 - 26418\n\n\nSTAY WITH COURSE 24178 UNLESS YOU HAVE RUN OUT OF CREDITS OR >$90 USED!\nNote that you will have to repeat several setup steps:\n\nsecurity group\nEC2 keypair uploading (the AWS part only)\nsagemaker setup\nany S3 uploading or copying as well as bucket creation as necessary\nEMR configuration"
  },
  {
    "objectID": "slides/10-slides.html#connected-and-extensible",
    "href": "slides/10-slides.html#connected-and-extensible",
    "title": "Lecture 10",
    "section": "Connected and extensible",
    "text": "Connected and extensible"
  },
  {
    "objectID": "slides/10-slides.html#caching-and-persistence",
    "href": "slides/10-slides.html#caching-and-persistence",
    "title": "Lecture 10",
    "section": "Caching and Persistence",
    "text": "Caching and Persistence\nBy default, RDDs are recomputed every time you run an action on them. This can be expensive (in time) if you need to use a dataset more than once.\nSpark allows you to control what is cached in memory.\nTo tell spark to cache an object in memory, use persist() or cache():\n\ncache(): is a shortcut for using default storage level, which is memory only\npersist(): can be customized to other ways to persist data (including both memory and/or disk)\n\n\n# caches error RDD in memory, but only after an action is run\nerrors = logs.filter(lambda x: \"error\" in x and \"2019-12\" in x).cache()"
  },
  {
    "objectID": "slides/10-slides.html#collect-caution",
    "href": "slides/10-slides.html#collect-caution",
    "title": "Lecture 10",
    "section": "collect CAUTION",
    "text": "collect CAUTION"
  },
  {
    "objectID": "slides/10-slides.html#spark-ui---executors",
    "href": "slides/10-slides.html#spark-ui---executors",
    "title": "Lecture 10",
    "section": "Spark UI - Executors",
    "text": "Spark UI - Executors"
  },
  {
    "objectID": "slides/10-slides.html#udf-speed-comparison",
    "href": "slides/10-slides.html#udf-speed-comparison",
    "title": "Lecture 10",
    "section": "UDF Speed Comparison",
    "text": "UDF Speed Comparison\n\n\n\n\nCosts:\n\nSerialization/deserialization (think pickle files)\nData movement between JVM and Python\nLess Spark optimization possible\n\nOther ways to make your Spark jobs faster source:\n\nCache/persist your data into memory\nUsing Spark DataFrames over Spark RDDs\nUsing Spark SQL functions before jumping into UDFs\nSave to serialized data formats like Parquet"
  },
  {
    "objectID": "slides/10-slides.html#pipelines",
    "href": "slides/10-slides.html#pipelines",
    "title": "Lecture 10",
    "section": "Pipelines",
    "text": "Pipelines"
  },
  {
    "objectID": "slides/10-slides.html#most-popular-aiml-packages",
    "href": "slides/10-slides.html#most-popular-aiml-packages",
    "title": "Lecture 10",
    "section": "Most Popular AI/ML Packages",
    "text": "Most Popular AI/ML Packages"
  },
  {
    "objectID": "slides/10-slides.html#spark-nlp-terminology",
    "href": "slides/10-slides.html#spark-nlp-terminology",
    "title": "Lecture 10",
    "section": "Spark NLP Terminology",
    "text": "Spark NLP Terminology\nAnnotators\n\nLike the ML tools we used in Spark\nAlways need input and output columns\nTwo flavors:\n\nApproach - like ML estimators that need a fit() method to make an Annotator Model or Transformer\nModel - like ML transformers and uses transform() method only\n\n\nAnnotator Models\n\nPretrained public versions of models available through .pretained() method"
  },
  {
    "objectID": "slides/10-slides.html#reddit-data",
    "href": "slides/10-slides.html#reddit-data",
    "title": "Lecture 10",
    "section": "Reddit Data!",
    "text": "Reddit Data!"
  },
  {
    "objectID": "slides/10-slides.html#assignment-details",
    "href": "slides/10-slides.html#assignment-details",
    "title": "Lecture 10",
    "section": "Assignment Details",
    "text": "Assignment Details\nReviewing Project Milestone 1: EDA"
  },
  {
    "objectID": "slides/10-slides.html#open-qa",
    "href": "slides/10-slides.html#open-qa",
    "title": "Lecture 10",
    "section": "Open Q&A",
    "text": "Open Q&A\nQuestions about the project requirements?\nQuestions about using Databricks?"
  },
  {
    "objectID": "slides/10-slides.html#up-to-now-weve-worked-with-batch-data",
    "href": "slides/10-slides.html#up-to-now-weve-worked-with-batch-data",
    "title": "Lecture 10",
    "section": "Up to now, we’ve worked with batch data",
    "text": "Up to now, we’ve worked with batch data\nProcessing large, already collected, batches of data."
  },
  {
    "objectID": "slides/10-slides.html#batch-examples",
    "href": "slides/10-slides.html#batch-examples",
    "title": "Lecture 10",
    "section": "Batch examples",
    "text": "Batch examples\nExamples of batch data analysis?\n\n\nAnalysis of terabytes of logs collected over a long period of time\nAnalysis of code bases on GitHub or other large repositories of textual information such as Wikipedia\nNightly analysis on large data sets collected over a 24 hour period"
  },
  {
    "objectID": "slides/10-slides.html#streaming-examples",
    "href": "slides/10-slides.html#streaming-examples",
    "title": "Lecture 10",
    "section": "Streaming Examples",
    "text": "Streaming Examples\nExamples of streaming data analysis?\n\n\nCredit card fraud detection\nSensor data processing\nOnline advertising based on user actions\nSocial media notifications\n\n\n\nInternational Data Coporation (IDC) forecasts that by 2025 IoT devices will generate 79.4 zettabytes of data."
  },
  {
    "objectID": "slides/10-slides.html#how-do-we-work-with-streams",
    "href": "slides/10-slides.html#how-do-we-work-with-streams",
    "title": "Lecture 10",
    "section": "How do we work with streams?",
    "text": "How do we work with streams?\nProcessing every value coming from a stream of data. That is, data values that are constantly arriving"
  },
  {
    "objectID": "slides/10-slides.html#spark-solved-this-problem-by-creating-dstreams-using-microbatching",
    "href": "slides/10-slides.html#spark-solved-this-problem-by-creating-dstreams-using-microbatching",
    "title": "Lecture 10",
    "section": "Spark solved this problem by creating DStreams using microbatching",
    "text": "Spark solved this problem by creating DStreams using microbatching\nDStreams are represented as a sequence of RDDs.\n\n\n\nA StreamingContext object can be created from an existing SparkContext object.\n\nsc = ...\nssc = StreamingContext(sc, 1)"
  },
  {
    "objectID": "slides/10-slides.html#important-points-about-streamingcontext",
    "href": "slides/10-slides.html#important-points-about-streamingcontext",
    "title": "Lecture 10",
    "section": "Important points about StreamingContext",
    "text": "Important points about StreamingContext\n\nOnce the context has been started, no new streaming computations can be setup or added\nOnce a context has been stopped, it cannot be restarted\nOnly one StreamingContext can active with a Spark session at the same time\nstop() on the StreamingContext also stops the the SparkContext\nMultiple StreamingContext can be created as long as the previous one is stopped"
  },
  {
    "objectID": "slides/10-slides.html#dstreams-had-some-issues",
    "href": "slides/10-slides.html#dstreams-had-some-issues",
    "title": "Lecture 10",
    "section": "DStreams had some issues",
    "text": "DStreams had some issues\n\nLack of a single API for batch and stream processing: Even though DStreams and RDDs have consistent APIs (i.e., same operations and same semantics), developers still had to explicitly rewrite their code to use different classes when converting their batch jobs to streaming jobs.\nLack of separation between logical and physical plans: Spark Streaming executes the DStream operations in the same sequence in which they were specified by the developer. Since developers effectively specify the exact physical plan, there is no scope for automatic optimizations, and developers have to hand-optimize their code to get the best performance.\nLack of native support for event-time windows: DStreams define window operations based only on the time when each record is received by Spark Streaming (known as processing time). However, many use cases need to calculate windowed aggregates based on the time when the records were generated (known as event time) instead of when they were received or processed. The lack of native support of event-time windows made it hard for developers to build such pipelines with Spark Streaming."
  },
  {
    "objectID": "slides/10-slides.html#what-is-structured-streaming",
    "href": "slides/10-slides.html#what-is-structured-streaming",
    "title": "Lecture 10",
    "section": "What is Structured Streaming?",
    "text": "What is Structured Streaming?\nA single, unified programming model and interface for batch and stream processing\nThis unified model offers a simple API interface for both batch and streaming workloads. You can use familiar SQL or batch-like DataFrame queries on your stream as you would on a batch, leaving dealing with the underlying complexities of fault tolerance, optimizations, and tardy data to the engine.\nA broader definition of stream processing\nBig data processing applications have grown complex enough that the line between real-time processing and batch processing has blurred significantly. The aim with Structured Streaming was to broaden its applicability from traditional stream processing to a larger class of applications; any application that periodically (e.g., every few hours) to continuously (like traditional streaming applications) processes data should be expressible using Structured Streaming."
  },
  {
    "objectID": "slides/10-slides.html#the-programming-model-of-structured-streaming",
    "href": "slides/10-slides.html#the-programming-model-of-structured-streaming",
    "title": "Lecture 10",
    "section": "The Programming Model of Structured Streaming",
    "text": "The Programming Model of Structured Streaming"
  },
  {
    "objectID": "slides/10-slides.html#the-programming-model-of-structured-streaming-1",
    "href": "slides/10-slides.html#the-programming-model-of-structured-streaming-1",
    "title": "Lecture 10",
    "section": "The Programming Model of Structured Streaming",
    "text": "The Programming Model of Structured Streaming\n\n\nEvery new record received in the data stream is like a new row being appended to the unbounded input table.\nStructured Streaming will automatically convert this batch-like query to a streaming execution plan. This is called incrementalization\nStructured Streaming figures out what state needs to be maintained to update the result each time a record arrive\nFinally, developers specify triggering policies to control when to update the results. Each time a trigger fires, Structured Streaming checks for new data (i.e., a new row in the input table) and incrementally updates the result."
  },
  {
    "objectID": "slides/10-slides.html#specifying-output-mode",
    "href": "slides/10-slides.html#specifying-output-mode",
    "title": "Lecture 10",
    "section": "Specifying output mode",
    "text": "Specifying output mode\nAppend mode\nOnly the new rows appended to the result table since the last trigger will be written to the external storage. This is applicable only in queries where existing rows in the result table cannot change (e.g., a map on an input stream).\nUpdate mode\nOnly the rows that were updated in the result table since the last trigger will be changed in the external storage. This mode works for output sinks that can be updated in place, such as a MySQL table.\nComplete mode\nThe entire updated result table will be written to external storage."
  },
  {
    "objectID": "slides/10-slides.html#the-5-fundamentals-steps-of-a-structured-streaming-query",
    "href": "slides/10-slides.html#the-5-fundamentals-steps-of-a-structured-streaming-query",
    "title": "Lecture 10",
    "section": "The 5 Fundamentals steps of a Structured Streaming Query",
    "text": "The 5 Fundamentals steps of a Structured Streaming Query\n\nDefine input sources\nTransform data\nDefine output sink and output mode\nSpecify processing details\nStart the query"
  },
  {
    "objectID": "slides/10-slides.html#define-input-sources",
    "href": "slides/10-slides.html#define-input-sources",
    "title": "Lecture 10",
    "section": "1. Define input sources",
    "text": "1. Define input sources\n\n\n spark = SparkSession...\n    lines = (spark\n      .readStream.format(\"socket\")\n      .option(\"host\", \"localhost\")\n      .option(\"port\", 9999)\n      .load())\n\n\n\n\nThis code generates the lines DataFrame as an unbounded table of newline-separated text data read from localhost:9999. Note that, similar to batch sources with spark.read, this does not immediately start reading the streaming data; it only sets up the configurations necessary for reading the data once the streaming query is explicitly started.\nBesides sockets, Apache Spark natively supports reading data streams from Apache Kafka and various file-based formats (Parquet, ORC, JSON, etc.). A streaming query can define multiple input sources, both streaming and batch, which can be combined using DataFrame operations like unions and joins."
  },
  {
    "objectID": "slides/10-slides.html#transform-data",
    "href": "slides/10-slides.html#transform-data",
    "title": "Lecture 10",
    "section": "2. Transform data",
    "text": "2. Transform data\n\nfrom pyspark.sql.functions import *\nwords = lines.select(split(col(\"value\"), \"\\\\s\").alias(\"word\")) \ncounts = words.groupBy(\"word\").count()\n\n\nNow we can apply the usual DataFrame operations\nNote that these operations to transform the lines streaming DataFrame would work in the exact same way if lines were a batch DataFrame."
  },
  {
    "objectID": "slides/10-slides.html#define-output-sink-and-output-mode",
    "href": "slides/10-slides.html#define-output-sink-and-output-mode",
    "title": "Lecture 10",
    "section": "3. Define output sink and output mode",
    "text": "3. Define output sink and output mode\n\nwriter = counts.writeStream.format(\"console\").outputMode(\"complete\")"
  },
  {
    "objectID": "slides/10-slides.html#specify-processing-details",
    "href": "slides/10-slides.html#specify-processing-details",
    "title": "Lecture 10",
    "section": "4. Specify Processing details",
    "text": "4. Specify Processing details\n\n\ncheckpointDir = \"...\"\n    writer2 = (writer\n      .trigger(processingTime=\"1 second\")\n      .option(\"checkpointLocation\", checkpointDir))\n\n\n\nTriggering details\n\nDefault: When the trigger is not explicitly specified, then by default, the streaming query executes data in micro-batches where the next micro-batch is triggered as soon as the previous micro-batch has completed.\nProcessing time with trigger interval: You can explicitly specify the Processing Time trigger with an interval, and the query will trigger micro-batches at that fixed interval.\nOnce: In this mode, the streaming query will execute exactly one micro-batch. It processes new data available in a single batch and then stops itself. This is useful when you want to control the triggering and processing from an external scheduler that will start the query on a schedule (e.g., to control cost by only executing a query once per day).\nContinuous:  experimental as of Spark 3.0"
  },
  {
    "objectID": "slides/10-slides.html#start-the-query",
    "href": "slides/10-slides.html#start-the-query",
    "title": "Lecture 10",
    "section": "5. Start the query",
    "text": "5. Start the query\n\nstreamingQuery = writer2.start()"
  },
  {
    "objectID": "slides/10-slides.html#spark-streaming-under-the-hood",
    "href": "slides/10-slides.html#spark-streaming-under-the-hood",
    "title": "Lecture 10",
    "section": "Spark Streaming under the hood",
    "text": "Spark Streaming under the hood\n\n\nSpark SQL analyzes and optimizes this logical plan to ensure that it can be executed incrementally and efficiently on streaming data.\nSpark SQL starts a background thread that continuously executes a loop\nThis loop continues until the query is terminated"
  },
  {
    "objectID": "slides/10-slides.html#spark-streaming-under-the-hood-1",
    "href": "slides/10-slides.html#spark-streaming-under-the-hood-1",
    "title": "Lecture 10",
    "section": "Spark Streaming under the hood",
    "text": "Spark Streaming under the hood\n\nThe loop\n\nBased on the configured trigger interval, the thread checks the streaming sources for the availability of new data.\nIf available, the new data is executed by running a micro-batch. From the optimized logical plan, an optimized Spark execution plan is generated that reads the new data from the source, incrementally computes the updated result, and writes the output to the sink according to the configured output mode.\nFor every micro-batch, the exact range of data processed (e.g., the set of files or the range of Apache Kafka offsets) and any associated state are saved in the configured checkpoint location so that the query can deterministically reproc‐ ess the exact range if needed."
  },
  {
    "objectID": "slides/10-slides.html#spark-streaming-under-the-hood-2",
    "href": "slides/10-slides.html#spark-streaming-under-the-hood-2",
    "title": "Lecture 10",
    "section": "Spark Streaming under the hood",
    "text": "Spark Streaming under the hood\n\nThe loop continues until the query is terminated which can be for one of the following reasons:\n\nA failure has occurred in the query (either a processing error or a failure in the cluster).\nThe query is explicitly stopped using streamingQuery.stop().\nIf the trigger is set to Once, then the query will stop on its own after executing a single micro-batch containing all the available data."
  },
  {
    "objectID": "slides/10-slides.html#data-transformations",
    "href": "slides/10-slides.html#data-transformations",
    "title": "Lecture 10",
    "section": "Data Transformations",
    "text": "Data Transformations\nEach execution is considered as a micro-batch. DataFrame operations can be broadly classified into stateless and stateful operations based on whether executing the operation incrementally requires maintaining a state."
  },
  {
    "objectID": "slides/10-slides.html#stateless-transformations",
    "href": "slides/10-slides.html#stateless-transformations",
    "title": "Lecture 10",
    "section": "Stateless Transformations",
    "text": "Stateless Transformations\nAll projection operations (e.g., select(), explode(), map(), flatMap()) and selection operations (e.g., filter(), where()) process each input record individually without needing any information from previous rows. This lack of dependence on prior input data makes them stateless operations.\nA streaming query having only stateless operations supports the append and update output modes, but not complete mode. This makes sense: since any processed output row of such a query cannot be modified by any future data, it can be written out to all streaming sinks in append mode (including append-only ones, like files of any format). On the other hand, such queries naturally do not combine information across input records, and therefore may not reduce the volume of the data in the result. Complete mode is not supported because storing the ever-growing result data is usually costly. This is in sharp contrast with stateful transformations, as we will discuss next."
  },
  {
    "objectID": "slides/10-slides.html#stateful-transformations",
    "href": "slides/10-slides.html#stateful-transformations",
    "title": "Lecture 10",
    "section": "Stateful Transformations",
    "text": "Stateful Transformations\nThe simplest example of a stateful transformation is DataFrame.groupBy().count(), which generates a running count of the number of records received since the beginning of the query. In every micro-batch, the incremental plan adds the count of new records to the previous count generated by the previous micro-batch. This partial count communicated between plans is the state. This state is maintained in the memory of the Spark executors and is checkpointed to the configured location in order to tolerate failures. While Spark SQL automatically manages the life cycle of this state to ensure correct results, you typically have to tweak a few knobs to control the resource usage for maintaining state. In this section, we are going to explore how different stateful operators manage their state under the hood."
  },
  {
    "objectID": "slides/10-slides.html#stateful-streaming-aggregations",
    "href": "slides/10-slides.html#stateful-streaming-aggregations",
    "title": "Lecture 10",
    "section": "Stateful Streaming Aggregations",
    "text": "Stateful Streaming Aggregations\nStructured Streaming can incrementally execute most DataFrame aggregation operations. You can aggregate data by keys (e.g., streaming word count) and/or by time (e.g., count records received every hour).\n\nAggregations Not Based on Time\nAggregations with Event-Time Windows"
  },
  {
    "objectID": "slides/10-slides.html#mapping-of-event-time-to-tumbling-windows",
    "href": "slides/10-slides.html#mapping-of-event-time-to-tumbling-windows",
    "title": "Lecture 10",
    "section": "Mapping of event time to tumbling windows",
    "text": "Mapping of event time to tumbling windows"
  },
  {
    "objectID": "slides/10-slides.html#mapping-of-event-time-to-multiple-overlapping-windows",
    "href": "slides/10-slides.html#mapping-of-event-time-to-multiple-overlapping-windows",
    "title": "Lecture 10",
    "section": "Mapping of event time to multiple overlapping windows",
    "text": "Mapping of event time to multiple overlapping windows"
  },
  {
    "objectID": "slides/10-slides.html#updated-counts-in-the-result-table-after-each-five-minute-trigger",
    "href": "slides/10-slides.html#updated-counts-in-the-result-table-after-each-five-minute-trigger",
    "title": "Lecture 10",
    "section": "Updated counts in the result table after each five-minute trigger",
    "text": "Updated counts in the result table after each five-minute trigger"
  },
  {
    "objectID": "slides/10-slides.html#using-watermarks",
    "href": "slides/10-slides.html#using-watermarks",
    "title": "Lecture 10",
    "section": "Using watermarks",
    "text": "Using watermarks\nA watermark is defined as a moving threshold in event time that trails behind the maximum event time seen by the query in the processed data. The trailing gap, known as the watermark delay, defines how long the engine will wait for late data to arrive."
  },
  {
    "objectID": "slides/10-slides.html#aws-kinesis",
    "href": "slides/10-slides.html#aws-kinesis",
    "title": "Lecture 10",
    "section": "AWS Kinesis",
    "text": "AWS Kinesis\n\n\n\n\nSimilar to Apache Kafka\nAbstracts away much of the configuration details\nCosts based on usage\n\nRead more comparisons here."
  },
  {
    "objectID": "slides/10-slides.html#spark-sql-connector-for-aws-kinesis",
    "href": "slides/10-slides.html#spark-sql-connector-for-aws-kinesis",
    "title": "Lecture 10",
    "section": "Spark SQL Connector for AWS Kinesis",
    "text": "Spark SQL Connector for AWS Kinesis\nConnector to make Kinesis work with Spark Structured Streaming\nhttps://github.com/qubole/kinesis-sql"
  },
  {
    "objectID": "slides/11-slides.html#aws-academy",
    "href": "slides/11-slides.html#aws-academy",
    "title": "Lecture 11",
    "section": "AWS Academy",
    "text": "AWS Academy\n\nCredit limit - $100\nCourse numbers:\n\nCourse #1 - 24178\nCourse #2 - 27354\nCourse #3 - 22802\nCourse #4 - 26418\n\n\nSTAY WITH COURSE 24178 UNLESS YOU HAVE RUN OUT OF CREDITS OR >$90 USED!\nNote that you will have to repeat several setup steps:\n\nsecurity group\nEC2 keypair uploading (the AWS part only)\nsagemaker setup\nany S3 uploading or copying as well as bucket creation as necessary\nEMR configuration"
  },
  {
    "objectID": "slides/11-slides.html#ray",
    "href": "slides/11-slides.html#ray",
    "title": "Lecture 11",
    "section": "Ray",
    "text": "Ray\n\n\nRay is an open-source unified compute framework that makes it easy to scale AI and Python workloads — from reinforcement learning to deep learning to tuning, and model serving. Learn more about Ray’s rich set of libraries and integrations."
  },
  {
    "objectID": "slides/11-slides.html#why-do-we-need-ray",
    "href": "slides/11-slides.html#why-do-we-need-ray",
    "title": "Lecture 11",
    "section": "Why do we need Ray?",
    "text": "Why do we need Ray?\n\nTo scale any Python workload from laptop to cloud\nRemember the Spark ecosystem with all its integrations…\nHadoop (fault tolerance, resiliency using commodity hardware) -> Spark (in memory data processing) -> Ray (asynch processing, everything you need for scaling AI workloads)\n\n\n\nRay Summit 2022 - Ray and Large Language Models"
  },
  {
    "objectID": "slides/11-slides.html#what-can-we-do-with-ray",
    "href": "slides/11-slides.html#what-can-we-do-with-ray",
    "title": "Lecture 11",
    "section": "What can we do with Ray?",
    "text": "What can we do with Ray?"
  },
  {
    "objectID": "slides/11-slides.html#what-comes-in-the-box",
    "href": "slides/11-slides.html#what-comes-in-the-box",
    "title": "Lecture 11",
    "section": "What comes in the box?",
    "text": "What comes in the box?"
  },
  {
    "objectID": "slides/11-slides.html#our-focus-today",
    "href": "slides/11-slides.html#our-focus-today",
    "title": "Lecture 11",
    "section": "Our focus today …",
    "text": "Our focus today …\n\n\nRay Core Ray Core provides a small number of core primitives (i.e., tasks, actors, objects) for building and scaling distributed applications.\nhttps://docs.ray.io/en/latest/ray-core/walkthrough.html\n\nRay Datasets\nRay Datasets are the standard way to load and exchange data in Ray libraries and applications. They provide basic distributed data transformations such as maps (map_batches), global and grouped aggregations (GroupedDataset), and shuffling operations (random_shuffle, sort, repartition), and are compatible with a variety of file formats, data sources, and distributed frameworks.\nhttps://docs.ray.io/en/latest/data/dataset.html"
  },
  {
    "objectID": "slides/11-slides.html#ray-core---tasks",
    "href": "slides/11-slides.html#ray-core---tasks",
    "title": "Lecture 11",
    "section": "Ray Core - Tasks",
    "text": "Ray Core - Tasks\n\nRay enables arbitrary functions to be executed asynchronously on separate Python workers.\nSuch functions are called Ray remote functions and their asynchronous invocations are called Ray tasks.\n\n# By adding the `@ray.remote` decorator, a regular Python function\n# becomes a Ray remote function.\n@ray.remote\ndef my_function():\n    # do something\n    time.sleep(10)\n    return 1\n\n# To invoke this remote function, use the `remote` method.\n# This will immediately return an object ref (a future) and then create\n# a task that will be executed on a worker process.\nobj_ref = my_function.remote()\n\n# The result can be retrieved with ``ray.get``.\nassert ray.get(obj_ref) == 1\n\n# Specify required resources.\n@ray.remote(num_cpus=4, num_gpus=2)\ndef my_other_function():\n    return 1\n\n# Ray tasks are executed in parallel.\n# All computation is performed in the background, driven by Ray's internal event loop.\nfor _ in range(4):\n    # This doesn't block.\n    my_function.remote()"
  },
  {
    "objectID": "slides/11-slides.html#ray-core---actors",
    "href": "slides/11-slides.html#ray-core---actors",
    "title": "Lecture 11",
    "section": "Ray Core - Actors",
    "text": "Ray Core - Actors\n\nActors extend the Ray API from functions (tasks) to classes.\nAn actor is a stateful worker (or a service).\nWhen a new actor is instantiated, a new worker is created, and methods of the actor are scheduled on that specific worker and can access and mutate the state of that worker.\n\n@ray.remote(num_cpus=2, num_gpus=0.5)\nclass Counter(object):\n    def __init__(self):\n        self.value = 0\n\n    def increment(self):\n        self.value += 1\n        return self.value\n\n    def get_counter(self):\n        return self.value\n\n# Create an actor from this class.\ncounter = Counter.remote()\n# Call the actor.\nobj_ref = counter.increment.remote()\nassert ray.get(obj_ref) == 1"
  },
  {
    "objectID": "slides/11-slides.html#ray-core---objects",
    "href": "slides/11-slides.html#ray-core---objects",
    "title": "Lecture 11",
    "section": "Ray Core - Objects",
    "text": "Ray Core - Objects\n\nTasks and actors create and compute on objects.\nObjects are referred as remote objects because they can be stored anywhere in a Ray cluster\n\nWe use object refs to refer to them.\n\nRemote objects are cached in Ray’s distributed shared-memory object store\nThere is one object store per node in the cluster.\n\nIn the cluster setting, a remote object can live on one or many nodes, independent of who holds the object ref(s).\n\n\n\n\n\n\n\n\nNote\n\n\nRemote objects are immutable. That is, their values cannot be changed after creation. This allows remote objects to be replicated in multiple object stores without needing to synchronize the copies."
  },
  {
    "objectID": "slides/11-slides.html#ray-core---objects-contd.",
    "href": "slides/11-slides.html#ray-core---objects-contd.",
    "title": "Lecture 11",
    "section": "Ray Core - Objects (contd.)",
    "text": "Ray Core - Objects (contd.)\n# Put an object in Ray's object store.\ny = 1\nobject_ref = ray.put(y)\n\n# Get the value of one object ref.\nobj_ref = ray.put(1)\nassert ray.get(obj_ref) == 1\n\n# Get the values of multiple object refs in parallel.\nassert ray.get([ray.put(i) for i in range(3)]) == [0, 1, 2]"
  },
  {
    "objectID": "slides/11-slides.html#ray-datasets",
    "href": "slides/11-slides.html#ray-datasets",
    "title": "Lecture 11",
    "section": "Ray Datasets",
    "text": "Ray Datasets\n\nRay Datasets are the standard way to load and exchange data in Ray libraries and applications.\nThey provide basic distributed data transformations such as maps, global and grouped aggregations, and shuffling operations.\nCompatible with a variety of file formats, data sources, and distributed frameworks.\nRay Datasets are designed to load and preprocess data for distributed ML training pipelines.\nDatasets simplify general purpose parallel GPU and CPU compute in Ray.\n\nProvide a higher-level API for Ray tasks and actors for such embarrassingly parallel compute, internally handling operations like batching, pipelining, and memory management."
  },
  {
    "objectID": "slides/11-slides.html#ray-datasets-1",
    "href": "slides/11-slides.html#ray-datasets-1",
    "title": "Lecture 11",
    "section": "Ray Datasets",
    "text": "Ray Datasets\n\nCreate\n\nimport ray\n\ndataset = ray.data.read_csv(\"s3://anonymous@air-example-data/iris.csv\")\n\ndataset.show(limit=1)\n\nTransform\n\nimport pandas as pd\n\n# Find rows with spepal length < 5.5 and petal length > 3.5.\ndef transform_batch(df: pd.DataFrame) -> pd.DataFrame:\n    return df[(df[\"sepal length (cm)\"] < 5.5) & (df[\"petal length (cm)\"] > 3.5)]\n\ntransformed_dataset = dataset.map_batches(transform_batch)\nprint(transformed_dataset)\n\nConsume\n\nbatches = transformed_dataset.iter_batches(batch_size=8)\nprint(next(iter(batches)))\n\nSave\n\nimport os\n\ntransformed_dataset.write_parquet(\"iris\")\n\nprint(os.listdir(\"iris\"))"
  },
  {
    "objectID": "slides/11-slides.html#references",
    "href": "slides/11-slides.html#references",
    "title": "Lecture 11",
    "section": "References",
    "text": "References\n\nRay website\nLarge Scale Data Loading and Data Preprocessing with Ray\nRay NYC March 2023 with NYC ML/AI Meetup\nLarge-scale deep learning training and tuning with Ray at Uber\nAccelerating AI/ML scaling and AI development with Anyscale and AWS\nScaling AI with Project Ray, the Successor to Spark"
  },
  {
    "objectID": "slides/12-slides.html#aws-academy",
    "href": "slides/12-slides.html#aws-academy",
    "title": "Lecture 12",
    "section": "AWS Academy",
    "text": "AWS Academy\n\nCredit limit - $100\nCourse numbers:\n\nCourse #1 - 24178\nCourse #2 - 27354\nCourse #3 - 22802\nCourse #4 - 26418\n\n\nSTAY WITH COURSE 24178 UNLESS YOU HAVE RUN OUT OF CREDITS OR >$90 USED!\nNote that you will have to repeat several setup steps:\n\nsecurity group\nEC2 keypair uploading (the AWS part only)\nany S3 uploading or copying as well as bucket creation as necessary\nEMR configuration"
  },
  {
    "objectID": "slides/12-slides.html#lambda",
    "href": "slides/12-slides.html#lambda",
    "title": "Lecture 12",
    "section": "Lambda",
    "text": "Lambda\n\nLambda is a powerful tool to build serverless microfunctions in a variety of programming languages that can be triggered in a variety of ways.\nThe execution of a Lambda can scale to meet the burst needs of users going to a website, or the number of rows of incoming data.\nThere can be thousands of executions happening simultaneously.\nLambda is billed by the millisecond (ms)."
  },
  {
    "objectID": "slides/12-slides.html#docker",
    "href": "slides/12-slides.html#docker",
    "title": "Lecture 12",
    "section": "Docker",
    "text": "Docker\nhttps://www.docker.com/resources/what-container/\n\n\nA container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another.\nA Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings.\nContainer images become containers at runtime and in the case of Docker containers – images become containers when they run on Docker Engine."
  },
  {
    "objectID": "slides/12-slides.html#dockerfile",
    "href": "slides/12-slides.html#dockerfile",
    "title": "Lecture 12",
    "section": "Dockerfile",
    "text": "Dockerfile\n# syntax=docker/dockerfile:1\n\n# adapted from https://www.philschmid.de/aws-lambda-with-custom-docker-image\n# https://docs.aws.amazon.com/lambda/latest/dg/python-image.html\nFROM public.ecr.aws/lambda/python:3.9\n\n# copy requirements file and install necessary packages\nADD requirements.txt ${LAMBDA_TASK_ROOT}\nRUN pip3 install -r ${LAMBDA_TASK_ROOT}/requirements.txt --target \"${LAMBDA_TASK_ROOT}\"\n\n# Copy function code to docker container\nCOPY app.py ${LAMBDA_TASK_ROOT}\n\n# app (name of py file)\n# handler (name of function to execute for lambda job)\nCMD [ \"app.lambda_handler\" ]"
  },
  {
    "objectID": "slides/12-slides.html#what-is-data-engineering",
    "href": "slides/12-slides.html#what-is-data-engineering",
    "title": "Lecture 12",
    "section": "What is Data Engineering?",
    "text": "What is Data Engineering?\nAs the scale of the data grew, the existing ETL processes alone were not sufficient, a separate discipline was needed for:\n\nCollecting data\nManaging storage\nCataloging\nMaking it available for applications such as analytics & machine learning)\nSecurity\nLifecycle management\nAnd more…\n\n\nFrom Wikipedia: Data engineering refers to the building of systems to enable the collection and usage of data. This data is usually used to enable subsequent analysis and data science; which often involves machine learning. Making the data usable usually involves substantial compute and storage, as well as data processing and cleaning."
  },
  {
    "objectID": "slides/12-slides.html#what-do-data-engineers-do",
    "href": "slides/12-slides.html#what-do-data-engineers-do",
    "title": "Lecture 12",
    "section": "What do Data Engineers do?",
    "text": "What do Data Engineers do?\nData engineers build systems that collect data from different sources and make this data available for analytics and ML application. This usually involves the following:\n\nAcquisition: Finding all the different datasets around the business.\n\nThese could be availble in databases, shared drives, ingested directly from IoT devices, external datasets, and more.\n\nCleansing: The raw data usually cannot be used as is, it needs to be cleaned.\nConversion: Since the data is coming from different sources, it would probably in different formats (database tables, CSV, JSON, custom). Needs to be converted into a common format (such as parquet) before it becomes usable.\n\nMultiple datasets need to be joined together to answer a business question."
  },
  {
    "objectID": "slides/12-slides.html#what-do-data-engineers-do-contd.",
    "href": "slides/12-slides.html#what-do-data-engineers-do-contd.",
    "title": "Lecture 12",
    "section": "What do Data Engineers do (contd.)?",
    "text": "What do Data Engineers do (contd.)?\n\nDisambiguation: How to interpret what the data means? Use a data catalog and then with the help of subject matter experts (often called Data Stewards) add meaningful description to the datasets.\nDeduplication: Having a single source of truth!\nData Governance: for how long to store the data, how to enforce access controls (Principle of least privilege) etc.\n\nOnce this is done, data may be stored in a central repository such as a data lake or data lakehouse. Data engineers may also copy and move subsets of data into a data warehouse."
  },
  {
    "objectID": "slides/12-slides.html#data-engineering-tools-technologies",
    "href": "slides/12-slides.html#data-engineering-tools-technologies",
    "title": "Lecture 12",
    "section": "Data engineering tools & technologies",
    "text": "Data engineering tools & technologies\nData engineers work with a variety of tools and technologies, including:\n\nETL Tools: ETL (extract, transform, load) tools move data between systems. They access data, then apply rules to “transform” the data through steps that make it more suitable for analysis.\nSQL: Structured Query Language (SQL) is the standard language for querying relational databases.\nPython: Python is a general programming language. Data engineers may choose to use Python for ETL tasks. Spark (pyspark) for Big Data, Apache Flink for streaming data.\nCloud Data Storage: Including Amazon S3, Azure Data Lake Storage (ADLS), Google Cloud Storage, etc.\nCloud Data Warehouses: Data ready for use by data scientists and analysts is stored in data warehouses, such as Amazon Redshift, Google BigQuery, Azure Data Warehouse, Snowflake etc."
  },
  {
    "objectID": "slides/12-slides.html#popular-data-engineering-tools",
    "href": "slides/12-slides.html#popular-data-engineering-tools",
    "title": "Lecture 12",
    "section": "Popular Data engineering tools",
    "text": "Popular Data engineering tools\n\nSource: https://www.secoda.co/blog/the-top-20-most-commonly-used-data-engineering-tools"
  },
  {
    "objectID": "slides/12-slides.html#data-lakes",
    "href": "slides/12-slides.html#data-lakes",
    "title": "Lecture 12",
    "section": "Data Lakes",
    "text": "Data Lakes\n\n\nFrom Wikipedia: A data lake is a system or repository of data stored in its natural/raw format, usually object blobs or files. A data lake is usually a single store of data including raw copies of source system data, sensor data, social data etc., and transformed data used for tasks such as reporting, visualization, advanced analytics and machine learning. A data lake can include structured data from relational databases (rows and columns), semi-structured data (CSV, logs, XML, JSON), unstructured data (emails, documents, PDFs) and binary data (images, audio, video)."
  },
  {
    "objectID": "slides/12-slides.html#data-lake-cloud-provider-definitions",
    "href": "slides/12-slides.html#data-lake-cloud-provider-definitions",
    "title": "Lecture 12",
    "section": "Data Lake (Cloud Provider Definitions)",
    "text": "Data Lake (Cloud Provider Definitions)\nDefinitions in the wild (emphasis mine).\n\n\nAWS\nA data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. You can store your data as-is, without having to first structure the data, and run different types of analytics—from dashboards and visualizations to big data processing, real-time analytics, and machine learning to guide better decisions.\n\nGCP\nA data lake provides a scalable and secure platform that allows enterprises to: ingest any data from any system at any speed—even if the data comes from on-premises, cloud, or edge-computing systems; store any type or volume of data in full fidelity; process data in real time or batch mode; and analyze data using SQL, Python, R, or any other language, third-party data, or analytics application.\n\nAzure\nAzure Data Lake includes all the capabilities required to make it easy for developers, data scientists, and analysts to store data of any size, shape, and speed, and do all types of processing and analytics across platforms and languages.\n\nDataBricks\nA data lake is a central location that holds a large amount of data in its native, raw format. Compared to a hierarchical data warehouse, which stores data in files or folders, a data lake uses a flat architecture and object storage to store the data.‍"
  },
  {
    "objectID": "slides/12-slides.html#working-with-a-cloud-data-lake",
    "href": "slides/12-slides.html#working-with-a-cloud-data-lake",
    "title": "Lecture 12",
    "section": "Working with a Cloud Data Lake",
    "text": "Working with a Cloud Data Lake\nA cloud data lake is setup using the cloud provider’s object store (S3, GCS, Azure Blob Storage).\n\nThe object stores are extremely scalable, for context, the maximum size of an object in S3 is 5 TB and there is no limit to number of objects in an S3 bucket.\nThey are extremely duarable, 99.999999999%. Provide strong consistency (read-after-write, listing buckets and objects, granting permissions etc.).\nCost effective, with multiple storage classes.\nIntegration with data processing tools such as Spark, machine learning tools such as SageMaker, data warehouses such as RedShift and data cataloging tools.\nThey provide Fine Grained Access Control to the data."
  },
  {
    "objectID": "slides/12-slides.html#data-warehouses",
    "href": "slides/12-slides.html#data-warehouses",
    "title": "Lecture 12",
    "section": "Data Warehouses",
    "text": "Data Warehouses\n\n\nFrom Wikipedia: In computing, a data warehouse (DW or DWH), also known as an enterprise data warehouse (EDW), is a system used for reporting and data analysis and is considered a core component of business intelligence. DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data in one single place that are used for creating analytical reports for workers throughout the enterprise."
  },
  {
    "objectID": "slides/12-slides.html#data-warehouse-cloud-provider-definitions",
    "href": "slides/12-slides.html#data-warehouse-cloud-provider-definitions",
    "title": "Lecture 12",
    "section": "Data Warehouse (Cloud Provider Definitions)",
    "text": "Data Warehouse (Cloud Provider Definitions)\nDefinitions in the wild (emphasis mine).\n\n\nAWS\nA data warehouse is a central repository of information that can be analyzed to make more informed decisions. Data flows into a data warehouse from transactional systems, relational databases, and other sources, typically on a regular cadence. Business analysts, data engineers, data scientists, and decision makers access the data through business intelligence (BI) tools, SQL clients, and other analytics applications.\nAzure\nA data warehouse is a centralized repository of integrated data from one or more disparate sources. Data warehouses store current and historical data and are used for reporting and analysis of the data.\n\nGCP\nA data warehouse is an enterprise system used for the analysis and reporting of structured and semi-structured data from multiple sources, such as point-of-sale transactions, marketing automation, customer relationship management, and more.\nSnowflake\nA data warehouse (DW) is a relational database that is designed for analytical rather than transactional work. It collects and aggregates data from one or many sources so it can be analyzed to produce business insights."
  },
  {
    "objectID": "slides/12-slides.html#working-with-a-cloud-data-warehouse",
    "href": "slides/12-slides.html#working-with-a-cloud-data-warehouse",
    "title": "Lecture 12",
    "section": "Working with a Cloud Data Warehouse",
    "text": "Working with a Cloud Data Warehouse\nAll cloud providers provide a data warehouse solution that works in conjunction with their data lake solution.\n\nAWS has Redshift, Azure has Synapse Analytics. GCP has BigQuery and then there is Snowflake.\nIn a data warehouse, Online analytical processing (OLAP) allows for fast querying and analysis of data from different perspectives. It also helps in pre-aggregating and pre-calculating the information available in the archive.\nData warehouses are Peta Byte scale (Amazon RedShift, Google BigQuery, Azure Synapse Analytics).\nData warehouses can have dedicated compute provisioned or be serverless (BigQuery is serverless, Redshift allows both options now).\nData warehouses now offer integrated ML capabilities, you can build models with SQL and use them in queries (Amazon RedShift ML, Google BigQuery ML, Azure Synapse Analytics ML).\nIntegration with reporting and dashboarding tools such as Tableau, Grafana, Looker etc. and data analytics tools such as Spark and data cataloging tools.\nThey provide Fine Grained Access Control to the data."
  },
  {
    "objectID": "slides/12-slides.html#combining-data-lakes-and-data-warehouses",
    "href": "slides/12-slides.html#combining-data-lakes-and-data-warehouses",
    "title": "Lecture 12",
    "section": "Combining Data Lakes and Data Warehouses",
    "text": "Combining Data Lakes and Data Warehouses\nCombine the flexibility, cost-efficiency, and scale of data lakes with the data management and ACID transactions of data warehouses to provide a single architecture that can enable business intelligence and machine learning on all data.\n\nBuild a lakehouse architecture on AWS\nThe Databricks lakehouse platform\nOpen data lakehouse on Google Cloud\nThe data lakehouse, the data warehouse and a modern data platform on Azure"
  },
  {
    "objectID": "slides/12-slides.html#nosql-databases",
    "href": "slides/12-slides.html#nosql-databases",
    "title": "Lecture 12",
    "section": "NoSQL Databases",
    "text": "NoSQL Databases\nAt some point we needed to think beyond relation databases, because: - Data became more and more complex (not all data is tabular, thinkin JSON data emitted by an IoT device). - Cost of storage decreased (everything did not need to be stored in the 3rd normal form) - More data on the cloud meant data needed to be placed across different servers (scale-out) - Data needed to be placed intelligently in geo locations of interest - And more…\n\nFrom Wikipedia: A NoSQL (originally referring to “non-SQL” or “non-relational”) database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases. Such databases have existed since the late 1960s, but the name “NoSQL” was only coined in the early 21st century, triggered by the needs of Web 2.0 companies. NoSQL databases are increasingly used in big data and real-time web applications. NoSQL systems are also sometimes called Not only SQL to emphasize that they may support SQL-like query languages or sit alongside SQL databases in polyglot-persistent architectures."
  },
  {
    "objectID": "slides/12-slides.html#types-of-nosql-databases",
    "href": "slides/12-slides.html#types-of-nosql-databases",
    "title": "Lecture 12",
    "section": "Types of NoSQL Databases",
    "text": "Types of NoSQL Databases\nOver time, four major types of NoSQL databases emerged: document databases, key-value databases, wide-column stores, and graph databases.\n\nDocument databases store data in documents similar to JSON (JavaScript Object Notation) objects. Each document contains pairs of fields and values. The values can typically be a variety of types including things like strings, numbers, booleans, arrays, or objects.\nKey-value databases are a simpler type of database where each item contains keys and values.\nWide-column stores store data in tables, rows, and dynamic columns.\nGraph databases store data in nodes and edges. Nodes typically store information about people, places, and things, while edges store information about the relationships between the nodes."
  },
  {
    "objectID": "slides/12-slides.html#examples-of-nosql-databases",
    "href": "slides/12-slides.html#examples-of-nosql-databases",
    "title": "Lecture 12",
    "section": "Examples of NoSQL Databases",
    "text": "Examples of NoSQL Databases\n\n\n\n\n\n\n\nNOSQL Database Type\nExamples\n\n\n\n\nDocument Database\nAmazon DocumentDB, MongoDB, Cosmos DB, ArangoDB, Couchbase Server, CouchDB\n\n\nKey-value Database\nAmazon DynamoDB, Couchbase, Memcached, Redis\n\n\nWide-column datastores\nAmazon DynamoDB, Apache Cassandra, Google Bigtable, Azure Tables\n\n\nGraph databases\nAmazon Neptune, Neo4j\n\n\n\nAs a data scientist you would work with a NoSQL database through an SDK/API. Several programming languages are supported including Python, Java, Go, C++ etc."
  },
  {
    "objectID": "slides/12-slides.html#example-of-documents-in-a-document-database",
    "href": "slides/12-slides.html#example-of-documents-in-a-document-database",
    "title": "Lecture 12",
    "section": "Example of documents in a document database",
    "text": "Example of documents in a document database\nHere is an example of a document inserted in a key-value/document database such as MongoDB.\n{\n   \"name\" : \"IS 350\",\n   \"year\" : \"2015\",\n   \"make\" : \"Lexus\",\n   \"colors\" : [\"black\",\"white\",\"grey\"],\n   \"spec\" : {\n      \"engine\" : \"V6\",\n      \"wheelbase\" : \"110.2 in\",\n      \"length\" : \"183.7 in\"\n   }\n}\nThe same example can be inserted in an Amazon DynamoDB table called (say) Cars.\ndatabase = boto3.resource('dynamodb')\ntable = database.Table('cars')\nitem = {\n   \"name\" : \"IS 350\",\n   \"year\" : \"2015\",\n   \"make\" : \"Lexus\",\n   \"colors\" : [\"black\",\"white\",\"grey\"],\n   \"spec\" : {\n      \"engine\" : \"V6\",\n      \"wheelbase\" : \"110.2 in\",\n      \"length\" : \"183.7 in\"\n   }\n}\ntable.put_item(Item = item)"
  },
  {
    "objectID": "slides/12-slides.html#other-data-stores-to-know-about",
    "href": "slides/12-slides.html#other-data-stores-to-know-about",
    "title": "Lecture 12",
    "section": "Other Data stores to know about",
    "text": "Other Data stores to know about\nBesides the general concepts about data lkakes, warehouses, different types of databases, there are some purpose built databases that are good to know about.\n\nSplunk\nElasticsearch\nDuckDB\nMany many more…"
  },
  {
    "objectID": "slides/12-slides.html#splunk",
    "href": "slides/12-slides.html#splunk",
    "title": "Lecture 12",
    "section": "Splunk",
    "text": "Splunk\nSplunk is a software platform to search, analyze and visualize the machine-generated data gathered from the websites, applications, sensors, devices etc. which make up your IT infrastructure and business. See https://www.splunk.com/.\n\nThe logfile we analyzed in assignment 4 s3://bigdatateaching/forensicswiki/2012_logs.txt, is the typical kind of data that gets ingested into Splunk.\n\n\nImage courtsey: https://docs.splunk.com/Documentation/Splunk/9.0.2/SearchTutorial/Aboutthesearchapp"
  },
  {
    "objectID": "slides/12-slides.html#elasticsearch",
    "href": "slides/12-slides.html#elasticsearch",
    "title": "Lecture 12",
    "section": "Elasticsearch",
    "text": "Elasticsearch\nFrom https://www.elastic.co/what-is/elasticsearch, emphasis mine\nElasticsearch is a distributed, free and open search and analytics engine for all types of data, including textual, numerical, geospatial, structured, and unstructured. Elasticsearch is built on Apache Lucene and was first released in 2010 by Elasticsearch N.V. (now known as Elastic).\nCommonly referred to as the ELK Stack (after Elasticsearch, Logstash, and Kibana), the Elastic Stack now includes a rich collection of lightweight shipping agents known as Beats for sending data to Elasticsearch.\nData is inserted in Elasticsearch indexes as JSON documents using a REST API/SDK.\n\nImage courtsey: https://www.elastic.co/kibana"
  },
  {
    "objectID": "slides/12-slides.html#duckdb",
    "href": "slides/12-slides.html#duckdb",
    "title": "Lecture 12",
    "section": "DuckDB",
    "text": "DuckDB\nDuckDB is an in-process SQL OLAP database management system.\n\nIt is like sqllite, but for analytics. What does this mean? It means that your database runs inside your process, there are no servers to manage, no remote system to connect to. Very snappy, easy to experiment with SQL like syntax.\n\nDuckDB does vectorized processing i.e. loads chunks of data into memory (tries to keep everything in the CPU’s L1 and L2 cache) and is thus able to handle datasets bigger than the amount of RAM available.\nDuckDB supports Python, R and a host of other languages.\nWhen to use DuckDB: when you cannot use Pandas for rapid experimentation with Big Data, especially when combining DuckDB with Arrow.\nWhen not to use DuckDB: DuckDB has a valuable but very niche use-case.\n\n\nPaper on DuckDB by Hannes Mühleisen & Mark Raasveldt\nDuckDB: an Embeddable Analytical Database https://hannes.muehleisen.org/publications/SIGMOD2019-demo-duckdb.pdf"
  },
  {
    "objectID": "slides/12-slides.html#further-reading",
    "href": "slides/12-slides.html#further-reading",
    "title": "Lecture 12",
    "section": "Further Reading",
    "text": "Further Reading\nPlease lookup these topics on Google for further reading. Not providing specific links here because they all point to vendor specific products.\n\nData Catalog\nData Governance\nData Mesh\nData Fabric"
  },
  {
    "objectID": "slides/12-slides.html#lab-analyzing-nyc-taxi-dataset-with-duckdb",
    "href": "slides/12-slides.html#lab-analyzing-nyc-taxi-dataset-with-duckdb",
    "title": "Lecture 12",
    "section": "Lab: Analyzing NYC-Taxi dataset with DuckDB",
    "text": "Lab: Analyzing NYC-Taxi dataset with DuckDB"
  },
  {
    "objectID": "slides/12-slides.html#a-simple-example-of-using-duckdb-and-apache-arrow-using-nyc-taxi-dataset",
    "href": "slides/12-slides.html#a-simple-example-of-using-duckdb-and-apache-arrow-using-nyc-taxi-dataset",
    "title": "Lecture 12",
    "section": "A simple example of using DuckDB and Apache Arrow using NYC Taxi dataset",
    "text": "A simple example of using DuckDB and Apache Arrow using NYC Taxi dataset\nThis notebook reads the NYC taxi dataset files for the year 2021 (about ~29 million rows) and runs some analytics operation on this dataset. This dataset is too big to fit into memory.\n\nWe read the data from S3 using apache Arrow (pyarrow).\nThe zero-copy integration between DuckDB and Apache Arrow allows for rapid analysis of larger than memory datasets in Python and R using either SQL or relational APIs.\nWe create a DuckDB instance in memory and using the connection to this in-memory database We run some simple analytics operations using SQL syntax.\n\nAlso see https://duckdb.org/2021/12/03/duck-arrow.html"
  },
  {
    "objectID": "slides/13-slides.html#before-we-begin..",
    "href": "slides/13-slides.html#before-we-begin..",
    "title": "Lecture 13",
    "section": "Before we begin..",
    "text": "Before we begin..\n\nPandas: pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\n\n\n\nPandas is slow, well yes but also not so much if you use it the right way.\n\n\nApache Arrow and the “10 Things I Hate About pandas” (A 2017 post from the creator of Pandas..)\n50 times faster data loading for Pandas: no problem (but this is an old 2019 article..)\nIs Pandas really that slow?\n\n\nPandas 2.0 and the arrow revolution"
  },
  {
    "objectID": "slides/13-slides.html#polars",
    "href": "slides/13-slides.html#polars",
    "title": "Lecture 13",
    "section": "Polars",
    "text": "Polars\n\nPolars: Lightning-fast DataFrame library for Rust and Python\n\nWhy is it faster than Pandas?\n\n\nWritten in Rust (compiled not interpreted).\nUses all available cores of your machine.\nUse PyArrow.\n[My opinion] Makes it easier to write code the right away (has a strict schema and others)!"
  },
  {
    "objectID": "slides/13-slides.html#coming-from-pandas-to-polars",
    "href": "slides/13-slides.html#coming-from-pandas-to-polars",
    "title": "Lecture 13",
    "section": "Coming from Pandas to Polars",
    "text": "Coming from Pandas to Polars\n\n\n\n\n\n\n\n\n  \n    \n      \n      type\n      c\n      size\n    \n  \n  \n    \n      0\n      m\n      1\n      3\n    \n    \n      1\n      n\n      1\n      3\n    \n    \n      2\n      o\n      1\n      3\n    \n    \n      3\n      m\n      2\n      4\n    \n    \n      4\n      m\n      2\n      4\n    \n    \n      5\n      n\n      2\n      4\n    \n    \n      6\n      n\n      2\n      4\n    \n  \n\n\n\n\n\n\n\n\n\nshape: (7, 3)typecsizestri64u32\"m\"13\"n\"13\"o\"13\"m\"24\"m\"24\"n\"24\"n\"24\n\n\n\n\n\nhttps://pola-rs.github.io/polars-book/user-guide/coming_from_pandas.html"
  },
  {
    "objectID": "slides/13-slides.html#coming-from-spark-to-polars",
    "href": "slides/13-slides.html#coming-from-spark-to-polars",
    "title": "Lecture 13",
    "section": "Coming From Spark to Polars",
    "text": "Coming From Spark to Polars\n\nhttps://pola-rs.github.io/polars-book/user-guide/coming_from_spark.html"
  },
  {
    "objectID": "slides/13-slides.html#polars-useful-links",
    "href": "slides/13-slides.html#polars-useful-links",
    "title": "Lecture 13",
    "section": "Polars: useful links",
    "text": "Polars: useful links\n\nPolars\nUser guide\nPolars GitHub repo\nPandas Vs Polars: a syntax and speed comparison\nTips & tricks for working with strings in Polars"
  },
  {
    "objectID": "slides/13-slides.html#duckdb",
    "href": "slides/13-slides.html#duckdb",
    "title": "Lecture 13",
    "section": "DuckDB",
    "text": "DuckDB\nDuckDB is an in-process SQL OLAP database management system\npip install duckdb==0.7.1\n\nDuck DBAlso checkout MotherDuck"
  },
  {
    "objectID": "slides/13-slides.html#duckdb-contd.",
    "href": "slides/13-slides.html#duckdb-contd.",
    "title": "Lecture 13",
    "section": "DuckDB (contd.)",
    "text": "DuckDB (contd.)\n\nDuck DB"
  },
  {
    "objectID": "slides/13-slides.html#rapids",
    "href": "slides/13-slides.html#rapids",
    "title": "Lecture 13",
    "section": "RAPIDS",
    "text": "RAPIDS\nRAPIDS is a suite of open-source software libraries and APIs for executing data science pipelines entirely on GPUs—and can reduce training times from days to minutes. Built on NVIDIA® CUDA-X AI™, RAPIDS unites years of development in graphics, machine learning, deep learning, high-performance computing (HPC), and more.\nhttps://www.nvidia.com/en-us/deep-learning-ai/software/rapids"
  },
  {
    "objectID": "slides/14-slides.html#data-preparation-accounts-for-about-80-of-the-work-of-data-scientists",
    "href": "slides/14-slides.html#data-preparation-accounts-for-about-80-of-the-work-of-data-scientists",
    "title": "Lecture 14",
    "section": "Data preparation accounts for about 80% of the work of data scientists",
    "text": "Data preparation accounts for about 80% of the work of data scientists\n > Source: Forbes article"
  },
  {
    "objectID": "slides/14-slides.html#why-does-this-happen",
    "href": "slides/14-slides.html#why-does-this-happen",
    "title": "Lecture 14",
    "section": "Why does this happen?",
    "text": "Why does this happen?\n\n\nSame set of data sources…\nMultiple different feature pipelines..\nMultiple ML models..\nBut, an overlapping set of ML features..\nMore problems…\n\nFeature duplication\nSlow time to market\nInaccurate predictions"
  },
  {
    "objectID": "slides/14-slides.html#solution",
    "href": "slides/14-slides.html#solution",
    "title": "Lecture 14",
    "section": "Solution…",
    "text": "Solution…\nMachine Learning Feature Store\n\n\nFor a moment, think of the feature store as a database but for ML features.\nIn a Feature Store\n\nFeatures are now easy to find (GUI, SDK)\nFeature transformations are reproducible (feature engineering pipelines can now refer to a consistent set of data)\nML training pipeline has a reliable, curated, maintained data source to get training datasets rather than having to look at the data lake directly\nLow latency lookup for realtime inference\nConsistent features for training and inference"
  },
  {
    "objectID": "slides/14-slides.html#feature-stores-you-can-use",
    "href": "slides/14-slides.html#feature-stores-you-can-use",
    "title": "Lecture 14",
    "section": "Feature Stores you can use",
    "text": "Feature Stores you can use\n\nFeast: Open Source for Production ML\nfeathr: A scalable, unified data and AI engineering platform for enterprise\nTecton: Feature Platform for Real-Time Machine Learning\nAmazon Sagemaker Feature Store"
  },
  {
    "objectID": "slides/14-slides.html#why-do-we-need-vector-databases",
    "href": "slides/14-slides.html#why-do-we-need-vector-databases",
    "title": "Lecture 14",
    "section": "Why do we need vector databases?",
    "text": "Why do we need vector databases?"
  },
  {
    "objectID": "slides/14-slides.html#algorithms-for-similarity-searh",
    "href": "slides/14-slides.html#algorithms-for-similarity-searh",
    "title": "Lecture 14",
    "section": "Algorithms for similarity searh",
    "text": "Algorithms for similarity searh\n\nk-Nearest Neighbor (k-NN)\nApproximate Nearest Neighbor\n\nHierarchical Navigable Small Worlds algorithm (HNSW)\nFaiss"
  },
  {
    "objectID": "slides/14-slides.html#langchain-and-vector-dbs",
    "href": "slides/14-slides.html#langchain-and-vector-dbs",
    "title": "Lecture 14",
    "section": "Langchain and Vector DBs",
    "text": "Langchain and Vector DBs"
  },
  {
    "objectID": "slides/14-slides.html#large-scale-data-ingestion",
    "href": "slides/14-slides.html#large-scale-data-ingestion",
    "title": "Lecture 14",
    "section": "Large scale data ingestion",
    "text": "Large scale data ingestion"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
<<<<<<< HEAD
    "text": "Amit Arora\n   Online\n   aa1603@georgetown.edu\n   aarora79\n   Schedule an appointment\n\n\n\n\n\n   Thursdays\n   January 11 – May 9, 2024\n   6:30–9:00 PM\n   Car Barn 204\n   Slack\nThis syllabus was last updated on Thursday, February 01, 2024 at 04:13 PM"
=======
    "text": "Amit Arora\n   Online\n   aa1603@georgetown.edu\n   aarora79\n   Schedule an appointment\n\n\n\n\n\n   Thursdays\n   January 11 – May 9, 2024\n   6:30–9:00 PM\n   Car Barn 204\n   Slack\nThis syllabus was last updated on Thursday, February 01, 2024 at 08:28 PM"
>>>>>>> 5cfd7cc16e257c0a05ca9589ee545fcbfa2d7d0d
  },
  {
    "objectID": "syllabus.html#books-software-and-cloud-resources",
    "href": "syllabus.html#books-software-and-cloud-resources",
    "title": "Syllabus",
    "section": "Books, Software and Cloud Resources",
    "text": "Books, Software and Cloud Resources\n\nReadings (for assigned readings)\nThere is no required textbook for the course. We have selected specific chapters from several sources as well as several seminal papers in the big data space, and these will be provided to you in PDF format. We may also provide supplemental materials (articles, links, videos, etc.) to complement the readings. You must read assigned readings prior to the lectures.\n\n\nCloud Resources\nYou will use cloud resources on Amazon Web Services. We will discuss how to setup your accounts and environments in class and lab within the first couple of weeks. You will get credits on both platforms that will be enough to support your coursework throughout the semester.\n\n\nModules\n\n\n\n\n\n\n  \n    \n    \n    \n    \n  \n  \n    \n      Schedule for the semester PPOL 5206 Spring 2024\n    \n    \n      This course is divided into 3 phases: Intro to Big Data and the Cloud, Apache Spark Ecosystem, Other Big Data technologies & misc. topics.\n    \n  \n  \n    \n      \n      Date\n      Module\n      Notes\n    \n  \n  \n    \n      Phase 1\n    \n    Intro to Big Data and the Cloud\n2024-01-11\nCourse overview\n\nCourse overview. Introduction to big data concepts. The Cloud.\n\n    Intro to Big Data and the Cloud\n2024-01-18\nCloud services\n\nSetting up your AWS account.\n\n    Intro to Big Data and the Cloud\n2024-01-25\nParallelization\n\nExplore parallelization with Python multiprocessing.\n\n    Intro to Big Data and the Cloud\n2024-02-01\nEvent driven cloud processing, containers and code portability\n\nLab 4,\"Building Docker containers and working with AWS Lambda.\"\n\n    Intro to Big Data and the Cloud\n2024-02-08\nDuckDB, Polars and file formats\n\nWorking with Big Data on a single machine wht DuckDB and Python Polars.\n\n    \n      Phase 2\n    \n    Apache Spark Ecosystem\n2024-02-15\nIntro to Apache Spark - Spark RDD\n\nSetting up Apache Spark on Amazon SageMaker & EMR. Analyze datasets using Spark Resilient Distributed Datasets (RDD).\n\n    Apache Spark Ecosystem\n2024-02-22\nSpark SQL & Spark Dataframes\n\nStructured data processing with Spark SQL and the Spark DataFrame API.\n\n    \n2024-03-07\nNO CLASS - Spring Break\n\n\n    Apache Spark Ecosystem\n2024-03-14\nSpark ML\n\nScaling machine learning with Spark.\n\n    Apache Spark Ecosystem\n2024-03-21\nSpark NLP\n\nNatural Language Propcessing Using Spark.\n\n    Apache Spark Ecosystem\n2024-04-11\nAccelerate Python workloads with Ray, RAPIDS\n\nUsing Ray and RAPIDS (GPUs for big data analytics).\n\n    \n      Phase 3\n    \n    \n2024-03-28\nNO CLASS - Easter Break\n\n\n    Apache Spark Ecosystem\n2024-04-04\nSpark Streaming\n\nWorking with streaming data using Spark Streaming.\n\n    Other Big Data technologies & misc. topics\n2024-04-18\nML Feature Store\n\nPreparing and sotring ML ready data with feature stores.\n\n    Other Big Data technologies & misc. topics\n2024-04-25\nVector Databases\n\nBig data for generative AI.\n\n    Other Big Data technologies & misc. topics\n2024-05-02\nProject discussion & Open Session\n\nLast class for the semester\n\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIT IS YOUR RESPONSIBILITY TO MANAGE THE CREDITS AND RESOURCES PROVIDED TO YOU. YOU MUST SHUT DOWN YOUR CLOUD RESOURCES WHEN NOT IN USE."
  },
  {
    "objectID": "syllabus.html#learning-activities-communication-and-evaluation",
    "href": "syllabus.html#learning-activities-communication-and-evaluation",
    "title": "Syllabus",
    "section": "Learning Activities, Communication and Evaluation",
    "text": "Learning Activities, Communication and Evaluation\nThis is a hands-on, practical, workshop style course that provides opportunities to use the tools and techniques discussed in class. Although this is not a programming course per se, there is programming involved.\n\nLectures and Labs\nThis course is split into a lecture/lab format, where every class session will have a lecture portion, and most sessions will have an in-class lab portion:\n\nDuring the lecture, we will discuss the concepts and techniques as well as the history and development of these big data tools and cloud platforms.\nDuring the lab sessions, you will be completing exercises and following examples which are designed to show you how to implement the ideas and concepts with various tools. We will start the labs in class but we will not finish. It is your responsibility to complete the labs (which is part of the grade) and will enable your learning.\n\nLectures may not cover all the material and some topics will be introduced in the lab or through readings/assignments.\n\n\nOffice Hours\nInstructors and TAs will hold recurring office hours to answer questions, review material, and support your learning. The times, dates, and location of office hours will be announced via Canvas and Slack in advance.\n\n\nReadings\nOn certain weeks, readings will be assigned to prepare you for the lecture material being presented. These readings should take an hour or less per week.\n\n\nOnline Quizzes\nQuizzes will be given a few times during the semester during lab or lecture at random intervals and times. Quizzes ensure you are keeping up with the material presented in the class. Quizzes are meant to be brief and low-stress with a time limit of 5-10 minutes. The material will be drawn from lectures, labs, and readings.\n\n\nLab Deliverables\nEach lab will have a deliverable. It is essential that you learn the skills presented in the labs so that you can effectively complete the assignments and the big data project. The lab deliverables can sometimes be completed during lab, however, it is your responsibility to complete the deliverable as part of your work outside of lecture/lab time.\n\n\nHomework Assignments\nYou will be several homework assignments for roughly half of the semester. The goal of these problem sets is to hone your big data skills by answering some questions about large datasets. The problem sets will build on the labs and will be much more in-depth. Deliverables from the assignment will usually include code written for your programs and the output produced.\n\n\n\n\n\n\nWarning\n\n\n\nPlease start assignments as soon as they are posted. These assignments can take several hours to complete depending on your familiarity with the material. You will not complete the assignments on time if you start the day they are due.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe reuse problem set questions, we expect students not to copy, refer to, or look at the solutions in preparing their answers. Since this is a graduate-level class, we expect students to want to learn and not search online for answers. See Academic Integrity section.\n\n\n\n\nBig Data Analytics Project\nStudents will assemble into groups of no more than 4 students. You will perform and write up an analysis of a big dataset using the tools learned in class. Big is defined as “a dataset that is so large that you cannot work with it on a laptop.”\nYou will each conduct big data project using Reddit textual data. This project will encompass all of the skills you will learn throughout the semester including data ingestion, transformation, analysis, natural language processing, and machine learning. Similar to previous core courses, the big data project will be a product you can share with the world to demonstrate your data science expertise. Intermediate project assignments will help you incrementally build towards your final portfolio. There will be assignments on exploratory data analysis, machine learning, and natural language processing. Each successive assignment and the final submissions will incorporate feedback from your peers, TAs, and professors. The final timelines and deliverables for the project will be announced in class."
  },
  {
    "objectID": "syllabus.html#grading-and-evaluation",
    "href": "syllabus.html#grading-and-evaluation",
    "title": "Syllabus",
    "section": "Grading and Evaluation",
    "text": "Grading and Evaluation\n\nGroup project : 40%\nAssignments : 30%\nLab completions : 20%\nQuizzes : 10%\n\nTotal is 100%. We have no plans to curve the final grade, and the final letter grade will be:\n\nA: >= 92.5\nA-: 89.5 - 92.49\nB+: 87.99 - 89.49\nB: 81.5 - 87.98\nB-: 79.5 - 81.49"
  },
  {
    "objectID": "syllabus.html#submitting-your-work",
    "href": "syllabus.html#submitting-your-work",
    "title": "Syllabus",
    "section": "Submitting your work",
    "text": "Submitting your work\n\nGitHub classroom\nWe use Github Classroom for all class deliverables: assignments, labs, and the final project. Submitting your work is the process of committing your files and results to your local repository and then pushing it to GitHub.\n\n\n\n\n\n\nImportant\n\n\n\nYou must submit everything through GitHub!\n\n\n\nUse the final-submission commit message\nWhen you are ready for your work to be evaluated, you MUST use the commit message final-submission. If you do not use the commit message final-submission we will assume that you are still working in the repository and we will only grade what is present. By submitting that commit message, you are stating that you are finished with the assignment and are ready for feedback.\n\n\n\n\n\n\nImportant\n\n\n\nMake sure you understand the difference between a git commit and a push, and that you push your repository successfully to GitHub.\n\n\nIn case you need to make a correction after your final-submission and the submission deadline has not yet passed, then you can amend your previous commit. See amending a commit for instructions. Do not change the commit message, it should continue say “final-submission” after the amend.\n\n\n\n\n\n\nWarning\n\n\n\nNo further edits to your GitHub repository are allowed after using the final-submission commit message.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWe will use commit datetime and commit message to assess lateness.\n\n\n\n\n\nLate policy\nIn lieu of extensions, there is a tiered deduction scale if a deliverable is late. Late penalties only apply to labs and assignments.\nWe will assess exceptional circumstances on a case-by-case basis, and only if we are made aware before a deliverable’s deadline, not after.\n\nA late penalty of 10% per day, up to 4 days, will be assessed for assignments and labs that are submitted with a final-submission commit message after the deadline. You may still submit a missed lab or assignment up until the last day of class (May 2) with a maximum possible grade of 60%.\nMissed in-class quizzes cannot be made up and will receive a grade of zero.\nProject deadlines are fixed and have no extensions or late penalty. A missed project deliverable will receive a grade of zero."
  },
  {
    "objectID": "syllabus.html#other-course-policies",
    "href": "syllabus.html#other-course-policies",
    "title": "Syllabus",
    "section": "Other course policies",
    "text": "Other course policies\n\nAttendance and punctuality\nAttendance is mandatory and will be taken. Given the technical nature of this course, and the breadth of topics discussed, you are expected to attend each class, to complete all readings, and to participate actively in lectures, discussions and exercises. We understand there may be times you may need to miss class, please inform us in advance if you are not able to attend class for any reason. However, it is up to you to keep up.\n\n\nParticipation\nWe love participation. Read. Raise your hand. Ask questions. Make comments. Challenge us. Acknowledge us. If we speak for three hours to a silent classroom, it is a lot more boring and tiring for everyone.\n\n\nLaptop and phone use\nYou must bring your laptop to class to work on labs. No phone use is allowed during lecture. You may use your laptop during lecture to take notes, but please refrain from other activities. We reserve the right to ask you to put your phones and laptops away. You may not use your computer or phone while your peers or guest speakers are presenting.\n\n\nCommunication and Slack Rules\n\nAll announcements will be posted on Canvas and Slack\nUse Slack for any question you may have about the course, about assignments or any technical issue. This way everyone can learn from each others questions. We will be monitoring and providing answers on a regular basis. Make sure you understand what is allowed in Slack.\nIndividual emails containing any course question that is not personal will not be answered\nSlack DMs are not to be used unless we DM you first and you can respond to our message. Students may not initiate DMs.\nKeep an eye on the questions posted in Slack. Use the search function. It’s very possible that we have already answered a question, and we reserve the right to point you to the syllabus, previous Slack messages, or other document containing the information requested\nAssignment, lab and project questions will only be answered on Slack up to 12 hours before something is due\n\n\n\nOpen Door Policy\nPlease approach or get in touch with us if something is not working for you regarding the class, methods, etc. Our pledge to you is to provide the best learning experience possible. If you have any issue please do not wait until the last minute to speak with us. You will find that we are fair, reasonable, and flexible and we care deeply about your learning and success."
  },
  {
    "objectID": "syllabus.html#academic-integrity",
    "href": "syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nAs a Jesuit, Catholic university, committed to the education of the whole person, Georgetown expects all members of the academic community, students and faculty, to strive for excellence in scholarship and in character.The University spells out the specific minimum standards for academic integrity in its Honor Code, as well as the procedures to be followed if academic dishonesty is suspected.\nOver and above the honor code, in this course we will seek to create an engaged and passionate learning environment, characterized by respect and courtesy in both our discourse and our ways of paying attention to one another.\nThe code of academic integrity applies to all courses at Georgetown University. Please become familiar with the code. All students are expected to maintain the highest level of academic integrity throughout the course of the semester.Please note that acts of academic dishonesty during the course will be prosecuted and harsh penalties may be sought for such acts. Students are responsible for knowing what acts constitute academic dishonesty. The code may be found at https://bulletin.georgetown.edu/regulations/honor/.\n\n\n\n\n\n\nDanger\n\n\n\nWe have a ZERO TOLERANCE POLICY and students found to be in violation will be reported and penalized. The consequences of any violation may include: additional points penalty, getting a grade of zero, automatically failing the course, and suspension or expulsion from the program.\n\n\n\nDefinition of collaboration\nIn the spirit of fostering a collective and inclusive learning environment, we acknowledge that you will work and study with your peers. We also acknowledge that you use web resources (code examples specifically), and that in writing a program many of you will most likely use the same libraries, functions and other similar instructions in your scripts. However:\n\nYou must write your own code. This will be verified for every assignment against every submission, and any similarity greater than 60% between students on a given assignment will be considered to be unauthorized collaboration.\nYou must do your individual work in your own cloud resources. This will be verified for every assignment. We know the fingerprint of your cloud account and subscriptions and we can tell.\n\n\n\nWhat is allowed\n\nCollaborating with other students during in-class labs to facilitate collective learning\nUsing Slack for helping one-another as long as:\n\nYou do not provide answers directly but only discuss potential approaches\nYou only share up to a few lines of code for everyone’s benefit for the resolution of a specific question or issue\n\nUsing anything (code, resources, tips, approaches, etc.) provided by the instructional team\n\n\n\nWhat is forbidden\nThe following actions are not permitted in any way and are considered a violation of academic integrity:\n\nCopying and sharing code between students in individual assignments or across goups in the group project\nSharing anything on any individual assignment\nUsing code snippets found online (stack overflow, etc.) and not commenting the source\nPlagiarism of any kind\nUsing any Generative Artificial Intelligence tool without acknowledging it\nUsing someone else’s cloud resources\nMaking your private GitHub repos public\nSharing or posting any course materials anywhere\nFaking or tampering with git commit dates or messages"
  },
  {
    "objectID": "syllabus.html#use-of-generative-ai-tools",
    "href": "syllabus.html#use-of-generative-ai-tools",
    "title": "Syllabus",
    "section": "Use of Generative AI tools",
    "text": "Use of Generative AI tools\nWe recognize the recent availability of very powerful generative AI tools like Chat-GPT, GitHub Copilot, and others. These tools can help us be more effective and we embrace their use.\n\n\n\n\n\n\nImportant\n\n\n\nYou are allowed to use GAI tools in a non substantial way.\nWhat does non substantial mean?\nIt means that whatever is generated by GAI must not make up the majority of the work you do.\nAny use of these tools must abide to the following rules:\n\nYou must comment which code blocks were generated by GAI\nYou must note which written sections were generated by GAI\nIf you used a prompt to ask the GAI tool to do something, you must include it\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAny deviation from these rules is considered a violation of academic integrity and will be acted on."
  },
  {
    "objectID": "syllabus.html#var-university.name-resources-and-policies",
    "href": "syllabus.html#var-university.name-resources-and-policies",
    "title": "Syllabus",
    "section": "Georgetown University resources and policies",
    "text": "Georgetown University resources and policies\n\nGeorgetown University’s Plagiarism Policy\nPlagiarism or academic dishonesty in any form will not be tolerated and may result in a failing grade. All Honor Code violations will be submitted to the Honor Council.\nAcademic integrity is central to the learning and teaching process. Students are expected to conduct themselves in a manner that will contribute to the maintenance of academic integrity by making all reasonable efforts to prevent the occurrence of academic dishonesty. Academic dishonesty includes (but is not limited to) obtaining or giving aid on an examination, having unauthorized prior knowledge of an examination, doing work for another student, and plagiarism of all types, including copying code.\nPlagiarism is the intentional or unintentional presentation of another person’s idea or product as one’s own. Plagiarism includes, but is not limited to the following: copying verbatim all or part of another’s written work; using phrases, charts, figures, illustrations, code, or mathematical/scientific solutions without citing the source; paraphrasing ideas, conclusions, or research without citing the source; and using all or part of a literary plot, poem, film, musical score, or other artistic product without attributing the work to its creator. Students can avoid unintentional plagiarism by following carefully accepted scholarly practices. Notes taken for papers and research projects should accurately record sources cited, quoted, paraphrased, or summarized sources and articles should be acknowledged in footnotes.\n\n\nHonor System\nAll students are expected to maintain the highest standards of academic and personal integrity in pursuit of their education at Georgetown. Academic dishonesty, including plagiarism, in any form, is a serious offense, and students found in violation are subject to academic penalties that include, but are not limited to, failure of the course, termination from the program, and revocation of degrees already conferred. All students are held to the Georgetown University Honor Code. For more information about the Honor Code http://gervaseprograms.georgetown.edu/honor/\n\n\nAcademic Integrity and Courtesy\nAs a Jesuit, Catholic university committed to the education of the whole person, Georgetown expects all members of the academic community, students and faculty, to strive for excellence in scholarship and in character. The University spells out the specific minimum standards for academic integrity in its Honor Code and the procedures to be followed if academic dishonesty is suspected. Over and above the honor code, in this course, we will seek to create an engaged and passionate learning environment characterized by respect and courtesy in both our discourse and our ways of paying attention to one another.\n\n\nAcademic Resource Center\nThe Academic Resource Center (ARC) is the campus office responsible for reviewing medical documentation and determining reasonable accommodations for students with disabilities. You can reach the ARC via email at arc@georgetown.edu.\n\n\nCounseling and Psychiatric Services (CAPS)\nAs Georgetown faculty, you are among the most important individuals in some of the students’ lives. They may turn to you when they are struggling and in times of need, or you may be one of the first to notice when they are distressed.\nThe CAPS website has tips for faculty on how to deal with struggling or distressed students. 202.687.6985 or after hours, call (833) 960-3006 to reach Fonemed, a telehealth service; individuals may ask for the on-call CAPS clinician.\n\n\nEmergency Preparedness and HOYAlert\nWe encourage all faculty to become familiar with Georgetown’s Office of Emergency Management and sign up for HOYAlert to receive important safety and University operating status updates. Faculty teaching at the Georgetown Downtown campus might also want to sign up for AlertDC to obtain safety and traffic updates.\n\n\nOffice of Institutional Compliance and Ethics\nThe Office of Institutional Compliance and Ethics supports and coordinates many compliance-related activities the University undertakes. With the endorsement and assistance of the University’s senior leadership, this Office is responsible for leading the development, implementation, and operation of the Georgetown Institutional Compliance and Ethics Program.\n\n\nOffice of Institutional Diversity, Equity and Affirmative Action (IDEAA)\nThe mission of IDEAA is to promote a deep understanding and appreciation among the diverse members of the University community to result in justice and equality in educational, employment, and contracting opportunities, as well as to lead efforts to create an inclusive academic and work environment.\n\n\nTitle IX/Sexual Misconduct\nGeorgetown University and its faculty are committed to supporting survivors and those impacted by sexual misconduct, which includes sexual assault, sexual harassment, relationship violence, and stalking. Georgetown requires faculty members unless otherwise designated as confidential, to report all disclosures of sexual misconduct to the University Title IX Coordinator or a Deputy Title IX Coordinator. Suppose you disclose an incident of sexual misconduct to a professor in or outside of the classroom (except disclosures in papers). In that case, that faculty member must report the incident to the Title IX Coordinator or Deputy Title IX Coordinator. The coordinator will, in turn, reach out to the student to provide support, resources, and the option to meet—[Please note that the student is not required to meet with the Title IX coordinator.]. More information about reporting options and resources can be found on the Sexual Misconduct Website.\nIf you would prefer to speak to someone confidentially, Georgetown has a number of fully confidential professional resources that can provide support and assistance. These resources include:\n\nHealth Education Services for Sexual Assault Response and Prevention: confidential email sarp@georgetown.edu\nCounseling and Psychiatric Services (CAPS): 202.687.6985 or after hours, call (833) 960-3006 to reach Fonemed, a telehealth service; individuals may ask for the on-call CAPS clinician\n\nTitle IX Sexual Misconduct Statement Please know that as faculty members, we are committed to supporting survivors of sexual misconduct, including relationship violence and sexual assault. However, university policy also requires us to report any disclosures about sexual misconduct to the Title IX Coordinator, whose role is to coordinate the University’s response to sexual misconduct.\nGeorgetown has a number of fully confidential professional resources who can provide support and assistance to survivors of sexual assault and other forms of sexual misconduct. These resources include:\n\nGetting Help\nJen Schweer, MA, LPCAssociate Director of Health Education Services for Sexual Assault Response and Prevention (202) 687-032jls242@georgetown.edu\nErica Shirley, Trauma SpecialistCounseling and Psychiatric Services (CAPS)(202) 687-6985els54@georgetown.edu\n\n\n\nThreat Assessment\nGeorgetown University established its Threat Assessment program as part of an extensive emergency planning initiative. The program at Georgetown has been developed and implemented to meet current best practices and national standards for hazard planning in higher education institutions and workplace violence prevention.\n\n\nSpecial Accommodations\nIf you believe that you have a disability that will affect your performance in this class, don’t hesitate to get in touch with the Academic Resource Center for further information. The center is located in the Leavey Center, Suite 335. The Academic Resource Center is the campus office responsible for reviewing documentation provided by students with disabilities and determining reasonable accommodations according to the Americans with Disabilities Act (ADA) and University policies."
  }
]