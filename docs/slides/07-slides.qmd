---
title: "Lecture 7"
subtitle: "Spark DataFrames"
author: "{{< var instructor.name >}}"
institute:
- "{{< var university.name >}}"
- "{{< var course.semester >}}"
format:
  revealjs:
    slide-number: true
    show-slide-number: print
    theme: custom.scss
    transition: fade
    background-transition: fade
    highlight-style: ayu-mirage
    code-copy: true
---


```{r packages, echo = FALSE}
library(magrittr, quietly = T, warn.conflicts = F)
library(tibble, quietly = T, warn.conflicts = F)
library(gt, quietly = T, warn.conflicts = F)
library(dplyr, quietly = T, warn.conflicts = F)
library(kableExtra, quietly = T, warn.conflicts = F)
```



::: {.column width="47%"}
### Looking Back

* Intro to Hadoop and MapReduce
* Hadoop Streaming
* Dask
* Intro to Spark 
* Spark RDDs

### Future  

* Spark ML
* Project Introduction
* Spark Streaming
* Spark NLP
* DataBricks on Azure

:::

::: {.column width="47%"}
### Today

* Review of Distributed Hardware and Spark RDDs
* Spark DataFrames
* SparkSQL

* Lab: 
  * Start a Hadoop cluster on AWS Elastic MapReduce (EMR) **with a bootstrap script**
  * Use Jupyter and Pyspark as front end to Spark
  * Get to know Spark DataFrames

* Quiz on Hadoop Streaming and HDFS (due Friday October 14)

:::


# Essential Hardware/Hadoop Topics {.section}

## AWS Academy

* Credit limit - $100
* Course numbers:

  * Course #1 - 24178
  * Course #2 - 27354
  * Course #3 - 22802
  * Course #4 - 26418
  
STAY WITH COURSE 24178 UNLESS YOU HAVE RUN OUT OF CREDITS OR >$90 USED! 

Note that you will have to repeat several setup steps:

- security group
- EC2 keypair uploading (the AWS part only)
- sagemaker setup
- any S3 uploading or copying as well as bucket creation as necessary
- EMR configuration


## Memory, Disk and Network

::: {.imgcenter}
<img src="img/latency4.png" width=800>
:::

## MapReduce/Hadoop was groundbreaking

* It provided a simple API (map and reduce steps)

* It provided **fault tolerance**, which made it possible to scale to 100s/1000s of nodes of _commodity machines_ where the likelihood of a node failing midway through a job was very high
  * Computations on very large datasets failed and recovered and jobs completed


## **Fault tolerance** came at a cost! 

* Between each map and reduce step, MapReduce shuffles its data and writes intermediate data to disk
  * Reading/writing to disk is 100x slower than in-memory
  * Network communication is 1,000,000x slower than in-memory


## Review of File Systems

What are the possible file system options for each item: **S3**, **HDFS**, **Local file system**

::: {.fragment .highlight-red fragment-index=1}
hadoop jar /usr/lib/hadoop/hadoop-streaming.jar #1 \
:::
::: {.fragment .highlight-red fragment-index=2}
-files basic-mapper.py,basic-reducer.py #2 \
:::
::: {.fragment .highlight-red fragment-index=4}
-input /user/hadoop/in_data #3 \
-output /user/hadoop/in_data #3 \
:::
::: {.fragment .highlight-red fragment-index=6}
-mapper basic-mapper.py #4 \
-reducer basic-reducer.py #4
:::

::: {.fragment .fade-up fragment-index=2}
1. Local file system
:::
::: {.fragment .fade-up fragment-index=4}
2. Local file system or S3
:::
::: {.fragment .fade-up fragment-index=6}
3. HDFS or S3
:::
::: {.fragment .fade-up fragment-index=8}
4. HDFS - why??
:::

# Start your Spark Cluster on AWS {.section}

## Cluster Setup - 1 (install miniconda)

```
echo "Installing Miniconda"
curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o /tmp/miniconda.sh

echo $(date "+%B %d %T")

bash /tmp/miniconda.sh -b -p /mnt/miniconda
rm /tmp/miniconda.sh
ln -s /mnt/miniconda $HOME/miniconda
echo -e '\nexport PATH=$HOME/miniconda/bin:$PATH' >> $HOME/.bashrc
source $HOME/.bashrc
conda update conda -y

echo $(date "+%B %d %T")

```

---

## Cluster Setup - 2 (install cluster packages)

```
echo "Installing common Python pacakges on master and workers"
#conda install \
#-c defaults \
#-c conda-forge \
#-y \
#python=3.7 \
#dask-yarn \
#pyarrow \
#s3fs \
#bokeh \
#conda-pack \
#tornado

aws s3 cp s3://bigdatateaching/bootstrap/worker_environment.yaml /tmp/worker_environment.yaml

conda env update -n base --file /tmp/worker_environment.yaml

echo $(date "+%B %d %T")
```

---

## Cluster Setup - 3 (Print python packages)

```
if [ "$IS_MASTER" = false ]; then
    echo "Python packages installed in worker nodes:"
    conda list
fi
```

---

## Cluster Setup - 4 (Setup master node)

```
sudo yum install -y git libcurl-devel htop
echo "Installing Master node Python libraries"
conda install \
-c defaults \
-c conda-forge \
-y \
notebook \
ipywidgets \
jupyter-server-proxy \
findspark \
matplotlib \
jupyterlab \
scikit-learn \
nltk \
scipy \
beautifulsoup4 \
nose \
lxml \
hdf5 \
seaborn \
boto3

pip install -U nbclassic>=0.2.8 
# issue with older nbclassic - https://github.com/jupyterlab/jupyterlab/issues/10228

echo $(date "+%B %d %T")

pip install spark-nlp

echo $(date "+%B %d %T")
```

---

## Cluster Setup - 5 (Jupyter Lab Setup)

```
mkdir -p $HOME/.jupyter

cat <<EOF >> $HOME/.jupyter/jupyter_notebook_config.py
c.NotebookApp.open_browser = False
c.NotebookApp.ip = '0.0.0.0'
c.NotebookApp.port = 8765
c.NotebookApp.token = ''
EOF
```

---

## Cluster Setup - 6 (Other Sys Admin)

```
cat <<EOF > /tmp/jupyter-lab.service
[Unit]
Description="Jupyter Lab Server"

[Service]
Type=simple
ExecStart=/usr/bin/su -s /bin/bash hadoop -c "cd $HOME; $HOME/miniconda/bin/jupyter lab" >> /mnt/tmp/jupyter-lab.log 2>&1
[Install]
WantedBy=multi-user.target
EOF
sudo mv /tmp/jupyter-lab.service /etc/systemd/system
sudo chmod 644 /etc/systemd/system/jupyter-lab.service
sudo systemctl start jupyter-lab

echo "Starting Jupyter Notebook Server"
sudo systemctl daemon-reload
sudo systemctl start jupyter-lab

conda init
echo -e "\nalias pip='$HOME/miniconda/bin/pip'" >> $HOME/.bashrc
echo -e "\nalias python='$HOME/miniconda/bin/python'" >> $HOME/.bashrc
```

---

## Review of Logs in EMR

::: {.imgcenter}
<img src="img/emr-folder-log.png" width=500>
:::

* Node -> first node option -> bootstrap-actions -> 1 -> stdout.gz
* **Click to View/Download**
* [Example log file](https://aws-logs-011789619187-us-east-1.s3.amazonaws.com/elasticmapreduce/j-2C1SB2C6GEQBU/node/i-00492036dbf62ebcf/bootstrap-actions/1/stdout.gz?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEKH%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIQCNBbhXwxL0fIE4NWjwIZf7BspSCwTAAyTV1AdGnyswFAIfIxC7cbxfiuPw5E9pAmyt7frztH%2B5XObOYlnF7ahwISruAghqEAAaDDAxMTc4OTYxOTE4NyIMKvhPAmeUG0StWi6AKssCDHArk70gzuQmP5Lv0HYDQN0I7Mt93ydDHhCjFqQjgC1uBIRPLRt4nVnf3MZE48pFwOVQkKvfmIvbP0aFkI1iSZ4Wzo6Fuzf%2Fr5QG54YxX274YVOoiPpxyi78OW%2BumZT1IyyZ8dSKTrYXNVZLEGP3GJU73KdtlHQJblHP%2BO7jeepbBPz5U%2BTZn7aP0hMujGA35Zy9IXU0EMu%2BxhL0vMEwsk9WSwFuy8khCVZut9ngnC%2B%2FwB8v%2FpB5tPXtUJUJs6fJfpcc6LM0qjjYIwNrf9PjbPjkdFlEVoi0j7V%2B0exbqurmarWE%2Fzy87JWSDJimFPl%2Ft0EkQmvEFHK1sWVXqHj62q7HBKFEFN2%2B87rH8SOaHFwd%2FdE789c00%2BSujJYnRjKhSWzrr1lVtukbFNNJBvQCA7npXQV%2Fz0ro8eBUE4SaCjpZEsaoqOq88kHcvjDd8pKaBjqIAptxmNhNcT9P6YLjcDFO8mjJdkQGa5gW2vt70E6qfl27uZh%2FqtkvBEo3uPzdh2f%2FVdxlG9DW8Q1zpVg1q9cK4WqMoVbffKIvkgCfi%2FMm3w9T%2FVJcGe0shBkh1%2Fl7vOABEpZRtWCtFaP4Ep8Co%2F8zIOlDBIadY4GuWSgvH5pgBJz4Y3waZX4Xf0r1y05usPYj0OFttvGU7Dxu2T82jum7w%2FYhjdjh4E5QvIMBXhrrZ34rs3bcNzaLvZ2k2wffLlTAtzHGOnKZemrx%2BXmGPbj4FP4EB1dwB%2BSPjYQ0K7gkVtS392qk60FBzdUR6ZIGNALmg%2BskkSeG8bGmd9l3Mj5vE76c8u8fItNi%2FA%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20221011T030707Z&X-Amz-SignedHeaders=host&X-Amz-Expires=29&X-Amz-Credential=ASIAQFPVXJ7Z3ZIDSAUP%2F20221011%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=fb11604e1cba7fd52130eea6e039f122e24c48b830c554050942ecace3aabc87)

---

## Check the Bootup Routine for your EC2

* Hardware tab
* Master Instance Group
* Instance ID of EC2
* EC2 Hardware Details
* Actions -> "Monitor and troubleshoot" -> Get system log
* Actions -> "Monitor and troubleshoot" -> Get instance screenshot

# Spark: a Unified Engine {.section}

## Connected and extensible

::: {.imgcenter}
<img src="img/spark-connectors.png" width=600>
:::


## Three data structure APIs

1. **RDDs** (Resilient Distributed Datasets)

1. **DataFrames** SQL-like structured datasets with query operations

1. **Datasets** A mixture of RDDs and DataFrames 


## Spark vs. Hadoop

```{r, echo = FALSE}
tribble( ~"Hadoop Limitation", ~"Spark Approach",
         "For iterative processes and interactive use, Hadoop and MapReduce's mandatory dumping of output to disk proved to be a huge bottleneck. In ML, for example, users rely on iterative processes to train-test-retrain.", "Spark uses an in-memory processing paradigm, which lowers the disk IO substantially. Spark uses DAGs to store details of each transformation done on a parallelized dataset and does not process them to get results until required (lazy).",
         "Traditional Hadoop applications needed the data first to be copied to HDFS (or other distributed filesystem) and then did the processing.", "Spark works equally well with HDFS or any POSIX style filesystem. However, parallel Spark needs the data to be distributed.",
         "Mappers needed a data-localization phase in which the data was written to the local filesystem to bring resilience.", "Resilience in Spark is brough about by the DAGs, in which a missing RDD is re-calculated by following the path from which the RDD was created.",
         "Hadoop is built on Java and you must use Java to take advantage of all of it's capabilities. Although you can run non-Java scripts with Hadoop Streaming, it is still running a Java Framework.", 
         "Spark is developed in Scala, and it has a unified API with so you can use Spark with Scala, Java, R and Python."
         ) %>% 
    gt() 
```

# Reviewing the RDD {.section}

## Transformations and Actions (key Spark concept)


::: {.imgcenter}
<img src="img/transformations-and-actions.png">
:::


## How to create and RDD?

RDDs can be created in two ways:

::: incremental
* **Transforming an existing RDD:** just like a call to `map` on a list returns a new list, many higher order functions defined on RDDs return a **new** RDD

* **From a `SparkContext` or `SparkSession` object:** the `SparkContext` object (renamed `SparkSession`) can be though of as your handle to the Spark cluster. It represents a connection _between_ the Spark Cluster and your application/client. It defines a handful of methods which can be used to create and populate a new RDD:

  * **`parallelize:`** converts a local object into an RDD
  * **`textFile:`** reads a **text** file from your filesystem and returns an RDD of strings
:::

## Transformations and Actions

Spark defines _**transformations**_ and _**actions**_ on RDDs:

**Transformations** return new RDDs as results.

::: {.fragment .fade-in fragment-index=1}
[Transformations are **lazy**, their result RDD is not immediately computed.]{style="color:#cc0000"}
:::

<br/>
<br/>

**Actions** compute a result based on an RDD which is either returned or saved to an external filesystem.

::: {.fragment .fade-in fragment-index=1}
[Actions are **eager**, their result is immediately computed.]{style="color:#cc0000"}
:::


## Common `RDD` Transformations

```{r, echo = FALSE}
tribble( ~"Method", ~"Description",
         "`map`", "Expresses a one-to-one transformation and transforms each element of a collection into one element of the resulting collection",
         "`flatMap`", "Expresses a one-to-many transformation and transforms each element to 0 or more elements",
         "`filter`", "Applies filter function that returns a boolean and returs an RDD of elements that have passed the filter condition",
         "`distinct`", "Returns RDD with duplicates removed"
) %>% 
  gt::gt() %>% 
  fmt_markdown(columns = vars(Method, Description))
```

---

## Common `RDD` Actions

```{r, echo = FALSE}
tribble( ~"Method", ~"Description",
         "`collect`", "Returns **all** distributed elements of the `RDD` to the driver",
         "`count`", "Returns the number of elements in an `RDD`",
         "`take`", "Returns the first _n_ elements of the `RDD`",
         "`reduce`", "Combines elements of the `RDD` together using some function and returns result") %>% 
  gt::gt() %>% 
  fmt_markdown(columns = vars(Method, Description))
```

---

## `collect` **CAUTION**


::: {.imgcenter}
<img src="img/collect-warning.png">
:::

## Caching and Persistence

By default, RDDs are recomputed every time you run an action on them. This can be expensive (in time) if you need to use a dataset more than once.

**Spark allows you to control what is cached in memory.**

To tell spark to cache an object in memory, use `persist()` or `cache()`:

* `cache():` is a shortcut for using default storage level, which is memory only
* `persist():` can be customized to other ways to persist data (including both memory and/or disk)

```{python, echo = T, eval = F, python.reticulate=F}
# caches error RDD in memory, but only after an action is run
errors = logs.filter(lambda x: "error" in x and "2019-12" in x).cache()
```

# DataFrames {.section}

## Review of PySparkSQL Cheatsheet

[https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf)

## DataFrames in a nutshell

`DataFrames` are...

**Datasets organized into named columns**

Conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood.

**A relational API over Spark's RDDs**

Because sometimes it's more convenient to use declarative relational APIs than functional APIs for analysis jobs:

::: {.column width="47%"}
* `select`
* `where`
* `limit`
:::

::: {.column width="47%"}
* `orderBy`
* `groupBy`
* `join`
:::

**Able to be automatically aggresively optimized**

SparkSQL applies years of research on relational optimizations in the database community to Spark



## DataFrame Data Types

SparkSQL's `DataFrames` operate on a restricted (yet broad) set of data types. These are the most common:

* Integer types (at different lengths): **`ByteType`**, **`ShortType`**, **`IntegerType`**, **`LongType`**
* Decimal types: **`Float`**, **`Double`**
* **`BooleanType`**
* **`StringType`**
* Date/Time: **`TimestampType`**, **`DateType`**


## A `DataFrame`

::: {.imgcenter}
<img src="img/dataframe.png">
:::

## Getting a look at your data

There are a few ways you can have a look at your data in `DataFrames`:

* **`show()`** pretty-prints `DataFrame` in tabular form. Shows first 20 elements

* **`printSchema()`** prints the schema of your `DataFrame` in a tree format.


## Common `DataFrame` Transformations

Like on RDD's, transformations on `DataFrames` are:

1. Operations that return another `DataFrame` as a results
1. Are lazily evaluated


## Some common transformations include:

```{r, echo = F}
tribble( ~"Method", ~"Description",
         "`select`","Selects a set of named columns and returns a new `DataFrame` with those columns as a result",
         "`agg`", "Performs aggregations on a series of columns and returns a new `DataFrame` with the calculated output",
         "`groupBy`", "Groups the DataFrame using the specified columns, usually used before some kind of aggregation",
         "`join`", "Inner join with another DataFrame") %>% 
  gt::gt() %>% 
  fmt_markdown(columns = vars(Method, Description))
```

Other transformations include: `filter`, `limit`, `orderBy`, `where`.

## Specifying columns

Most methods take a parameter of type `Column` or `String`, always referring to some attribute/column in the the DataFrame.

You can select and work with columns in ways using the DataFrame API:

1. Using $ notation: `df.filter($"age" > 18)`

2. Referring to the DataFrame: `df.filter(df("age") > 18)`

3. Using SQL query string: `df.filter("age > 18")`

---

## Filtering in SparkSQL

The DataFrame API makes two methods available for filtering: `filter` and `where`. They are equivalent!

```{python, echo = T, eval = F, python.reticulate=F}
employee_df.filter("age > 30").show()
```

is equivalent to 

```{python, echo = T, eval = F, python.reticulate=F}
employee_df.where("age > 30").show()
```


## Use either DataFrame API and SparkSQL

The DataFrame API and SparkSQL syntax can be used interchangeably!

**Example:** return the firstname and lastname of all the employees over the age over 25 that reside in Washington D.C.

## DataFrame API
```{python, echo = T, eval = F, python.reticulate = F}
results = df.select("firstname", "lastname") \
            .where("city == 'Washington D.C.' && age >= 25")
```

## SparkSQL
```{python, echo = T, eval = F, python.reticulate = F}
spark.sql("select firstname, lastname from df_view where city == 'Washington D.C.' and age >= 25")
          
# * Note: you have to register `df` using `df.createOrReplaceTempView("df_view")`
```

---

## Grouping and aggregating on DataFrames

Some of the most common tasks on structured data tables include:

1. Grouping by a certain attributed
1. Doing some kind of aggregation on the grouping, like a count

For grouping and aggregating, SparkSQL provides a **`groupBy`** function which returns a `RelationalGroupedDataset` which has several standard aggregation functions like **`count`**, **`sum`**, **`max`**, **`min`**, and **`avg`**.

## How to group

* Call a `groupBy` on a specific attribute/column of a DataFrame
* followed by a call to `agg`

```{python, echo = T, eval = F, python.reticulate=F}
results = df.groupBy("state") \
            .agg(sum("sales"))
```

---

## Actions on DataFrames

Like RDDs, `DataFrames` also have their own set of actions:

```{r, echo = FALSE}
tribble( ~"Method", ~"Description",
         "`collect`", "Returns an **array** that contains all the rows in the DataFrame to the driver",
         "`count`", "Returns the number of rows in a DataFrame",
         "`first`", "Returns the first row in the DataFrame",
         "`show`", "Displays the top 20 rows in the DataFrame",
         "`take`", "Returns the first _n_ rows of the RDD") %>% 
    gt::gt() %>% 
  fmt_markdown(columns = vars(Method, Description))
```


## `collect` **CAUTION**

::: {.imgcenter}
<img src="img/collect-warning.png">
:::

## Limitations on `DataFrame`

* Can only use DataFrame data types
* If your unstructured data cannot be reformulated to adhere to some kind of schema, it would be better to use RDDs.

# One of the most common performance bottlenecks of new Spark users happens because you re-evaluate several transformations when you could cache intermediate results to memory!

# Lab {.section}

