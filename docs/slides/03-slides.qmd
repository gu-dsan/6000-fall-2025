---
title: "Lecture 3"
subtitle: "Parallelization"
author: "{{< var instructor.name >}}"
institute:
- "{{< var university.name >}}"
- "{{< var course.semester >}}"
format:
  revealjs:
    slide-number: true
    show-slide-number: print
    theme: custom.scss
    transition: fade
    background-transition: fade
    highlight-style: ayu-mirage
    code-copy: true
---




## Agenda and Goals for Today

-   Scaling up and scaling out
-   Parallelization
-   Map and Reduce functions
-   Lab Preview: Parallelization with Python
    -   Use the `multiprocessing` module
    -   Implement synchronous and asynchronous processing
-   Homework Preview: Parallelization with Python
    -   Parallel data processing

## Looking back

- Continued great use of [Slack]{style="color: red;"} {{< fa brands slack >}}
  - Nice interactions
- Due date reminders:  
  - Assignment 2: **Wednesday, 31st January**
  - Lab 3: **Wednesday, 31st January**
  - Assignment 3: **Wednesday, 7th February**


## Glossary

```{r, echo = FALSE}
library(magrittr)
library(tibble)
library(kableExtra)
tribble( ~"Term", ~"Definition",
         "Local", "Your current workstation (laptop, desktop, etc.), wherever you start the terminal/console application.",
         "Remote", "Any machine you connect to via ssh or other means.",
         "EC2", "Single virtual machine in the cloud where you can run computation (ephemeral)",
         "SageMaker", "Integrated Developer Environment where you can conduct data science on single machines",
         "Ephemeral","Lasting for a short time - any machine that will get turned off or place you will lose data",
         "Persistent","Lasting for a long time - any environment where your work is NOT lost when the timer goes off"
) %>%
  kable(format = "html") %>%
  kable_styling(full_width = F) %>%
  column_spec(1, bold = T, border_right = T) %>%
  column_spec(2, width = "40em")
```

# Parallelization {.section}

## Typical real world scenarios

::: incremental
- You are a Data Scientist and you want to cross-validate your models. This involves running the model *1000 times* but each run takes over an hour.
- You are a Data Scientist and you want to run multiple models on your data, where each run can take up to 1 hour.
- You are a genomics researcher and have been using small datasets of sequence data but soon you will receive a new type of sequencing data that is *10 times* as large. This means 10x more transcripts to process, but the processing for each transcript is similar.
-   You are an engineer using a fluid dynamics package that has an option to run in parallel. So far, you haven't used this option on your workstation. When moving from 2D to 3D simulations, the simulation time has more than tripled so it may make sense to take advantage of the parallel feature
-   You are a Data Scientist at the Federal Reserve and you have millions of text to process. So far, you have only executed NLP on thousands of articles and have not implemented machine learning models on them.
:::

# Parallel Programming {.section}

## Linear vs. Parallel

::: columns
::: {.column width="47%"}
### Linear/Sequential

1.  A program starts to run
2.  The program issues an instruction
3.  The instruction is executed
4.  Steps 2 and 3 are repeated
5.  The program finishes running
:::

::: {.column width="47%"}
### Parallel

1.  A program starts to run
2.  The program divides up the work into chunks of instructions and data
3.  Each chunk of work is executed independently
4.  The chunks of work are reassembled
5.  The program finishes running
:::
:::

## Linear vs. Parallel

::: columns
::: {.column width="47%"}
<img src="img/linear.png" width="400"/>
:::

::: {.column width="47%"}
<img src="img/parallel.png" width="400"/>
:::
:::

## Linear vs. Parallel

From a data science perspective

::: columns
::: {.column width="47%"}
### Linear

-   The data remains monolithic
-   Procedures act on the data sequentially
    -   Each procedure has to complete before the next procedure can start
-   You can think of this as a single pipeline
:::

::: {.column width="47%"}
### Parallel

-   The data can be split up into chunks
-   The same procedures can be run on each chunk at the same time
-   Or, independent procedures can run on different chunks at the same time
-   Need to bring things back together at the end
:::
:::

::: fragment
### What are some examples of linear and parallel data science workflows?
:::

## Embarrasingly Parallel

It's **easy** to speed things up when:

-   You need to calculate the same thing many times
-   Calculations are **independent** of each other
-   Each calculation takes a decent amount of time

Just run **multiple calculations at the same time**

## Embarrasingly Parallel

The concept is based on the old middle/high school math problem:

> If 5 people can shovel a parking lot in 6 hours, how long will it take 100 people to shovel the same parking lot?

Basic idea is that many hands (cores/instances) make lighter (faster/more efficient) work of the same problem, as long as the effort can be split up appropriately into nearly equal parcels

::: {.aside}
The classical answer to the problem is 18 minutes
:::

## Embarassingly parallel 

::: {.incremental}

- If you can truly split up your problem into multiple **independent** parts, then you can
often get linear speedups with the number of parallel components (to a limit)
  - The more cores you use and the more you parallelize, the more you incur communication overhead and decrease available RAM, so the speedup is almost certainly sub-linear, i.e. for a 4-core machine you'll probably get a 3-4x speedup, but rarely a full 4x speedup^[Gorelick & Ozsvald, 2020. *High Performance Python*, O'Reilly]
- The question often is, which part of your problem is embarassingly parallel?
- Amdahl's law (which we'll see in a few slides) shows how parallelization can benefit overall if a large proportion of the problem is parallelizable
- It's not all milk and honey. Setting up, programming, evaluating, debugging parallel computations requires better infrastructure and more expertise.

:::

# When might parallel programming not be more efficient? {.section}

## Some limitations

#### You can get speedups by parallelizing computations, but

- Having to transport data between parallel processes (memory bottlenecks)  and communication between processes (I/O bottlenecks) can make things more expensive and can exceed the benefits of parallelization
- If you're moving a lot of data but not doing a lot of parallel computing, it's often not worth the effort

::: {.fragment}

#### Setting up and debugging parallel programs can be difficult

But this has become easier with better software, like the `multiprocessing` module in Python

:::

::: {.fragment}
#### Making sure that we can get back all the pieces needs monitoring

- Failure tolerance and protections (Hadoop, e.g.)
- Proper collection and aggregation of the processed data

:::

## Amdahl's Law

::: {.column width="50%"}
::: {.imgcenter }
![](img/AmdahlsLaw.svg)
:::
:::

::: {.column width="45%"}
$$
\lim_{s\rightarrow\infty} S_{latency} = \frac{1}{1-p}
$$

where $s$ is the speedup of that part of the task (which is $p$ proportion of the overall task) benefitting from improved resources.

:::

If 50% of the task is embarassingly parallel, you can get a maximum speedup of 2-fold, while if 90% is embarassingly parallel, you can get a maximum speedup of $1/(1-0.9) = 10$ fold. 

::: {.aside}
By <a href="https://en.wikipedia.org/wiki/User:Daniels220" class="extiw" title="wikipedia:User:Daniels220">Daniels220</a> at <a href="https://en.wikipedia.org/wiki/" class="extiw" title="wikipedia:">English Wikipedia</a>, <a href="https://creativecommons.org/licenses/by-sa/3.0" title="Creative Commons Attribution-Share Alike 3.0">CC BY-SA 3.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=6678551">Link</a>
:::

## Pros and cons of parallelization

::: {.column width="47%"}
### Yes

-   Group by analysis
-   Simulations
-   Resampling / Bootstrapping
-   Optimization
-   Cross-validation
-   Training bagged models (like Random Forests)
-   Multiple chains in a Bayesian MCMC
-   Scoring (predicting) using trained models
:::

::: {.column width="47%"}
::: fragment
### No

-   SQL Operations
-   Inverting a matrix
-   Training linear regression
-   Training logistic regression
-   Training trees
-   Training neural nets
-   Training boosted models (like gradient boosted trees)
-   Each chain in a Bayesian MCMC
-   Most things time series
:::
:::

::: {.fragment}
::: {.callout-note appearance="simple"}
For processes in the "No" column, each step depends on a previous step, and so they cannot be parallelized. However, there are approximate numerical methods applicable to big data which are parallelizable and get you to the right answer, based on parallely taking random subsets of the data. We'll see some of these when we look at Spark ML
:::
:::

## Pros and cons of parallelization

::: columns
::: {.column width="47%"}
### Pros

-   Higher efficiency

-   Using modern infrastructure

-   Scalable to larger data, more complex procedures

    -   *proviso* procedures are embarassingly parallel
:::

::: {.column .right width="47%"}
::: fragment
### Cons

-   Higher programming complexity
-   Need proper software infrastructure (MPI, Hadoop, etc)
-   Need to ensure right packages/modules are distributed across processors
-   Need to account for a proportion of jobs failing, and recovering from them
-   Hence, Hadoop/Spark and other technologies
-   Higher setup cost in terms of time/expertise/money
:::
:::
:::

::: fragment
There are good solutions today for most of the cons, so the pros have it and so this paradigm is widely accepted and implemented
:::


{{< include parallelization-python/_module-parallel.qmd >}} 

{{< include parallelization-python/_module-parallel-computing.qmd >}} 
{{< include parallelization-python/_module-map-and-reduce.qmd >}}
{{< include parallelization-python/_module-multiprocessing.qmd >}}
{{< include parallelization-python/_module-asyncio.qmd >}}

